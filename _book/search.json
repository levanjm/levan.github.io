[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics and Statistics",
    "section": "",
    "text": "Data Science and the Liberal Arts\nWhy is it important to study data analytics through the lens of the liberal arts? It bridges the gap between quantitative analysis and the qualitative insights that are the hallmark of liberal arts education. Here are some key points that highlight its importance:\nData science can be applied across a wide range of disciplines within the liberal arts, including sociology, psychology, political science, economics, history, and more. It provides tools to analyze complex data sets, uncover patterns, and generate insights that can inform and enrich these fields.\nLiberal arts education emphasizes critical thinking and problem-solving skills. Data science enhances these skills by teaching students how to approach problems methodically, use data to support their arguments, and make data-driven decisions.\nIn today’s data-driven world, data literacy is becoming as important as traditional literacy. Understanding data science equips liberal arts students with the ability to interpret data, understand statistical concepts, and critically evaluate the information presented to them.\nData science provides powerful tools for conducting empirical research. Liberal arts students can use data analysis techniques to test hypotheses, validate theories, and contribute to academic knowledge in their respective fields.\nThe liberal arts often emphasize ethical considerations and the impact of decisions on society. Data science intersects with ethics, particularly in areas like data privacy, bias in algorithms, and the use of data for social good. Understanding these ethical implications is crucial for responsible data use.\nData science involves not only analyzing data but also communicating findings effectively. Liberal arts students, who typically excel in written and verbal communication, can leverage these skills to present data insights in a compelling and understandable manner.\nLiberal arts education fosters creativity and innovation. Data science can benefit from this creative approach, as students can explore novel ways to visualize data, generate hypotheses, and apply data-driven insights to real-world problems.\nData science skills are in high demand across various industries. Liberal arts students with data science expertise can pursue diverse career paths, including roles in research, policy analysis, marketing, journalism, and more.\nData science can be a powerful tool for social justice and advocacy. By analyzing data related to social issues, liberal arts students can highlight disparities, inform policy decisions, and advocate for positive change.\nUnderstanding data science empowers individuals to make informed decisions in their personal lives, from interpreting health data to making financial choices. It also enables them to engage more critically with information in the media and public discourse.\nIn summary, data science enriches the liberal arts by providing valuable skills, fostering critical thinking, and enabling interdisciplinary applications. It prepares students to navigate and contribute to a data-driven world while maintaining the ethical and humanistic perspectives central to the liberal arts.\nWe hope that you will find this course to be a valuable experience and that you will be able to apply the skills you learn to your future studies and career.",
    "crumbs": [
      "Data 1004 - Data Analytics and Statistics"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Analytics and Statistics",
    "section": "Welcome!",
    "text": "Welcome!",
    "crumbs": [
      "Data 1004 - Data Analytics and Statistics"
    ]
  },
  {
    "objectID": "index.html#data-science-and-the-liberal-arts",
    "href": "index.html#data-science-and-the-liberal-arts",
    "title": "Data Analytics and Statistics",
    "section": "",
    "text": "Interdisciplinary Applications\n\n\n\nCritical Thinking and Problem-Solving\n\n\n\nData Literacy\n\n\n\nEmpirical Research\n\n\n\nEthical Considerations\n\n\n\nCommunication Skills\n\n\n\nInnovation and Creativity\n\n\n\nCareer Opportunities\n\n\n\nSocial Justice and Advocacy\n\n\n\nPersonal Empowerment",
    "crumbs": [
      "Data 1004 - Data Analytics and Statistics"
    ]
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About",
    "section": "",
    "text": "As we were preparing the courses for our new Data Analytics program, we quickly came to realize that what we are trying to accomplish is not what many people would consider a “traditional” introductory data science course. As we looked through our peer institutions, we found that many introductory data science courses were focused on the tools and techniques of data science. While we certainly cover those topics, we also wanted to focus on the process of data science. We wanted to teach students how to think about data, how to ask questions of data, and how to use data to answer those questions. Another caveat of this course is that it is designed for students who have little to no experience with data science, computer programming, or statistics.\nWe wanted to create a course that would be accessible to students from a wide variety of backgrounds. This is a course that our institution allows to be taken as a general education course, so we wanted to make sure that it was accessible to students who may not have a strong background in mathematics or computer science.\nThis course is designed to be a gentle introduction to the world of data science. We will cover the basics of data science, including data visualization, data wrangling, and data analysis. We will also cover some of the tools and techniques that are commonly used in data science, such as the R programming language and the tidyverse. We will also cover some of the ethical and social implications of data science, such as privacy, bias, and fairness.\nWe hope that this course will be a valuable resource for students who are interested in learning more about data science, as well as for students who are interested in pursuing a career in data science. We also hope that this material will be a valuable resource for instructors who are looking for a gentle introduction to data science that they can use in their own courses.\nUnfortunately, there are not any textbooks that cover the material that we are trying to cover in this course. Most books used to teach data science or R courses are written for students that are intending to major in these fields. That is not the purpose of this course. This course is an amalgamation of data analytics, statistics, and R programming. We are certainly hoping that this will be a good gateway into our data science program, but we also want to make sure that this course is accessible to students who may not have any intention of majoring in data science.\nThe information here is a dynamic document. We will be updating it as we go through the course. We will be adding new material, updating old material, and fixing any errors that we find. Here is how we intend to structure the course:\n\n(Mostly) Daily lessons : We have flipped the classroom for this course. We provide a series of lessons the students are expected to watch, take notes, and to work through the examples before coming to class. This allows us to spend more time in class working through the assignments. We have found this to be especially useful for those stuents with little to no programming background.\n(Mostly) Daily Assignments : These are shorter assignments that are given in class after the students have watched the online lesson before coming to class. These are intended to reinforce the material that was covered in the lesson.\nIndividual Labs : These are longer assignments that are intended to be completed outside of class. These are intended to give the students more in depth practice with the material that was covered in the lesson. There are some days where we will have the students work on these in class as they are intended to be more challenging than the daily assignments.\nGroup projects : We have three group projects throught the term. Students are usually broken into groups of size 3 - 4. We usually take information from students before assigning the groups. I try to make sure that each group has someone that has at least a little programming experience. The group projects are intended to give the students experience working with others on a data science project. We have found that this is a valuable experience in collaborative work. These assignments are intended to be more challenging than the individual labs and generally give up to two weeks to complete.\nExams : We have no exams in this course. We wanted to focus more on the process, tools, and techniques of data science. We felt that exams would not be a good way to assess the learning outcomes. We wanted the students to learn the materail through their individual and group assignments. However, we do have short quizzes that are intended to be a check on the students’ progress through the course. These are generally given at the beginning of class to make sure the students are preparing adequately for that day’s lesson.\n\nThere are sample assignments given at the end of the book. Again, these are intended for students with a relatively low level of programming experience and with no expectations of their having any kind of higher mathematics courses.\nWe hope that you find this material useful. We are always looking for feedback on how to improve the course. If you have any suggestions, please feel free to reach out to us.\nThanks, Mike\n\nDr. Mike LeVan  Transylvania University mlevan@transy.edu",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "What_Is_Data_Science.html",
    "href": "What_Is_Data_Science.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "Step 1 : Data Collection\nThe first step is to collect the data. As was mentioned above, this can be done in a variety of ways. The data can be collected from a variety of sources, some of which are going on and we don’t even know it. The data can be collected from a variety of sources, from the internet, to social media, to smart devices, and even from the government. The data can be collected in a variety of ways, from surveys, to interviews, to observations, to experiments. The data can be collected in a variety of formats, from text, to images, to audio, to video. Once the data collection process is completed, the job of a data scientist is to take this data and find what story it is trying to tell.",
    "crumbs": [
      "What is Data Science?"
    ]
  },
  {
    "objectID": "What_Is_Data_Science.html#step-2-data-preparation",
    "href": "What_Is_Data_Science.html#step-2-data-preparation",
    "title": "What is Data Science?",
    "section": "Step 2 : Data Preparation",
    "text": "Step 2 : Data Preparation\n\n\n\nIf you talk with someone who is a data scientist, they will undoubtedly tell you that the majority of their time is spent preparing the data. This is because the data is often messy and needs to be cleaned up before it can be analyzed. The data needs to bet set up in a way that the data scientist will be able to analyze it. The data will need to be set up so that it can be analyzed by a script. That means it needs to be prepared (often called “cleaned”) so that the script can read in the data, label the variables, and start the analysis. We are trying to structure the data set so that the script can take in the data and start the anaylsis. \nThis can be done in a variety of ways, from removing missing data, combining variables, removing duplicates, dealing with outliers, and removing irrelevant data. Once the data is cleaned up, the job of a data scientist is to take this data and find what story it is trying to tell.",
    "crumbs": [
      "What is Data Science?"
    ]
  },
  {
    "objectID": "What_Is_Data_Science.html#step-3-exploration-and-visualization",
    "href": "What_Is_Data_Science.html#step-3-exploration-and-visualization",
    "title": "What is Data Science?",
    "section": "Step 3 : Exploration and Visualization",
    "text": "Step 3 : Exploration and Visualization\n\n\n\nOnce the data is cleaned up, the data scientist will start to explore the data. This can be done in a variety of ways, from looking at the data in a table, looking at the data in a graph, or perhaps looking at the data in a map. The data scientist will start to look at the data with many different tools to see what story it is trying to tell. \nTraditionally, the data scientist will start by looking at the data in a table or a graph. This initial look at the data will help the data scientist to see if there are any obvious patterns or trends in the data. If there are, then the data scientist will start to explore these patterns or trends in more detail. \nFor example, conside the story Little Women. The following graph shows the cumulative number of times the names of the main characters are used in the book.\n\n\n\n\nThe graph shows that the name “Jo” is used the most, followed by “Meg”, “Amy”, and “Laurie”. If we were going to ask the question of who the main character is in the book, the data would suggest that the protagonist of the story is Jo. Her name appears the most in the book, and it is not very close.While this is an interesting idea to consider, we could also delve a little deeper at this graph. In the story, two of the five characters listed get married to one another. Based on this graph, can we hypothesize as to whom those two people might be? \nBased on the graph, we could hypothesize that Amy and Laurie get married. If we examine the graph, we can see that the pattern for the number of times Amy appears in the book is very similar to the pattern for the number of times Laurie appears in the book starting around chapter 35. This could suggest that the two of them are in several scenes at the same time and when one is mentioned, then the other is also there. Thus they are starting to spend more time together. \nThis proves to be true as Amy and Laurie get married in chapter 44.",
    "crumbs": [
      "What is Data Science?"
    ]
  },
  {
    "objectID": "What_Is_Data_Science.html#step-4-experimentation-and-prediction",
    "href": "What_Is_Data_Science.html#step-4-experimentation-and-prediction",
    "title": "What is Data Science?",
    "section": "Step 4 : Experimentation and Prediction",
    "text": "Step 4 : Experimentation and Prediction\n\n\n\n\nOne of the goals of a data scientist is to take data and try to figure out what comes next.\n\nIs a stock going to rise or fall?\nIs a virus going to continue to spread or die out?\nIs a customer going to buy a product?\n\nThese are all questions that a data scientist will try to answer. We are now at the stage of trying to make predictions based on the data that we have collected, cleaned, and explored. A data scientist will make a “Model” that will try to predict what comes next. When we say “model”, are really saying a mathematical equation that represents the data set. This model will be used to make predictions about the future. There are several ways to make a model, but this course will focus primarily on what is know as “linear regression”. This is a beginning modelling technique that will introduce you to the modelling concept and get you started on the path to making predictions.",
    "crumbs": [
      "What is Data Science?"
    ]
  },
  {
    "objectID": "What_Is_Data_Science.html#step-5-learning-goals",
    "href": "What_Is_Data_Science.html#step-5-learning-goals",
    "title": "What is Data Science?",
    "section": "Step 5 : Learning Goals",
    "text": "Step 5 : Learning Goals\n\n\n\n\n\nAs we walked through above, one of the main goals of this course is for you to be able to take in a data set and find some story that it is trying to tell. In other words, what insights can you pull out of a data set?\n\n\n\n\n\nIf a data scientist, or any scientist for that matter, wants their work to be taken seriously, then it needs to be reproducible. This means that if you were to take the data set and the script that was used to analyze the data, you should be able to get the same results. This is a big part of the scientific method. If you can’t reproduce the results, then the results can not be considered reliable or valid. \nFor our consideration, what does it mean for work to be “reproducible”? \n\n\n\n\nIn the graphic above, the near-term goals show you what to consider for the project in which you are currently working. The long-term goals show you what you should be thinking about for the future as you are working on you current project. \nYou don’t want to be always “recreating the wheel”. Can you write code that can be adapted to other data sets easily? Can you write code that you, or someone else, could modify to find a different story in your data set or in a different data set?\n\n\n\n\n\nThe final two learning goals we will discuss will be working collaboratively and using the same tools as used by data science professionals. As you will hear from some of our data scientist guests, they are almost always working with a group. This group analyses the data, creates the visualizations, writes code, and finalize reports together. Being able to work with and listen to others is a skill that can help you become a better data scientist, better communicator, and better team member. \nWhen it comes to tools for data analysis, the industry standards are R and Python. These are the two most common programming languages used by data scientists. Academic or scientific research is often done in R, while industry research is often done in Python. If you are going to be a data scientist, you will need to know how to use these tools. This course will focus on R, but do not worry if you are worried about not using python. The two languages are very similar and if you know one, the other is not too difficult to learn.",
    "crumbs": [
      "What is Data Science?"
    ]
  },
  {
    "objectID": "What_Is_Tidyverse.html",
    "href": "What_Is_Tidyverse.html",
    "title": "What Is Tidyverse",
    "section": "",
    "text": "Table with Column Span\n\n\n\n\n\nTable with Column Span\n\n\n\n\n\n\n\n\n\n\nThe tidyverse is a collection of R packages that share an underlying design philosophy, grammar, and data structures. The tidyverse is designed to make data science faster, easier, and more fun!  https://www.tidyverse.org\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse is a very important package that contains several other packages that are used for data manipulation and visualization. This provides a consistent, efficient, and user-friendly framework for data manipulation, visualization, and analysis. Here are some key reasons why learning the tidyverse is beneficial:\n\nUnified and Consistent Approach : The tidyverse is a collection of R packages (e.g., ggplot2, dplyr, tidyr, readr, purrr, tibble) that follow a consistent syntax and philosophy. Functions work seamlessly together, reducing the learning curve compared to working with disparate R packages.\nData Wrangling Made Easy : dplyr provides intuitive functions like filter(), mutate(), select(), and group_by() to transform and manipulate data efficiently. tidyr helps reshape and clean messy data using functions like pivot_longer(), pivot_wider(), and separate().\nPowerful Data Visualization : ggplot2 is one of the most popular visualization tools, allowing you to create elegant, customizable plots using a layered grammar of graphics. ggplot2 makes it easy to create complex visualizations with minimal code.\nEfficient Data Import & Export : readr enables fast reading and writing of CSV, Excel, and other data formats without unnecessary conversions.\nFunctional Programming with Purrr : purrr provides functions like map() to perform operations on lists and apply functions efficiently.\nReproducibility and Readability : The pipe operator (%&gt;%) makes code more readable by allowing operations to be chained together in a logical sequence. Code written using the tidyverse is often easier to understand and reproduce.\nWide Adoption and Strong Community Support : The tidyverse is widely used in academia, research, and industry, with extensive documentation and community support. \n\nBy mastering the tidyverse, you can streamline your data science workflow, making data manipulation, visualization, and analysis more intuitive and efficient. \n\nAs we go through this course, you will use several of these packages many times. Here is a list of the four main tidyverse packages we will be using in this course:\n\nreadr\nThe readr( ) package makes it easy to read in files that contain data. It can read in files that are in CSV, TSV, XLS, and other formats. While there are built in base commands that can do this, the readr package is faster and more consistent.\n\n\ntibble\nThe tibble package is a modern version of the data.frame. Tibbles are easier to read and work with than data.frames. Tibbles also have an enhanced print( ) method which makes them easier to use with large datasets containing complex objects. \nWe will use the tibble package to create data frames as we learn to read in data.\n\n\ndplyr\nThe dplyr package is a grammar of data manipulation. The dplyr package makes it easy to analyse data using commands such as filter( ), select( ), mutate( ), arrange( ), and summarize( ).\nWe will use the dplyr package to manipulate data frames as we learn to clean data.\n\n\nggplot2\nThe ggplot2( ) package is a grammar of graphics. The ggplot2 package makes it easy to create beautiful and informative plots. This will be the primary method we will use to create vizualizations in this course. \nWe will go over several different types of plots that can be created using ggplot2. \n\nHere are some other tidyverse packages that are not as commonly used in this course, but are still very important to know: \n\n\ntidyr\nThe tidyr package makes it easy to tidy data. The tidyr package makes it easy to convert data from wide to long format. \nThis type of data cleaning will not be used as much in this course, but it is a very important concept to understand and is addressed in a more advanced course.\n\n\nbroom\nThe broom package makes it easy to work with statistical models. The broom package makes it easy to tidy up the output of statistical models. \nWe will revisit this package when we delve into modelling a data set.\n\n\npurrr\nThe purrr package is a functional programming toolkit. The purrr package makes it easy to work with lists and vectors.\n\n\nstringr\nThe stringr package makes it easy to work with strings. The stringr package makes it easy to manipulate strings.\n\n\nforcats\nThe forcats package makes it easy to work with factors. The forcats package makes it easy to manipulate factors.\n\n\nlubridate\nThe lubridate package makes it easy to work with dates and times. The lubridate package makes it easy to manipulate dates and times. \n\n\n\nInstallation\nTo install the tidyverse package, you can use the following command:\n\n# If needed, you can install tidyverse by using the following command:\n\n# install.packages(\"tidyverse\")\n\n# Once the package is installed, you can load it up using the library command:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nYou can see that we now have access to the packages that we described above and by looking at the Packages tab in RStudio. You can see that the tidyverse package is loaded up and ready to use.\n\n\nConclusion\nThis is one of the more important packages to get to know if you want to become a data scientist. This package is constantly being updated and improved, so it is important to stay up to date with the latest changes. \nYou can find the latest updates and information on the tidyverse website: \n\nhttps://www.tidyverse.org",
    "crumbs": [
      "Step 1 - Data Collecting",
      "What Is Tidyverse"
    ]
  },
  {
    "objectID": "Statistics_Language.html",
    "href": "Statistics_Language.html",
    "title": "Statistics Language",
    "section": "",
    "text": "Population\nThe population is the entire group that we are interested in studying. For example, if we are interested in studying the average height of all people in the United States, then the population would be all people in the United States.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#parameter",
    "href": "Statistics_Language.html#parameter",
    "title": "Statistics Language",
    "section": "Parameter",
    "text": "Parameter\nA parameter is a number that describes a population. For example, the average height of all people in the United States is a parameter. Two of the more common parameters are the mean and the standard deviation. These are used to describe the central tendency and the spread of a population. The mean is typically denoted by the Greek letter mu (μ) and the standard deviation is typically denoted by the Greek letter sigma (σ). We will briefly talk about these measures below and go into more detail in later sections. \nMany studies that are done are ones that are trying to estimate a parameter. Unfortunately, there is only one way to know the true value of a parameter and that is to measure the entire population. This is not feasible in most cases as the amount of time and resources to contact every member of a population is not practical. We can, however, take what is called a sample of the population and use that to estimate the parameter.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#sample",
    "href": "Statistics_Language.html#sample",
    "title": "Statistics Language",
    "section": "Sample",
    "text": "Sample\nA sample is a subset of the population. For example, if we are interested in studying the average height of all people in the United States, then a sample could be a group of 100 people from the United States. This is a much easier way to get information about the population rather than having to contact every member of the population. \nThere are several different methods one can use to take a sample. We will discuss these methods in later sections.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#statistic",
    "href": "Statistics_Language.html#statistic",
    "title": "Statistics Language",
    "section": "Statistic",
    "text": "Statistic\nA statistic is a number that describes a sample. For example, the average height of a group of 100 people from the United States is a statistic. Two of the more common statistics are the sample mean and the sample standard deviation. These are used to describe the central tendency and the spread of a sample. The sample mean is typically denoted by the letter x-bar (x̄) and the sample standard deviation is typically denoted by the letter s.\nIf we are trying to answer a question about the population, for instance the average height of all people in the United States, we can take a sample of the population and use the value from the sample as an estimate for the parameter from the population. This is the basis of inferential statistics, which we mention next.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#inferential-statistics",
    "href": "Statistics_Language.html#inferential-statistics",
    "title": "Statistics Language",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nInferential statistics are used to make inferences from a sample to a population. In other words, inferential statistics are used to make predictions about a population based on a sample. The following graphic demonstrates the process.\n\n\n\n\n\nWe are wanting to determine a population parameter μ\nWe take a sample from the population\nCalculate the value of the statistic x̄ from the sample.\nUse the sample statistic x to estimate the population parameter μ. \n\nThis process is the basis of inferential statistics. Using the sample to help us determine estimates of the population parameter.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#descriptive-statistics",
    "href": "Statistics_Language.html#descriptive-statistics",
    "title": "Statistics Language",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nDescriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures. For our purposes we will focus on measures of central tendency (mean, median, mode) and measures of spread (standard deviation, variance, 5-Number summary). We will discuss these in more detail in later sections.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#confidence-interval",
    "href": "Statistics_Language.html#confidence-interval",
    "title": "Statistics Language",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nWe are using a sample to estimate a population parameter. Unfortunately, if we were to take multiple samples from the population, we would get different values for the parameter. This is because the sample is only a portion of the population. The sample you take the first time is almost assuredly going to be different from the sample you take the second time. This means that the statistic from the first sample is going to be different from the statistic from the second sample. \nThis is the reason why we will estimate a parameter with a confidence interval. A confidence interval is a range of values that is likely to contain the true value of a parameter. For example, a 95% confidence interval for the average height of all people in the United States would be a range of values that is likely to contain the true average height with 95% confidence. The confidence interval is based on the sample statistic and a margin of error. The higher the confidence level, the wider the confidence interval. We will discuss confidence intervals in more detail in later sections.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#distribution",
    "href": "Statistics_Language.html#distribution",
    "title": "Statistics Language",
    "section": "Distribution",
    "text": "Distribution\nThe distribution of a data set tells us two things : the values in the data set and how often they occur. \nHere is an example of a distribution of the number of times a person goes to the gym in a week.\n\n# We need the tibble function, so load up the tidyverse package\n\nlibrary(tidyverse)\n\n\n# Create a data frame\n\ndf &lt;- tibble(\n  gym_visits = c(0, 1, 2, 3, 4, 5, 6, 7),\n  frequency = c(10, 20, 30, 40, 50, 40, 30, 20)\n)\n\ndf\n\n# A tibble: 8 × 2\n  gym_visits frequency\n       &lt;dbl&gt;     &lt;dbl&gt;\n1          0        10\n2          1        20\n3          2        30\n4          3        40\n5          4        50\n6          5        40\n7          6        30\n8          7        20\n\n\n In this example, the distribution tells us how many times a person goes to the gym in a week and how often that occurs. For example, 40 people go to the gym 3 times a week or 30 people go to the gym 6 times a week.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#outlier",
    "href": "Statistics_Language.html#outlier",
    "title": "Statistics Language",
    "section": "Outlier",
    "text": "Outlier\nAn outlier is a data point that is significantly different from the other data points in a dataset. Outliers can have a significant impact on the results of statistical analyses, especially for means and standard deviations. Outliers can be caused by errors in data collection, measurement error, or natural variation in the data. \nIn the following example, if someone went to the gym 15 times in a week would be considered an outlier. This is because the number of times a person goes to the gym in a week is typically between 0 and 7. \nDepending on the data set, outliers could occur in either the high or the low direction.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#normal-distribution",
    "href": "Statistics_Language.html#normal-distribution",
    "title": "Statistics Language",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nOne of the more typical distributions one will work with is the normal distribution. A normal distribution is a symmetric unimodal distributionin which the values are distributed around the mean according to a bell-shaped curve. The normal distribution is characterized by its mean and standard deviation. In other words, the mean and standard deviation determine the shape of the normal distribution. The mean determines the center of the distribution and the standard deviation determines the height and width of the distribution.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Statistics_Language.html#skewness",
    "href": "Statistics_Language.html#skewness",
    "title": "Statistics Language",
    "section": "Skewness",
    "text": "Skewness\nOnce we know the distribution of a data set, we will often draw a picture of the distribution. One of the descriptors we will look for is skewness. Skewness is a measure of the asymmetry of a distribution. A distribution is symmetric if the left and right sides are mirror images of each other. A distribution is positively skewed (or skewed to the right)\nif the right tail is longer than the left tail, and negatively skewed (or skewed left) if the left tail is longer than the right tail. \nHere is a graphic giving examples of each type of skewness.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Statistics Language"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables.html",
    "href": "Qualitative_and_Quantitative_Variables.html",
    "title": "Qualitative and Quantitative Variables",
    "section": "",
    "text": "Qualitative Variables\nQualitative variables are variables that are not numerical. They are categorical variables that can be divided into groups, which is why they are often referred to as categorical variables. You have probably encountered these when you have filled out an application for employment. You may have had to check boxes regarding ethnicity, level of education or gender. These are all examples of qualitative variables. We can also break them down into two different types of qualitative variables, nominal and ordinal variables.\nNominal variables are variables that have no order. For example,\nEven though these are not numerical variables, we could still create some numerical structures based off of nominal variables that will help us understand the data. For example, assume we took a survey of 200 Transylvania University students about their favorite sport to watch and that 97 said Football, 62 said Basketball, and 41 said Others. Based on this data, we could create a bar graph to show the distribution of favorite sports to watch. (Don’t worry about knowing these commands yet. We will get to these commands later in the course.)\nWe need to first create the data frame for the variable :\n# Create a dataframe. We will first create a vector that has the names\n# of the sports and another vector that has the number of students that\n# like each sport.\n\nsports &lt;- c(\"Football\", \"Basketball\", \"Others\")\n\nstudents &lt;- c(97, 62, 41)\n\n# We will then put them together into a data frame.\n\ndf &lt;- data.frame(sports, students)\n\n# And print out the data frame to make sure it looks correct.\n\ndf\n\n      sports students\n1   Football       97\n2 Basketball       62\n3     Others       41\nEven though this data is nominal, we could also create a bar graph to show the distribution of favorite sports to watch.\n# Create a bar graph using ggplot\n\n# We need to load the ggplot library.\n\nlibrary(ggplot2)\n\n# We can now create the bar graph.\n\nggplot(df, aes(x = sports, y = students)) + \n  geom_bar(stat = \"identity\", fill = \"darkred\") +\n  geom_text(aes(label = students), vjust = -0.3, color = \"black\") +\n  labs(title = \"Transy Students Favorite Sports to Watch\", \n       x = \"Sports\", y = \"Number of Students\")\nOrdinal variables are qualitative variables that have an order. For example, the variable “education” is an ordinal variable because there is an order to the levels of education. They could include the following:\nOrdinal variables are often treated as categorical variables, but if it makes sense, then they can also be treated as numerical variables. What we would need to do is to assign each of the variables a numerical value that corresponds to the level of the variable.\nWe could perform the same commands we did above, but by adding numerical values to the variables, we might be able to do other meaningful analysis. For instance, if we were to assign a numerical value to the levels of education (high school = 1, bachelor’s = 2, master’s = 3), we could do something such as calculating the mean level of education for a group of people.\nNotice that we could always add numerical values to qualitative variables, but it doesn’t always make sense to do so. Consider the hair color example. We could assign value to different outcomes (black = 1, brown = 2, blonde = 3, etc.) but what would we then do with this data? It doesn’t make sense to calculate the mean hair color of a group of people, especially if the way you label the numbers to variables is different from how I would label them. What would the averages describe? This is why we will call a qualitative variable a nominal variable if it doesn’t make sense to assign numerical values to the variable, and we will call it an ordinal variable if it does make sense to assign numerical values to the variable.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Qualitative and Quantitative Variables"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables.html#qualitative-variables",
    "href": "Qualitative_and_Quantitative_Variables.html#qualitative-variables",
    "title": "Qualitative and Quantitative Variables",
    "section": "",
    "text": "Hair or Eye Color\nReligious Affiliation\nGender\nMarital Status\nPolitical Affiliation\n\n\n\n\n\n\n\n\neducational level (high school, bachelor’s, master’s)\nclothing size (small, medium, large)\ncustomer satisfaction rating (dissatisfied, neutral, satisfied)\npain level (mild, moderate, severe)\nsocioeconomic status (low, middle, high).",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Qualitative and Quantitative Variables"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables.html#quantitative-variables",
    "href": "Qualitative_and_Quantitative_Variables.html#quantitative-variables",
    "title": "Qualitative and Quantitative Variables",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\nQuantitative variables are numerical variables. Just like the qualitative variables above, we can be divide quantitative variables into two different categories, discrete and continuous variables.\nDiscrete variables are variables that can only take on certain values, are countable, and are indivisible. For example, the number of students in a class is a discrete variable because it can only take on certain values (1, 2, 3, 4, etc.). Clearly you can’t have a class with 12.5 students. The number of siblings a person has is also a discrete variable because it can only take on certain values (0, 1, 2, 3, etc.). The number of cars a person owns is a discrete variable because it can only take on certain values (0, 1, 2, 3, etc.). Similarly, you couldn’t have 2.3 siblings or 4.8 cars. \nWhen it comes to the analysis of discrete data, the tools we will use are similar to the ones we used for qualitative data. We will use bar graphs, tables, pie charts, and other visualizations to help us understand the data. \nContinuous variables are variables that can take on any value in a certain range. A continuous variable takes on an infinite number of possible values within a given range. For example, the height of a person is a continuous variable because it can take on any value (5.2, 5.317, 5.4611, etc.). The weight of a person is a continuous variable because it can take on any value (120, 121.3, 122, etc.). The amount of time it takes to complete a task is a continuous variable because it can take on any value (5.2, 5.3, 5.4, etc.). \nBecause the possible values for a continuous variable are infinite, we measure continuous variables (rather than count), often using a measuring device like a ruler or stopwatch. Continuous variables include all the fractional or decimal values within a range. \nSometimes we treat continuous variables as if they were discrete. Age is an excellent example of this. If you know a person’s time of birth, you could measure their age precisely up to the second or even millisecond if you wanted to. In this sense, age is a continuous variable. However, we don’t usually care about a person’s exact age. Instead, we treat age as a discrete variable and count age in years. \nAnalyzing continuous variables in R primarily involves calculating descriptive statistics like mean, median, standard deviation, and visualizing the data distribution using plots like histograms, density plots, and boxplots, allowing you to understand the central tendency, spread, and potential skewness of the data; you can also explore relationships between continuous variables using scatterplots and calculate correlation coefficients to assess their linear association \nLet’s create a data set of continuous variables and analyze it. We will pick 1000 values from 0 to 10.\n\n# We shall \"set the seed\" for randomization. This allows us to get the same\n# values everytime we run these commands. Without this, the results would be\n# different each time.\n\nset.seed(1123)\n\n# Generate 1000 random values between 0 and 10 and store them in the \n# variable 'data_example'\n\ndf &lt;- runif(1000, min = 0, max = 10) \n\n# Here are the first six values in the data set, using the head( ) command :\n\nhead(df)\n\n[1] 7.595128 8.609249 8.658766 5.518078 8.264048 1.620436\n\n\n We could now carry out some analysis such as calculating the mean, median, and standard deviation of the data set. We could also create a histogram to show the distribution of the data. These are the types of analysis we will be doing using continuous variables. \n\n# Calculate the mean of the data set\n\nmean(df)\n\n[1] 4.932104\n\n# Calculate the median of the data set\n\nmedian(df)\n\n[1] 4.919963\n\n# create a histogram with 10 bars (bins) for the data set\n\nggplot(data = data.frame(df), aes(x = df, y = after_stat(density))) +\n  geom_histogram(bins = 10, binwidth = 1, center=0.5, fill = \"darkred\", color = \"black\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1), limits = c(0,10))+\n    stat_bin(geom = \"text\", aes(label = round(after_stat(density),3), y=0.5*after_stat(density)), \n           breaks=seq(0, 10, by=1), colour=\"white\", vjust=0.5) +\n  labs(title = \"Distribution of Data\", x = \"Data\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nFor continuous variables, it doesn’t make since to have a histogram for each individual values like we did for discrete variables. If we tried this then we might need an infinte amount of bars or more! Instead, we group the data into intervals (bins) of the same length and then create a histogram based on these intervals. This is what we did above. We grouped the data into intervals of 1 and then created a histogram based on the intervals (0,1), (1, 2), (2, 3), etc. For this histogram the bars represent the proportions (rounded to 3 decimal places) of the data that fall into each interval. If you look at the second bar in the histogram, you will see the number 0.121 in the bar. This means that 12.1% of the data falls into the interval (1, 2).",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Qualitative and Quantitative Variables"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables.html#exercises",
    "href": "Qualitative_and_Quantitative_Variables.html#exercises",
    "title": "Qualitative and Quantitative Variables",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will distinguish between different types of variables: qualitative (nominal and ordinal) and quantitative (discrete and continuous). Each question will present a variable, and you will identify its type.\n\nProblem 1\nGender identity of individuals in a survey. \n\nType: \nExplanation: \n\n\n\nProblem 2\nAnnual income of households in a city. \n\nType: \nExplanation: \n\n\n\nProblem 3\nNumber of books read by students in a year. \n\nType: \nExplanation: \n\n\n\nProblem 4\nEducational attainment levels (e.g., high school diploma, bachelor’s degree, master’s degree, etc.). \n\nType: \nExplanation: \n\n\n\nProblem 5\nNumber of reported hate crimes in different states. \n\nType: \nExplanation: \n\n\n\nProblem 6\nTypes of housing (e.g., owned, rented, homeless). \n\nType: \nExplanation: \n\n\n\nProblem 7\nAge of participants in a community health study. \n\nType: \nExplanation: \n\n\n\nProblem 8\nPolitical party affiliation (e.g., Democrat, Republican, Independent). \n\nType: \nExplanation: \n\n\n\nProblem 9\nNumber of languages spoken by immigrants in a region. \n\nType: \nExplanation: \n\n\n\nProblem 10\nFrequency of attending religious services (e.g., never, occasionally, regularly). \n\nType: \nExplanation: \n\n\n\nProblem 11\nNumber of protests attended in a year. \n\nType: \nExplanation: \n\n\n\nProblem 12\nTypes of discrimination experienced (e.g., racial, gender, disability). \n\nType: \nExplanation: \n\n\n\nProblem 13\nHeight of students in a classroom. \n\nType: \nExplanation: \n\n\n\nProblem 14\nMarital status (e.g., single, married, divorced, widowed). \n\nType: \nExplanation: \n\n\n\nProblem 15\nNumber of social media accounts owned by individuals. \n\nType: \nExplanation: \n\n\n\nProblem 16\nSeverity of food insecurity (e.g., none, mild, moderate, severe). \n\nType: \nExplanation: \n\n\n\nProblem 17\nNumber of children in a family. \n\nType: \nExplanation: \n\n\n\nProblem 18\nTypes of employment (e.g., full-time, part-time, unemployed). \n\nType: \nExplanation: \n\n\n\nProblem 19\nMonthly electricity usage in kilowatt-hours. \n\nType: \nExplanation: \n\n\n\nProblem 20\nFrequency of using public transportation (e.g., never, sometimes, always). \n\nType: \nExplanation:",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Qualitative and Quantitative Variables"
    ]
  },
  {
    "objectID": "Reading_In_Data.html",
    "href": "Reading_In_Data.html",
    "title": "Reading In Data",
    "section": "",
    "text": "CSV Files\nLet’s consider the case to where we have data in a CSV file. If you are unaware, CSV stands for “comma separated values.” These are simple text files where the data is separated by commas. We can use the read.csv() or read_csv()function to read the data into a variable in R.\nA CSV file can be upload to RStudio by clicking on “File” and then “Import Dataset” button and selecting “From Text (base)” or “From Text (readr).”\nFor this particular example, I asked ChatGPT to create a CSV file for me using the following query :\nI then proceeded to upload it using the steps above. First, I went to Files -&gt; Import Dataset -&gt; From Text (readr). This gave me a new dialog and I clicked on the file Reds-2024. Notice that I picked the CSV version and not the Pages version!\nI then clicked on the “Import” button and a new dialog was presented to me :\nIf you look in the upper left hand corner, you will see that you are given a choice for the name of your file. The default one given is Reds.2024, but you can change that to anything you wish. I went ahead and just kept this default name. I clicked on the “Import” button and the data was loaded into RStudio into the variable Reds.2024. You can look at the variable in the Environment tab :\nYou can also see the CSV file in the Files tab :",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Reading In Data"
    ]
  },
  {
    "objectID": "Reading_In_Data.html#csv-files",
    "href": "Reading_In_Data.html#csv-files",
    "title": "Reading In Data",
    "section": "",
    "text": "read.csv( )\nWe can write part of our code to read in a CSV file using the read.csv() function. The read.csv() function takes in a file path as an argument. If you have a CSV file uploaded into the files section of RStudio, you can use the read.csv() function :\n\n# Read in the data\n\nReds2024_Another_Version &lt;- read.csv(\"./Reds-2024.csv\")\n\n# CAUTION : When creating variable names to use in your R script,\n# make sure you use an underscore and not a dash. When R sees a dash, \n# it can sometimes be interpreted as a minus sign and not a dash. \n# This can cause errors in your code.\n\n# You can try this by removing the comment tag below :\n\n# Reds-2024 &lt;- read.csv(\"Reds-2024.csv\")\n\n\nWe can now see the new version of the data in the Environment tab :\n\n\n\n\nread.csv() is a base R function. That means it is already loaded up when you start RStudio. You do not need to load any packages to use it.\nHowever, there is a more powerful function called read_csv() that is part of the readr package. It can read in data faster and it can also read in data that is not in a CSV format, but we will stick with the CSV format for now.\n\n\nread_csv( )\nOne of the differences between read.csv( ) and read_csv( ) is that read_csv( ) is part of the readr package. This means you will need to load the readr package to use it. This is part of the tidyverse package. If you do not have tidyverse installed, make sure you go ahead and install it. Once tidyverse is install, you can load up the readr package. \nIf you are unsure if the readr package is loaded, you can look at the Packages tab in RStudio to see if the readr package is loaded. If it is not, you can load it by clicking on the checkbox next to the package name.\n\n\n\n\nIf needed, you can also put this into your script so the user will have it loaded up for them. You can load the readr package by using the library() function :\n\n# Load the readr package\n\nlibrary(readr)\n\n\nYou can now use the read_csv() function to read in the data :\n\n# Read in the data\n\nReds2024_Version_3 &lt;- read_csv(\"Reds-2024.csv\")\n\nRows: 30 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Name\ndbl (4): Batting Average, Home Runs, Runs Scored, Stolen Bases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhy is read_csv better than read.csv? It is faster and it can read in data that is not in a CSV format. It can read in data that is in a TSV (Tab Separated Value) format, a DSV (Delimiter Separated Value) format, and a few others. It can also read in data that is in a fixed width format.\n Basically, when in doubt, use read_csv( ) instead of read.csv( ). \nNow that we have the data loaded into R, we can start to do some Elementary Data Analysis. We can use all the commands we discussed when going over Vectors, Data Frames, and Tibbles.\n\n# Look at the data\n\nhead(Reds2024_Version_3)\n\n# A tibble: 6 × 5\n  Name              `Batting Average` `Home Runs` `Runs Scored` `Stolen Bases`\n  &lt;chr&gt;                         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Elly De La Cruz               0.259          25           105             67\n2 Spencer Steer                 0.225          20            74             25\n3 Jonathan India                0.248          15            84             13\n4 Tyler Stephenson              0.258          19            69              1\n5 Jeimer Candelario             0.225          20            47              4\n6 Santiago Espinal              0.246           9            32             11\n\n# Look at the structure of the data\n\nclass(Reds2024_Version_3)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n# Look at the Stolen Bases category \n\nReds2024_Version_3$`Stolen Bases`\n\n [1] 67 25 13  1  4 11 20 16  9  9 13  1  1  2  0  0  3  2  2  2  0  0  0  5  0\n[26]  0  1  0  0  0\n\n\n\n\nUsing URL’s\nYou can also read in data from a URL. This is useful if you have a CSV file hosted on a website. You can use the read.csv() os read_csv() functions to read in the data from the URL. \nIf there is a direct URL to a CSV sheet, then you can use one of these commands to read in the data :\n\n# data_name &lt;- read.csv('URL')\n\ndata1 &lt;- read.csv('https://www.stats.govt.nz/assets/Uploads/Annual-enterprise-survey/Annual-enterprise-survey-2020-financial-year-provisional/Download-data/annual-enterprise-survey-2020-financial-year-provisional-csv.csv')\n\nhead(data1,2)\n\n  Year Industry_aggregation_NZSIOC Industry_code_NZSIOC Industry_name_NZSIOC\n1 2020                     Level 1                99999       All industries\n2 2020                     Level 1                99999       All industries\n               Units Variable_code\n1 Dollars (millions)           H01\n2 Dollars (millions)           H04\n                                    Variable_name     Variable_category   Value\n1                                    Total income Financial performance 733,258\n2 Sales, government funding, grants and subsidies Financial performance 660,630\n                                                                                            Industry_code_ANZSIC06\n1 ANZSIC06 divisions A-S (excluding classes K6330, L6711, O7552, O760, O771, O772, S9540, S9601, S9602, and S9603)\n2 ANZSIC06 divisions A-S (excluding classes K6330, L6711, O7552, O760, O771, O772, S9540, S9601, S9602, and S9603)\n\n# data_name &lt;- read_csv('URL')\ndata2 &lt;- read_csv('https://www.stats.govt.nz/assets/Uploads/Annual-enterprise-survey/Annual-enterprise-survey-2020-financial-year-provisional/Download-data/annual-enterprise-survey-2020-financial-year-provisional-csv.csv')\n\nRows: 37080 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): Industry_aggregation_NZSIOC, Industry_code_NZSIOC, Industry_name_NZ...\ndbl (1): Year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data2,2)\n\n# A tibble: 2 × 10\n   Year Industry_aggregation_N…¹ Industry_code_NZSIOC Industry_name_NZSIOC Units\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;                &lt;chr&gt;                &lt;chr&gt;\n1  2020 Level 1                  99999                All industries       Doll…\n2  2020 Level 1                  99999                All industries       Doll…\n# ℹ abbreviated name: ¹​Industry_aggregation_NZSIOC\n# ℹ 5 more variables: Variable_code &lt;chr&gt;, Variable_name &lt;chr&gt;,\n#   Variable_category &lt;chr&gt;, Value &lt;chr&gt;, Industry_code_ANZSIC06 &lt;chr&gt;\n\n\n\nNote : As of now, there is not an easy way to read in data from a Google Drive. You can try to use the googledrive package, but for our purposes, if we have a csv file on Google Drive, we will download the file and upload it as we did earlier.",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Reading In Data"
    ]
  },
  {
    "objectID": "Reading_In_Data.html#excel-files",
    "href": "Reading_In_Data.html#excel-files",
    "title": "Reading In Data",
    "section": "Excel Files",
    "text": "Excel Files\nWe can also read in data from an Excel file. We can use the readxl package to read in the data. If you do not have the readxl package installed, you can install it by using the install.packages() function :\n\n# Install the readxl package\n\n# install.packages(\"readxl\")\n\n\nYou can then load the readxl package by using the library() function :\n\n# Load the readxl package\n\nlibrary(readxl)\n\nYou can then use the read_excel() function to read in the data :\n\n# Read in the data\n\ndata3 &lt;- read_excel(\"Reds_1_Excel.xlsx\")\n\nhead(data3,2)\n\n# A tibble: 2 × 5\n  Name            `Batting Average` `Home Runs` `Runs Scored` `Stolen Bases`\n  &lt;chr&gt;                       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Elly De La Cruz             0.259          25           105             67\n2 Spencer Steer               0.225          20            74             25\n\n\n\nThis allows to now carry out some EDA on the variable data3. For example, if we wanted to sum up the number of home runs hit by the Reds in 2024, we could use the following code :\n\n# Sum up the number of home runs hit by the Reds in 2024\n\nsum(data3$`Home Runs`)\n\n[1] 174",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Reading In Data"
    ]
  },
  {
    "objectID": "Reading_In_Data.html#exercises",
    "href": "Reading_In_Data.html#exercises",
    "title": "Reading In Data",
    "section": "Exercises",
    "text": "Exercises\nWe will be working with a database from the Star Wars universe. We need to first install the package : \ninstall.packages(“starwarsdb”)\n\n# Response\n\n\nPlease make note that this is a different kind of way we can enter data into R. We have entered what is called a “database” which is different from a text file, a dataframe, a tibble, etc. \nWe will need to use the “dplyr” library to access this data, so install it if it is not already installed. \ninstall.packages(“dplyr”)\n\n# Response\n\n\ndplyr should now be installed, but may not be loaded. You can load the library by checking it off in the packages tab, or you can enter the following :\nlibrary(dplyr)\n\n# Response\n\n\nWe are now ready to create a dataframe using a combination of “dplyr” and the “as.data.frame” command. This is similar to the “as.tibble” command used above. Let’s store the information in the variable starwars_DF : \nstarwars_DF &lt;- as.data.frame(dplyr::starwars)\n\n# Response\n\n\nCheck the variable to make sure it is a dataframe.\nclass(starwars_DF)\n\n# Response\n\n\nNow we are ready to start answering questions.\n\nUse two different methods for finding the number of rows in your data frame. What does this represent?\n\n\n# Response\n\n\n\nUse two different methods for finding the number of columns in your data frame. What does this represent?\n\n\n# Response\n\n\n\nDetermine the dimensions of the dataframe using the dim command. What does this tell us?\n\n\n# Response\n\n\n\nWhat command can we use to list off the variable names in the dataframe? Carry out that command.\n\n\n# Response\n\n\n\nWe want to pull out the “height” variable. Carry out two different commands that will pull out this information and save it to a new variable called SW_height. How many values are listed? Does this raise any concerns?\n\n\n# Response\n\n\n\nDo all of the observations have a value for height? If not, how many are missing and who are they missing from?\n\n\n# Response\n\n\n\nPrint out the heights from smallest to largest. Also save the sorted list to a new variable called SW_height_sorted_1\n\n\n# Response\n\n\n\nUse the sort help file to see how to sort the values from largest to smallest. Then print out the heights from largest to smallest and save new sorted list to SW_height_sorted_2\n\n\n# Response\n\n\n\nFind two ways to find the maximum values of the height. Save this value to the variable SW_max_height.\n\n\n# Response\n\n\n\nFind two ways to find the minimum values of the height. Save this value to the variable SW_max_height.\n\n\n# Response\n\n\n\nTurn this dataframe into a tibble named SW_tibble.\n\n\n# Response",
    "crumbs": [
      "Step 1 - Data Collecting",
      "Reading In Data"
    ]
  },
  {
    "objectID": "What_Is_Tidy_Data_PDF.html",
    "href": "What_Is_Tidy_Data_PDF.html",
    "title": "What is “Tidy Data”?",
    "section": "",
    "text": "Example\nConsider the following data:\n# If needed, install the tidyverse package\n\n# install.packages(\"tidyverse\")\n\n# If it is already installed, make sure it is loaded up to use :\n\n# Load the tidyverse package\n\nlibrary(tidyverse)\nLet’s create a data frame that is not tidy. We will create a data frame with three rows and four columns. The first column contains the names of three people, and the other columns contain data for the years 2010, 2011, and 2012. Each row represents a person, and each column represents their age during that year.\ndf &lt;- tibble(\n  name = c(\"John Smith\", \"Jane Doe\", \"Mary Johnson\"),\n  `2010` = c(25, 30, 35),\n  `2011` = c(26, 31, 36),\n  `2012` = c(27, 32, 37)\n)\n\ndf\n\n# A tibble: 3 × 4\n  name         `2010` `2011` `2012`\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 John Smith       25     26     27\n2 Jane Doe         30     31     32\n3 Mary Johnson     35     36     37\nIn this case, the data is not tidy because the years are spread across columns. In order for this to be considered “tidy” data, we would need to think about the data in a different way.\nThe variables we are using are name, year, and age. In order for the data to be tidy, we want each observation (row) to contain a name, a year, and the age. This tells us we would need to have a column for each of these variables. In this case, we would need to have a column for the name of the person, a column for the year that the data was collected, and a column for the age of the person the year the data was collected.\nHere is what the tidy data would look like if we ordered the data in a tidy format by name, year, and age:\ndf_tidy &lt;- tibble(\n  name = c(\"John Smith\", \"John Smith\", \"John Smith\", \"Jane Doe\", \"Jane Doe\", \n           \"Jane Doe\", \"Mary Johnson\", \"Mary Johnson\", \"Mary Johnson\"),\n  year = c(2010, 2011, 2012, 2010, 2011, 2012, 2010, 2011, 2012),\n  age = c(25, 26, 27, 30, 31, 32, 35, 36, 37)\n)\n\ndf_tidy\n\n# A tibble: 9 × 3\n  name          year   age\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 John Smith    2010    25\n2 John Smith    2011    26\n3 John Smith    2012    27\n4 Jane Doe      2010    30\n5 Jane Doe      2011    31\n6 Jane Doe      2012    32\n7 Mary Johnson  2010    35\n8 Mary Johnson  2011    36\n9 Mary Johnson  2012    37\nWe have now cleaned the data so that we can work with it in a tidy format.\nAs an example of how you could use the tools from the tidyverse package, you could use the pivot_longer( ) function from the tidyr package to convert the data from the original data frame to a tidy data frame. This is just an example to show you a more elegant way to convert the data to a tidy format. You will perform more advanced cleaning options as you learn more about the tidyverse packages.\ndf_tidy &lt;- df %&gt;% \n  pivot_longer(cols = -name, names_to = \"year\", values_to = \"value\")\n\ndf_tidy\n\n# A tibble: 9 × 3\n  name         year  value\n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt;\n1 John Smith   2010     25\n2 John Smith   2011     26\n3 John Smith   2012     27\n4 Jane Doe     2010     30\n5 Jane Doe     2011     31\n6 Jane Doe     2012     32\n7 Mary Johnson 2010     35\n8 Mary Johnson 2011     36\n9 Mary Johnson 2012     37\nNow the data is tidy because each variable forms a column, each observation forms a row, and each type of observational unit forms a table.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "What is \"Tidy Data\"?"
    ]
  },
  {
    "objectID": "What_Is_Tidy_Data_PDF.html#summary",
    "href": "What_Is_Tidy_Data_PDF.html#summary",
    "title": "What is “Tidy Data”?",
    "section": "Summary",
    "text": "Summary\nTidy data is a standard way of organizing data that makes it easy to work with. The tidyverse packages provide tools for working with tidy data, making it easier to analyze and visualize data.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "What is \"Tidy Data\"?"
    ]
  },
  {
    "objectID": "What_Is_Tidy_Data_PDF.html#exercises",
    "href": "What_Is_Tidy_Data_PDF.html#exercises",
    "title": "What is “Tidy Data”?",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will identify whether a given dataset is in tidy format. Each problem will present a dataset from a different background along with a brief description. Your task is to determine if the dataset is tidy. If it is not tidy, describe why and provide a tidy version of the dataset.\n\nProblem 1: Weather Data\nThe following dataset contains weather data for three cities.\n\n\n\n\n\n\n\n\nProblem 2: Student Grades\nThe following dataset contains grades for students in three subjects. \n\n\n\n\n\n\n\n\nProblem 3: Sales Data\nThe following dataset contains monthly sales data for different products.\n\n\n\n\n\n\n\nProblem 4: Patient Health Data\nThe following dataset contains health data for patients.\n\n\n\n\n\n\n\nProblem 5: Financial Data\nThe following dataset contains quarterly financial data for companies.\n\n\n\n\n\n\n\nProblem 6: Sports Statistics\nThe following dataset contains statistics for players in a sports team.\n\n\n\n\n\n\n\nProblem 7: Movie Ratings\nThe following dataset contains ratings for movies by different critics.\n\n\n\n\n\n\n\nProblem 8: Employee Salary Data\nThe following dataset contains salary data for employees in different departments.\n\n\n\n\n\n\n\nProblem 9: Product Reviews\nThe following dataset contains reviews for products.\n\n\n\n\n\n\n\nProblem 10: Course Enrollment Data\nThe following dataset contains enrollment data for courses.\n\n\n\n\n\n\n\nProblem 11: Sales Data\nThe following table shows the monthly sales data for three products.\n\n\n\n\n\n\n\nProblem 12: Survey Data\nThe following table represents the results of a survey where respondents rated their satisfaction with three services.\n\n\n\n\n\n\n\nProblem 13: Weather Data\nThe table below shows the temperature readings at different times of the day for a week.\n\n\n\n\n\n\n\nProblem 14: Exam Scores\nThe following table lists the scores of students in three subjects.\n\n\n\n\n\n\n\nProblem 15: Hospital Data\nThe table below shows the number of patients admitted to different wards of a hospital over three months.\n\n\n\n\n\n\n\nProblem 16: Marketing Data\nThe following table represents the results of a marketing campaign showing the number of leads generated from different channels.\n\n\n\n\n\n\n\nProblem 17: Fitness Data\nThe table below shows the workouts completed by three athletes over a week.\n\n\n\n\n\n\n\nProblem 18: Financial Data\nThe following table shows the quarterly profits for three companies.\n\n\n\n\n\n\n\nProblem 19: Attendance Data\nThe table below shows the attendance numbers for different events over three days.\n\n\n\n\n\n\n\nProblem 20: Production Data\nThe following table represents the production output of different products over three shifts.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "What is \"Tidy Data\"?"
    ]
  },
  {
    "objectID": "Vectors_DataFrames_Tibbles.html",
    "href": "Vectors_DataFrames_Tibbles.html",
    "title": "Vectors, Data Frames, Tibbles",
    "section": "",
    "text": "Vectors\nA vector is a 1-dimensional (row) structure that can hold multiple elements. For example, we can create a vector that contains 10 numbers such as this one :\nThis example is a 1-dimensional vector that holds 10 values. You can see the values that we have saved are 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100. The index tells us what position each value is in. The first value is at index 1, the second value is at index 2, and so on.\nWhen creating a new vector, you want to make sure you are giving it a name that makes sense. For example, if you are creating a vector that holds the scores from a test, you might want to name it test_scores. This will help you remember what the vector is for when you are working with it later.\nIn order to create a vector, we can use the c() function. This function stands for “combine” and is used to combine multiple values into a single vector. For example, if we wanted to create a vector that holds the numbers from above, we could do it like this:\ntest_scores &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n\nclass(test_scores)\n\n[1] \"numeric\"\nNote : I used the class() function to check the type of the object. The output of this command is numeric. This tells us that the object is a numeric vector. This is because all of the values in the vector are numbers.\nWe can access the values by using the index. For example, if we wanted to access the first value in the vector, we could do it like this: test_scores[1]. This would return the value 10.\ntest_scores[1]\n\n[1] 10\nIf I wanted to examine the 4th - 7th entries, I could do it like this: test_scores[4:7]. This would return the values 40, 50, 60, 70.\ntest_scores[4:7]\n\n[1] 40 50 60 70\nNote that we can’t just pick random locations in the vector. For example, if I wanted to print out the values in the 2nd, 5th, and 9th locations? The command test_scores[2, 5, 9] would not work. You would get an error message that stops your script.\nInstead, we could create a vector that holds the locations we want to examine. For example, we could create a vector that holds the values 2, 5, 9 and use this vector in our command :\ntest_scores[c(2, 5, 9)]\n\n[1] 20 50 90\nYou can also create a vector that holds text values. For example, you could create a vector that holds the names of the students in a class. This would look something like this:\nstudent_names &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Hannah\", \"Ivy\", \"Jack\")\n\nclass(student_names)\n\n[1] \"character\"\nNotice that this vector is of type character. This is because all of the values in the vector are text values. You can access the values in the same way as you would with a numeric vector. For example, if you wanted to access the first value in the vector, you could do it like this: student_names[1].\nstudent_names[1]\n\n[1] \"Alice\"\nWe have seen two examples. The first was a vector that contained numeric values and the second was a vector that contained text values. You can also create a vector that contains a mix of both. For example, you could create a vector that contains the names of the students in a class along with their test scores. This would look something like this:\nstudent_data &lt;- c(\"Alice\", 100, \"Bob\", 90, \"Charlie\", 80, \"David\", 70, \"Eve\", 60, \"Frank\", 50, \"Grace\", 40, \"Hannah\", 30, \"Ivy\", 20, \"Jack\", 10)\nWhen you create a vector that contains a mix of text and numeric values, you need to be careful.\nWhen you create a vector that contains a mix of text and numeric values, R will convert the text values to a different type. For example, if you have a vector that contains both text and numeric values, the text values will be converted to a type called character. This is a type that is used to store text values. For example, if you have a vector that contains the values 10, 20, 30, \"Alice\", \"Bob\", the text values \"Alice\" and \"Bob\" will be converted to the type character. This is because R can’t store text values in a vector that is meant to hold numeric values.\nclass(student_data)\n\n[1] \"character\"\nIf a vector contains text and numeric values, the text values will be converted to a different type. For example, if you have a vector that contains both text and numeric values, the text values will be converted to a type called character. This is a type that is used to store text values. For example, if you have a vector that contains the values 10, 20, 30, \"Alice\", \"Bob\", the text values \"Alice\" and \"Bob\" will be converted to the type character. This is because R can’t store text values in a vector that is meant to hold numeric values.\nThis can be problematic. For instance, what if we wanted to find the average of the test scores from the student_data vector? We would need to convert the text values to numeric values first. We can do this using the as.numeric() function. This function converts a vector to a numeric type. For example, if we wanted to convert the student_data vector to a numeric type, we could do it like this:\nstudent_data &lt;- as.numeric(student_data)\n\nWarning: NAs introduced by coercion\n\nstudent_data\n\n [1]  NA 100  NA  90  NA  80  NA  70  NA  60  NA  50  NA  40  NA  30  NA  20  NA\n[20]  10\nThis is interesting because as we see the output, we get NA values. This is because the as.numeric() function can’t convert text values to numeric values. When it encounters a text value, it returns NA. This is a special value that is used to represent missing or undefined values. In this case, it is used to represent the text values that couldn’t be converted to numeric values.\nSo if we wanted to then find the average of the test scores, we could pull out the score sand save them into another vector. The test scores are in indices 2, 3, 6, 8, 10, 12, 14, 16, 18 and 20. We could save these into a new vector called test_scores2 and then find the average like this:\ntest_scores2 &lt;- student_data[c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)]\nLet’s check the vector to make sure it is correct :\ntest_scores2\n\n [1] 100  90  80  70  60  50  40  30  20  10\nWe can now find the average using the mean() function. This function calculates the average of a vector. For example, if we wanted to find the average of the test scores, we could do it like this:\nmean(test_scores2)\n\n[1] 55\nThe reason why a vector is not useful here is because it is trying to save two different forms of data at once. We are trying to save text and numeric data in the same vector. This is not a good idea. Surely there is an easier way to store the data! That is where data frames come in.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Vectors, Data Frames, Tibbles"
    ]
  },
  {
    "objectID": "Vectors_DataFrames_Tibbles.html#data-frames",
    "href": "Vectors_DataFrames_Tibbles.html#data-frames",
    "title": "Vectors, Data Frames, Tibbles",
    "section": "Data Frames",
    "text": "Data Frames\nA data frame is a 2-dimensional (row and column) structure that can hold multiple elements. It is similar to a matrix, but it can hold different types of data in each column. For example, you could create a data frame that holds the names of the students in a class along with their test scores. This would look something like this:\n\n\n\n\nThis example is a 2-dimensional data frame that holds 10 rows and 2 columns. This is an example of a data frame. In a data frame, each row is called an observation and each column is called a variable. This data frame has two variables: name and test_score. The name variable holds the names of the students in the class, while the test_score variable holds the test scores of the students. \nWhen creating a new data frame, you want to make sure you are giving it a name that makes sense. For example, if you are creating a data frame that holds the names of the students in a class along with their test scores, you might want to name it student_data. This will help you remember what the data frame is for when you are working with it later. \nIn order to create a data frame, we can use the data.frame() function. This function is used to create a new data frame. For example, if we wanted to create a data frame that holds the names of the students in a class along with their test scores, we could do it like this:\n\nstudent_data &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Hannah\", \"Ivy\", \"Jack\"),\n  test_score = c(100, 90, 80, 70, 60, 50, 40, 30, 20, 10)\n)\n\nclass(student_data)\n\n[1] \"data.frame\"\n\n\nIn this example, we are creating a data one column at a time. Since we named these columns as name and test_score, we can access the values in the same way as we would with a vector. For example, if we wanted to access the first value in the name column, we could do it like this: student_data$name[1]. This would return the value Alice.\n\nstudent_data$name[1]\n\n[1] \"Alice\"\n\n\nSimilarly, if we wanted to access the first value in the test_score column, we could do it like this: student_data$test_score[1]. This would return the value 100.\n\nstudent_data$test_score[1]\n\n[1] 100\n\n\nWe could also the value in the fifth row and second column like this:\n\nstudent_data[5, 2]\n\n[1] 60\n\n\nIf we wanted to print out the entire first column, we could do it like this:\n\nstudent_data$name\n\n [1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"     \"Frank\"   \"Grace\"  \n [8] \"Hannah\"  \"Ivy\"     \"Jack\"   \n\n\nIf we wanted to print out the entire second column, we could do it like this:\n\nstudent_data$test_score\n\n [1] 100  90  80  70  60  50  40  30  20  10\n\n\nWhat happens if we wanted to print out the entore data frame? We could do it like this:\n\nstudent_data\n\n      name test_score\n1    Alice        100\n2      Bob         90\n3  Charlie         80\n4    David         70\n5      Eve         60\n6    Frank         50\n7    Grace         40\n8   Hannah         30\n9      Ivy         20\n10    Jack         10\n\n\nThese commands are nice, but if you have a large data set then just printing it off can be a bit overwhelming. We can use the head() function to print out the first few rows of the data frame. For example, if we wanted to print out the first 3 rows of the data frame, we could do it like this:\n\nhead(student_data, 3)\n\n     name test_score\n1   Alice        100\n2     Bob         90\n3 Charlie         80\n\n\nIf we wanted to print out the last few rows of the data frame, we could use the tail() function. For example, if we wanted to print out the last 5 rows\n\ntail(student_data, 5)\n\n     name test_score\n6   Frank         50\n7   Grace         40\n8  Hannah         30\n9     Ivy         20\n10   Jack         10\n\n\nThe default amount of lines for head() and tail() is 6. If you don’t specify an amount of lines, it will print out 6 lines.\n\nhead(student_data)\n\n     name test_score\n1   Alice        100\n2     Bob         90\n3 Charlie         80\n4   David         70\n5     Eve         60\n6   Frank         50\n\n\nHow is a data frame different from a tibble? Let’s talk about that next.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Vectors, Data Frames, Tibbles"
    ]
  },
  {
    "objectID": "Vectors_DataFrames_Tibbles.html#tibbles",
    "href": "Vectors_DataFrames_Tibbles.html#tibbles",
    "title": "Vectors, Data Frames, Tibbles",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble is a modern version of a data frame that is part of the tidyverse. It is similar to a data frame, but it has some additional features that make it easier to work with. For example, tibbles have a nicer print method that makes it easier to view the data. They also have some additional functions that make it easier to manipulate the data. For example, you can use the select() function to select specific columns from a tibble. You can also use the filter() function to filter rows based on a condition. These functions make it easier to work with tibbles than with data frames.\nIn order to create a tibble, we can use the tibble() function. This function is used to create a new tibble. For example, if we wanted to create a tibble that holds the names of the students in a class along with their test scores, we could do it like this:\n\n# Make sure tidyverse is installed and loaded up. Remember, if you\n# need to install it, use the following :\n\n# install.packages(tidyverse)\n\n\n\n# If it is already downloaded, then you just need to load up the library :\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# I have already installed the tibble package, so I will just load it up\n\nlibrary(tibble)\n\nstudent_data2 &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Hannah\", \"Ivy\", \"Jack\"),\n  test_score = c(100, 90, 80, 70, 60, 50, 40, 30, 20, 10)\n)\n\nclass(student_data2)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nIn this example, we are creating a tibble that is similar to the data frame we created earlier. The main difference is that this tibble is part of the tidyverse. This means that it has some additional features that make it easier to work with. For example, we can use the select() function to select specific columns from the tibble. For example, if we wanted to select the name column from the tibble, we could do it like this:\n\nselect(student_data2, name)\n\n# A tibble: 10 × 1\n   name   \n   &lt;chr&gt;  \n 1 Alice  \n 2 Bob    \n 3 Charlie\n 4 David  \n 5 Eve    \n 6 Frank  \n 7 Grace  \n 8 Hannah \n 9 Ivy    \n10 Jack   \n\n\nIf we wanted to select the test_score column from the tibble, we could do it like this:\n\nselect(student_data2, test_score)\n\n# A tibble: 10 × 1\n   test_score\n        &lt;dbl&gt;\n 1        100\n 2         90\n 3         80\n 4         70\n 5         60\n 6         50\n 7         40\n 8         30\n 9         20\n10         10\n\n\nIf I want all of the students that got higher than a 65 for their test score, I could use the filter() command to help us out. The filter() command needs two arguments. The first is the name of the tibble and the second is the condition that we want to filter on. For example, if we wanted to filter out all of the students that got higher than a 65 on their test, we could do it like this:\n\nfilter(student_data2, test_score &gt; 65)\n\n# A tibble: 4 × 2\n  name    test_score\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Alice          100\n2 Bob             90\n3 Charlie         80\n4 David           70\n\n\nAnother advantage a tibble has over a data frame is that it is easier to work with when you are working with large data sets. For example, if you have a data set that has 1 million rows, it can be difficult to work with a data frame. This is because data frames are stored in memory, and if you have a large data set, it can take up a lot of memory. This can slow down your computer and make it difficult to work with the data. Tibbles are designed to be more memory efficient than data frames. This means that they can handle larger data sets more easily. This makes it easier to work with large data sets in R.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Vectors, Data Frames, Tibbles"
    ]
  },
  {
    "objectID": "Vectors_DataFrames_Tibbles.html#conclusion",
    "href": "Vectors_DataFrames_Tibbles.html#conclusion",
    "title": "Vectors, Data Frames, Tibbles",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, vectors, data frames, and tibbles are all useful structures that can be used to store data. Vectors are 1-dimensional structures that can hold multiple elements. Data frames are 2-dimensional structures that can hold multiple elements. Tibbles are a modern version of data frames that are part of the tidyverse. They have some additional features that make them easier to work with. All of these structures are useful for storing data in R, and you will likely use all of them at some point when working with data in R.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Vectors, Data Frames, Tibbles"
    ]
  },
  {
    "objectID": "Vectors_DataFrames_Tibbles.html#exercises",
    "href": "Vectors_DataFrames_Tibbles.html#exercises",
    "title": "Vectors, Data Frames, Tibbles",
    "section": "Exercises",
    "text": "Exercises\nIn this part of the assignment, you will practice creating different types of vectors in R and using common commands associated with them, such as determining their class. Each problem will involve creating a unique type of vector and performing specific operations.\n\nProblem 1: Numeric Vector\nTask: Create a numeric vector containing the first 10 positive integers and determine its class.\nSteps: 1. Create a numeric vector num_vector containing the numbers 1 to 10. 2. Determine the class of the vector.\nCode Example:\n\n# Create numeric vector\nnum_vector &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Determine the class of the vector\nclass_num_vector &lt;- class(num_vector)\n\nnum_vector\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass_num_vector\n\n[1] \"numeric\"\n\n\n\n\nProblem 2: Character Vector\nTask: Create a character vector containing the names of the first five months of the year and determine its class.\nSteps: 1. Create a character vector char_vector containing the names “January”, “February”, “March”, “April”, and “May”. 2. Determine the class of the vector.\nCode Example:\n\n# Create character vector\nchar_vector &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\")\n\n# Determine the class of the vector\nclass_char_vector &lt;- class(char_vector)\n\nchar_vector\n\n[1] \"January\"  \"February\" \"March\"    \"April\"    \"May\"     \n\nclass_char_vector\n\n[1] \"character\"\n\n\n\n\nProblem 3: Logical Vector\nTask: Create a logical vector containing alternating TRUE and FALSE values for a length of 8 and determine its class.\nSteps: 1. Create a logical vector log_vector with alternating TRUE and FALSE values. 2. Determine the class of the vector.\nCode Example:\n\n# Create logical vector\nlog_vector &lt;- c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE)\n\n# Determine the class of the vector\nclass_log_vector &lt;- class(log_vector)\n\nlog_vector\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nclass_log_vector\n\n[1] \"logical\"\n\n\n\n\nProblem 4: Factor Vector\nTask: Create a factor vector from a character vector containing three levels: “High”, “Medium”, and “Low”, and determine its class.\nSteps: 1. Create a character vector char_levels with values “High”, “Medium”, “Low”, “High”, “Low”. 2. Convert this character vector into a factor vector fact_vector. 3. Determine the class of the factor vector.\nCode Example:\n\n# Create character vector with levels\nchar_levels &lt;- c(\"High\", \"Medium\", \"Low\", \"High\", \"Low\")\n\n# Convert to factor vector\nfact_vector &lt;- factor(char_levels, levels = c(\"Low\", \"Medium\", \"High\"))\n\n# Determine the class of the factor vector\nclass_fact_vector &lt;- class(fact_vector)\n\nfact_vector\n\n[1] High   Medium Low    High   Low   \nLevels: Low Medium High\n\nclass_fact_vector\n\n[1] \"factor\"\n\n\n\n\nProblem 5: Complex Vector\nTask: Create a complex vector containing the first four complex numbers with real and imaginary parts, and determine its class.\nSteps: 1. Create a complex vector comp_vector with values 1+2i, 3+4i, 5+6i, 7+8i. 2. Determine the class of the vector.\nCode Example:\n\n# Create complex vector\ncomp_vector &lt;- c(1+2i, 3+4i, 5+6i, 7+8i)\n\n# Determine the class of the vector\nclass_comp_vector &lt;- class(comp_vector)\n\ncomp_vector\n\n[1] 1+2i 3+4i 5+6i 7+8i\n\nclass_comp_vector\n\n[1] \"complex\"\n\n\nIn this part of the assignment, you will practice creating different types of data frames in R and using common commands associated with them, such as pulling out values, and using head() and tail() functions. Each problem will involve creating a unique type of data frame and performing specific operations.\n\n\nProblem 6: Creating a Data Frame for Students’ Scores\nTask: Create a data frame containing the names and scores of five students in a math test. Use commands to pull out specific values and display the first few rows of the data frame.\nSteps:\n\nCreate a data frame students_df with columns Name and Score.\nPull out the score of the student named “Alice”.\nDisplay the first three rows of the data frame using head().\n\nCode Example:\n\n# Create data frame\nstudents_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  Score = c(85, 90, 78, 92, 88)\n)\n\n# Pull out the score of Alice\nalice_score &lt;- students_df$Score[students_df$Name == \"Alice\"]\n\n# Display the first three rows of the data frame\nhead_students_df &lt;- head(students_df, 3)\n\nstudents_df\n\n     Name Score\n1   Alice    85\n2     Bob    90\n3 Charlie    78\n4   David    92\n5     Eve    88\n\nalice_score\n\n[1] 85\n\nhead_students_df\n\n     Name Score\n1   Alice    85\n2     Bob    90\n3 Charlie    78\n\n\n\n\nProblem 7: Creating a Data Frame for Monthly Expenses\nTask: Create a data frame containing the monthly expenses for three categories (Rent, Food, Utilities) over six months. Use commands to pull out specific values and display the last few rows of the data frame.\nSteps:\n\nCreate a data frame expenses_df with columns Month, Rent, Food, and Utilities.\nPull out the food expense for the month of March.\nDisplay the last two rows of the data frame using tail().\n\nCode Example:\n\n# Create data frame\nexpenses_df &lt;- data.frame(\n  Month = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\"),\n  Rent = c(1000, 1000, 1000, 1000, 1000, 1000),\n  Food = c(300, 320, 310, 330, 340, 350),\n  Utilities = c(150, 160, 155, 165, 170, 175)\n)\n\n# Pull out the food expense for March\nmarch_food_expense &lt;- expenses_df$Food[expenses_df$Month == \"March\"]\n\n# Display the last two rows of the data frame\ntail_expenses_df &lt;- tail(expenses_df, 2)\n\nexpenses_df\n\n     Month Rent Food Utilities\n1  January 1000  300       150\n2 February 1000  320       160\n3    March 1000  310       155\n4    April 1000  330       165\n5      May 1000  340       170\n6     June 1000  350       175\n\nmarch_food_expense\n\n[1] 310\n\ntail_expenses_df\n\n  Month Rent Food Utilities\n5   May 1000  340       170\n6  June 1000  350       175\n\n\n\n\nProblem 8: Creating a Data Frame for Employee Information\nTask: Create a data frame containing the employee information (ID, Name, Department, Salary). Use commands to pull out specific values and display the first few rows of the data frame.\nSteps:\n\nCreate a data frame employee_df with columns ID, Name, Department, and Salary.\nPull out the department of the employee with ID 3.\nDisplay the first four rows of the data frame using head().\n\nCode Example:\n\n# Create data frame\nemployee_df &lt;- data.frame(\n  ID = c(1, 2, 3, 4, 5),\n  Name = c(\"John\", \"Jane\", \"Doe\", \"Smith\", \"Emily\"),\n  Department = c(\"HR\", \"Finance\", \"IT\", \"Marketing\", \"Admin\"),\n  Salary = c(50000, 60000, 55000, 70000, 65000)\n)\n\n# Pull out the department of the employee with ID 3\ndept_id_3 &lt;- employee_df$Department[employee_df$ID == 3]\n\n# Display the first four rows of the data frame\nhead_employee_df &lt;- head(employee_df, 4)\n\nemployee_df\n\n  ID  Name Department Salary\n1  1  John         HR  50000\n2  2  Jane    Finance  60000\n3  3   Doe         IT  55000\n4  4 Smith  Marketing  70000\n5  5 Emily      Admin  65000\n\ndept_id_3\n\n[1] \"IT\"\n\nhead_employee_df\n\n  ID  Name Department Salary\n1  1  John         HR  50000\n2  2  Jane    Finance  60000\n3  3   Doe         IT  55000\n4  4 Smith  Marketing  70000\n\n\n\n\nProblem 9: Creating a Data Frame for Product Sales\nTask: Create a data frame containing the product sales information (ProductID, ProductName, UnitsSold, Revenue). Use commands to pull out specific values and display the last few rows of the data frame.\nSteps:\n\nCreate a data frame sales_df with columns ProductID, ProductName, UnitsSold, and Revenue.\nPull out the revenue of the product named “Tablet”.\nDisplay the last three rows of the data frame using tail().\n\nCode Example:\n\n# Create data frame\nsales_df &lt;- data.frame(\n  ProductID = c(101, 102, 103, 104, 105),\n  ProductName = c(\"Laptop\", \"Tablet\", \"Smartphone\", \"Desktop\", \"Monitor\"),\n  UnitsSold = c(50, 100, 200, 30, 80),\n  Revenue = c(50000, 30000, 40000, 15000, 20000)\n)\n\n# Pull out the revenue of the product named Tablet\ntablet_revenue &lt;- sales_df$Revenue[sales_df$ProductName == \"Tablet\"]\n\n# Display the last three rows of the data frame\ntail_sales_df &lt;- tail(sales_df, 3)\n\nsales_df\n\n  ProductID ProductName UnitsSold Revenue\n1       101      Laptop        50   50000\n2       102      Tablet       100   30000\n3       103  Smartphone       200   40000\n4       104     Desktop        30   15000\n5       105     Monitor        80   20000\n\ntablet_revenue\n\n[1] 30000\n\ntail_sales_df\n\n  ProductID ProductName UnitsSold Revenue\n3       103  Smartphone       200   40000\n4       104     Desktop        30   15000\n5       105     Monitor        80   20000\n\n\n\n\nProblem 10: Creating a Data Frame for Weather Data\nTask: Create a data frame containing the weather data (Day, Temperature, Humidity, WindSpeed). Use commands to pull out specific values and display the first and last few rows of the data frame.\nSteps:\n\nCreate a data frame weather_df with columns Day, Temperature, Humidity, and WindSpeed.\nPull out the temperature on the 5th day.\nDisplay the first two and last two rows of the data frame using head() and tail().\n\nCode Example:\n\n# Create data frame\nweather_df &lt;- data.frame(\n  Day = 1:10,\n  Temperature = c(25, 27, 24, 26, 28, 29, 30, 31, 32, 33),\n  Humidity = c(80, 82, 78, 76, 79, 81, 83, 85, 84, 86),\n  WindSpeed = c(10, 12, 11, 13, 14, 15, 16, 17, 18, 19)\n)\n\n# Pull out the temperature on the 5th day\ntemp_day_5 &lt;- weather_df$Temperature[weather_df$Day == 5]\n\n# Display the first two rows of the data frame\nhead_weather_df &lt;- head(weather_df, 2)\n\n# Display the last two rows of the data frame\ntail_weather_df &lt;- tail(weather_df, 2)\n\nweather_df\n\n   Day Temperature Humidity WindSpeed\n1    1          25       80        10\n2    2          27       82        12\n3    3          24       78        11\n4    4          26       76        13\n5    5          28       79        14\n6    6          29       81        15\n7    7          30       83        16\n8    8          31       85        17\n9    9          32       84        18\n10  10          33       86        19\n\ntemp_day_5\n\n[1] 28\n\nhead_weather_df\n\n  Day Temperature Humidity WindSpeed\n1   1          25       80        10\n2   2          27       82        12\n\ntail_weather_df\n\n   Day Temperature Humidity WindSpeed\n9    9          32       84        18\n10  10          33       86        19\n\n\n\nIn this part of the assignment, you will practice creating different types of tibbles in R and using common commands associated with them, such as creating a tibble, converting a data frame to a tibble, selecting columns, and filtering rows. Each problem will involve creating or manipulating tibbles and performing specific operations.\n\n\nProblem 11: Creating a Tibble for Students’ Scores\nTask: Create a tibble containing the names and scores of five students in a math test. Use commands to select specific columns and display the tibble.\nSteps:\n\nCreate a tibble students_tbl with columns Name and Score.\nSelect the Name column from the tibble.\nDisplay the entire tibble.\n\nCode Example:\n\n# Load the tibble package\nlibrary(tibble)\n\n# Create tibble\nstudents_tbl &lt;- tibble(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  Score = c(85, 90, 78, 92, 88)\n)\n\n# Select the Name column\nname_column &lt;- select(students_tbl, Name)\n\n# Display the tibble\nstudents_tbl\n\n# A tibble: 5 × 2\n  Name    Score\n  &lt;chr&gt;   &lt;dbl&gt;\n1 Alice      85\n2 Bob        90\n3 Charlie    78\n4 David      92\n5 Eve        88\n\nname_column\n\n# A tibble: 5 × 1\n  Name   \n  &lt;chr&gt;  \n1 Alice  \n2 Bob    \n3 Charlie\n4 David  \n5 Eve    \n\n\n\n\nProblem 12: Converting a Data Frame to a Tibble for Monthly Expenses\nTask: Create a data frame containing the monthly expenses for three categories (Rent, Food, Utilities) over six months and convert it to a tibble. Use commands to filter specific rows and display the tibble.\nSteps:\n\nCreate a data frame expenses_df with columns Month, Rent, Food, and Utilities.\nConvert the data frame to a tibble expenses_tbl.\nFilter the tibble to include only the rows where Rent is greater than 1000.\nDisplay the entire tibble.\n\nCode Example:\n\n# Create data frame\nexpenses_df &lt;- data.frame(\n  Month = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\"),\n  Rent = c(1000, 1000, 1000, 1000, 1000, 1000),\n  Food = c(300, 320, 310, 330, 340, 350),\n  Utilities = c(150, 160, 155, 165, 170, 175)\n)\n\n# Convert data frame to tibble\nexpenses_tbl &lt;- as_tibble(expenses_df)\n\n# Filter the tibble\nfiltered_expenses_tbl &lt;- filter(expenses_tbl, Rent &gt; 1000)\n\n# Display the tibble\nexpenses_tbl\n\n# A tibble: 6 × 4\n  Month     Rent  Food Utilities\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 January   1000   300       150\n2 February  1000   320       160\n3 March     1000   310       155\n4 April     1000   330       165\n5 May       1000   340       170\n6 June      1000   350       175\n\nfiltered_expenses_tbl\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Month &lt;chr&gt;, Rent &lt;dbl&gt;, Food &lt;dbl&gt;, Utilities &lt;dbl&gt;\n\n\n\n\nProblem 13: Creating a Tibble for Employee Information and Selecting Columns\nTask: Create a tibble containing the employee information (ID, Name, Department, Salary). Use commands to select specific columns and display the tibble.\nSteps:\n\nCreate a tibble employee_tbl with columns ID, Name, Department, and Salary.\nSelect the Name and Salary columns from the tibble.\nDisplay the entire tibble.\n\nCode Example:\n\n# Load the tibble package\nlibrary(tibble)\n\n# Create tibble\nemployee_tbl &lt;- tibble(\n  ID = c(1, 2, 3, 4, 5),\n  Name = c(\"John\", \"Jane\", \"Doe\", \"Smith\", \"Emily\"),\n  Department = c(\"HR\", \"Finance\", \"IT\", \"Marketing\", \"Admin\"),\n  Salary = c(50000, 60000, 55000, 70000, 65000)\n)\n\n# Select the Name and Salary columns\nname_salary_columns &lt;- select(employee_tbl, Name, Salary)\n\n# Display the tibble\nemployee_tbl\n\n# A tibble: 5 × 4\n     ID Name  Department Salary\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1     1 John  HR          50000\n2     2 Jane  Finance     60000\n3     3 Doe   IT          55000\n4     4 Smith Marketing   70000\n5     5 Emily Admin       65000\n\nname_salary_columns\n\n# A tibble: 5 × 2\n  Name  Salary\n  &lt;chr&gt;  &lt;dbl&gt;\n1 John   50000\n2 Jane   60000\n3 Doe    55000\n4 Smith  70000\n5 Emily  65000\n\n\n\n\nProblem 14: Creating a Tibble for Product Sales and Filtering Rows\nTask: Create a tibble containing the product sales information (ProductID, ProductName, UnitsSold, Revenue). Use commands to filter specific rows and display the tibble.\nSteps:\n\nCreate a tibble sales_tbl with columns ProductID, ProductName, UnitsSold, and Revenue.\nFilter the tibble to include only the rows where UnitsSold is greater than 50.\nDisplay the entire tibble.\n\nCode Example:\n\n# Load the tibble package\nlibrary(tibble)\n\n# Create tibble\nsales_tbl &lt;- tibble(\n  ProductID = c(101, 102, 103, 104, 105),\n  ProductName = c(\"Laptop\", \"Tablet\", \"Smartphone\", \"Desktop\", \"Monitor\"),\n  UnitsSold = c(50, 100, 200, 30, 80),\n  Revenue = c(50000, 30000, 40000, 15000, 20000)\n)\n\n# Filter the tibble\nfiltered_sales_tbl &lt;- filter(sales_tbl, UnitsSold &gt; 50)\n\n# Display the tibble\nsales_tbl\n\n# A tibble: 5 × 4\n  ProductID ProductName UnitsSold Revenue\n      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1       101 Laptop             50   50000\n2       102 Tablet            100   30000\n3       103 Smartphone        200   40000\n4       104 Desktop            30   15000\n5       105 Monitor            80   20000\n\nfiltered_sales_tbl\n\n# A tibble: 3 × 4\n  ProductID ProductName UnitsSold Revenue\n      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1       102 Tablet            100   30000\n2       103 Smartphone        200   40000\n3       105 Monitor            80   20000\n\n\n\n\nProblem 15: Creating a Tibble for Weather Data and Using head() and tail()\nTask: Create a tibble containing the weather data (Day, Temperature, Humidity, WindSpeed). Use commands to display the first and last few rows of the tibble.\nSteps:\n\nCreate a tibble weather_tbl with columns Day, Temperature, Humidity, and WindSpeed.\nDisplay the first three rows of the tibble using head().\nDisplay the last three rows of the tibble using tail().\n\nCode Example:\n\n# Load the tibble package\nlibrary(tibble)\n\n# Create tibble\nweather_tbl &lt;- tibble(\n  Day = 1:10,\n  Temperature = c(25, 27, 24, 26, 28, 29, 30, 31, 32, 33),\n  Humidity = c(80, 82, 78, 76, 79, 81, 83, 85, 84, 86),\n  WindSpeed = c(10, 12, 11, 13, 14, 15, 16, 17, 18, 19)\n)\n\n# Display the first three rows of the tibble\nhead_weather_tbl &lt;- head(weather_tbl, 3)\n\n# Display the last three rows of the tibble\ntail_weather_tbl &lt;- tail(weather_tbl, 3)\n\nweather_tbl\n\n# A tibble: 10 × 4\n     Day Temperature Humidity WindSpeed\n   &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     1          25       80        10\n 2     2          27       82        12\n 3     3          24       78        11\n 4     4          26       76        13\n 5     5          28       79        14\n 6     6          29       81        15\n 7     7          30       83        16\n 8     8          31       85        17\n 9     9          32       84        18\n10    10          33       86        19\n\nhead_weather_tbl\n\n# A tibble: 3 × 4\n    Day Temperature Humidity WindSpeed\n  &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1          25       80        10\n2     2          27       82        12\n3     3          24       78        11\n\ntail_weather_tbl\n\n# A tibble: 3 × 4\n    Day Temperature Humidity WindSpeed\n  &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     8          31       85        17\n2     9          32       84        18\n3    10          33       86        19",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Vectors, Data Frames, Tibbles"
    ]
  },
  {
    "objectID": "Intro_To_R.html",
    "href": "Intro_To_R.html",
    "title": "Intro to R",
    "section": "",
    "text": "Comments\nComments are a very important part of coding. When you are writing code, you will want to leave notes to yourself and your collaborators to describe what you are doing at each step. You can also leave notes on parts of the code that are norking or that you feel should be changed. It is a way to remind yourself and your collaborators the work that has been done, why it was done, what is broken, other changes you want to make, and more. They are especially important when you come back to the code after not having looked at it for a while.\nBasically, leave as many comments as possible while coding. You should start with the names of those that are working on the project with a synopsis on what is the purpose of the project.\nA comment is any text following a hashtag (#) on the same line. This text is ignored by the R compiler and does not affect how the script is run. This is an example of how you should start every script you write in this class or at your job.\n# Mike LeVan\n# Data-1004 Data Analytics and Statistics\n# Date\n\n# Assignment Description\n\n# I can write out notes to myself and my colleagues!\n\n# R ignores all the lines that start with #",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Intro to R"
    ]
  },
  {
    "objectID": "Intro_To_R.html#calculator",
    "href": "Intro_To_R.html#calculator",
    "title": "Intro to R",
    "section": "Calculator",
    "text": "Calculator\n While this is quite a bit of overkill, R can be used as a basic calculator. Here are the operators :\n\nAddition ( + )\n\n# This is an example of addition.\n\n4 + 8\n\n[1] 12\n\n\nNotice the output above :\n[1] 12\nThe [1] means the first output followed the by value of the output 12. \n\n\nSubtraction ( - )\n\n# Here is an example of subtraction.\n\n5 - 14\n\n[1] -9\n\n\n\n\n\nMultiplication ( * )\n\n# Here is an example of multiplication\n\n8 * 17\n\n[1] 136\n\n\n\n\n\nDivision ( / )\n\n# Division\n\n22 / 7\n\n[1] 3.142857\n\n\n\n\n\nExponentiation ( ^ )\n\n# Exponentiation\n\n5^3\n\n[1] 125\n\n\n\n\n\nSquare Roots and Radicals\nRecall that we can use exponents to calculate radicals, too. If you recall, the square root function is the same as raising a value to the (1 / 2) power!\n\n# Here is the square root of 9\n\n9^(1/2)\n\n[1] 3\n\n\n\nNotice that there are some levels of estimation / rounding here. For example, calculate the square root of 2\n\n2^(1/2)\n\n[1] 1.414214\n\n\n\nObviously the real answer goes further than six decimal places. Also think about the square root of 2 multiplied by itself. We should get the value 2 right back :\n\n2^(1/2) * 2^(1/2)\n\n[1] 2\n\n\n\nBut notice what happens if we then subtract 2 from the previous result :\n\n2^(1/2) * 2^(1/2) - 2\n\n[1] 4.440892e-16\n\n\n This result is in scientific notation, but what does it mean? When you see the “e-16” part, that says to move the decimal places 16 spots to the left, so this answer is closer to 0.000000000000000440892. Notice that it is not zero, as it should be. This is because of the estimation that was talked about above. \n\n\nCreating a variable\n What is a variable? Imagine dumping some data in a bucket and then giving that bucket a name. Now, every time you use that name, you are really referring to what is in the bucket. \nNOTE 1 : Recall our discussion from class about naming your variables. You want to pick a name for your variables that make sense. It will make editing your script much easier in the long run. If I called a variable “Quiz_Scores” then we know what types of values we are working with. If I called the variable “x” instead, what does that tell us about the variable itself? \nWe can use an “arrow” to assign a value into a variable. An arrow is just a less than sign followed by a dash with no spaces between them : \n\n&lt;-\n\n\nFor example, what if I wanted to assign the value 3 into a variable called “x” and the value 7 into a variable called “y”, we could type this into our R script :\n\n# Assign 3 to the variable \"x\"\n\nx &lt;- 3\n\n# Assign 7 to the variable \"y\"\n\ny &lt;- 7\n\n\nNOTE 2 : At this point you should look at the ENVIRONMENT window. This window shows you the variables you are using and the values they contain.\nNow that we have values in these variables, we can now use them in our script. For example, what if I wanted to perform some basic calculations with these variables :\n\n\nAddition of two variables : x + y\n\n# We are going to calculate x + y. \n\n# Recall that x = 3 and y = 7 from above, so this should return 10\n\nx + y\n\n[1] 10\n\n\n\nWe could do the same for several different operations :\n\n# Example 1 : Subtract two variables :\n\ny - x\n\n[1] 4\n\n# Example 2 : Multiply a variable by a constant. We could multiply the variable x by 9 to\n# get 9 * 3 = 27\n\n9*x\n\n[1] 27\n\n# Example 3 : Take a linear combination of two different variables. For example, we can\n# multiply your by 2 and x by 3 and add them together to get \n# 2*7 + 3*3 = 13 + 9 = 23 \n\n2*y + 3*x\n\n[1] 23\n\n# Example 4 : Use one variable as an exponent for another variable. In this case, take the \n# variable x and raise it to the power y. In this case, we are computing 3 ^ 7\n# which is 3 * 3 * 3 * 3 * 3 * 3 * 3 = 2187:\n\nx^y\n\n[1] 2187\n\n\n\n\n\nAssigning Operations to Variables\n\nWe could also take the result from an operation and assign that to a different variable. For exampe, we could multiply x by 7 and multiply y by 2, subtract them and store the result in a new variable z.\n\nz &lt;- 7*x - 2*y\n\n\nIf you want to print out what is now in the variable z, you can now see it in the ENVIRONMENT window. You can also type it out in the script : \n\n# Print out the variable z\n\nz\n\n[1] 7\n\n\n\nRecall from class that there are different kinds of variables we might be asked to consider. We could have QUANTITATIVE data that could be in the form of continuous or discrete values. Another kind of data we talked about was CATEGORICAL data. Depending on the type of data you are analyzing, you would need to perform different operations.\n\n\nCreating vectors\nLet’s assume the class takes a quiz and I want to keep track of them. Instead of creating a variable for each individual quiz, I can create one vector that will have all of the scores. \nFor example, if I have the quiz scores 10, 5, 8, 9, 4 then I could create the variables Student_1_Quiz, Student_2_Quiz, etc. It is much easier to create a single variable that holds all of these values. \nWe will use the following command : c( ) \nThe “c” is shorthand for “concatenate” which means to link objects together in a chain or series. \nIf I wanted to create a vector for the quiz scores above, I would create a vector called Quiz1_Scores and assign the variables as follows : \nNotice the order is important. If we want to assign the first student a 10, the second a 5, the third an 8, the fourth a 9, and the fifth a 4, then we would do the following :\n\n# Create a vector and call it \"Quiz1_Scores\" \n\nQuiz1_Scores &lt;- c(10, 5, 8, 9 ,4)\n\n\nNotice how this is represented in the ENVIRONMENT window :\n\n# We can print out the variable to check what it contains\n\nQuiz1_Scores     \n\n[1] 10  5  8  9  4\n\n\nThis tells us the scores are located in spots 1, 2, 3, 4, 5 in the vector. In this vector, the values in the spots are then shown as the values : 10 5 8 9 4 \nLet’s say Student 3 comes to see us and ask about their quiz grade. We can then access the individual value as follows :\n\nQuiz1_Scores[3]\n\n[1] 8\n\n\n\nIf we wanted the fifth value in the vector we could say :\n\nQuiz1_Scores[5]\n\n[1] 4\n\n\n\nIf we wanted to print out the 3rd, 4th, and 5th scores, we could say :\n\nQuiz1_Scores[3:5]\n\n[1] 8 9 4\n\n\n\nObviously we would want to make sure we enter in the data in an appropriate order because if I rearrange the order I get a completely different vector :\n\nQuiz1_Scores_B &lt;- c(5, 10, 4, 9, 8)\n\n\nWhile I have the same scores, they are located in different spots of the vectors and would assign different values to Student 1, Student 2, etc. \nWe could also use characters in our vectors. We would need to make sure we use quotation marks so the compiler does not think we are using other variables in our vector.\n\nStudents &lt;- c(\"Alice\", \"Bob\", \"Chad\", \"Debbie\", \"Eric\")\n\nCheck out what is now in the ENVIRONMENT window :\n\n# Print out the Students vector :\n\nStudents\n\n[1] \"Alice\"  \"Bob\"    \"Chad\"   \"Debbie\" \"Eric\"  \n\n\n\nThe only real difference from above is the we are using characters instead of numeric values and that is identified because of the “chr” notation in the description. \nWe can look at the individual entries jsut as we did above. To look at the fourth entry in the vector we would type :\n\nStudents[4]\n\n[1] \"Debbie\"\n\n\n\nIf you are given a variable or vector and want to know what type of values it contains, you can use the “class” command to tell you. Here are some examples to check entire vectors or individual locations in a vector : \n\nclass(Quiz1_Scores)\n\n[1] \"numeric\"\n\nclass(Quiz1_Scores[2])\n\n[1] \"numeric\"\n\nclass(Students)\n\n[1] \"character\"\n\nclass(Students[1])\n\n[1] \"character\"\n\nclass(Students)\n\n[1] \"character\"\n\n\n\nWhat happens if we mix our variables and have numbers and characters in the same vector? \nHere is how one could be created :\n\nblah &lt;- c(4, \"dffdg\", 6, 9, \"trte\")\n\n\nHow does R interpret these values?\n\nclass(blah)\n\n[1] \"character\"\n\n\n\nNotice that it considers EVERY entry in this vector to be a CHARACTER even though we entered some as numbers. Be careful with this as it could cause issues on how we work and interact with this vector.\n\n\nLibraries and Packages\n\nWhen we start up a session of R, there are some commands that are already built into the program that we can use. For instance, we used the basic mathematics operations above. \nYou will eventually want to do a deeper analysis of the data that needs a command that is not already installed in your current session of R. This is where the idea of Libraries or Packages comes into play. \nIf there is a command we want to use that is not currently loaded into R, we can install the package that includes the command. \nYou can see what is loaded already by clicking on the PACKAGES tab. \nYou will see the packages that are loaded up as they will have a check mark indicated they have been installed. \nLet’s say there was something in the “tcltk” library I wanted to use. I could then click on the check box for “tcltk” and a message should come up in the Console showing that the package was installed.\n&gt; library(tcltk, lib.loc = “/opt/R/4.3.1/lib/R/library”) \nWe could remove the package by unclicking on the check box. We get a confirmation in the Console :\n&gt; detach(“package:tcltk”, unload = TRUE) \nWhat happens if we need a package that is not included in the list. There are hundreds of packages that people have developed to use in R. \nFor example, consider the tidyverse library. This is a library that contains several commands that we will be using over the semester. \nIf we know we are going to be using a specific library in our R script, then we should install it at the top of the script. We would enter in a command such as :  install.packages(“tidyverse”) \nOnce we do this, you will see several different commands that are now available for us to use to analyze our data set. \nIf you look at the Packages tab, you will see several new packages that we can add to use in the program. \ntidyverse comes with several packages. If you click on the tidyverse package, you will see several packages installed :\n&gt; library(tidyverse)\n── Attaching core tidyverse packages ────────────────────── tidyverse 2.0.0 ──\n✔ dplyr 1.1.2 ✔ readr 2.1.4\n✔ forcats 1.0.0 ✔ stringr 1.5.0\n✔ ggplot2 3.4.2 ✔ tibble 3.2.1\n✔ lubridate 1.9.2 ✔ tidyr 1.3.0\n✔ purrr 1.0.1\n\nThese packages are now loaded into R. Note that is we uncheck the tidyverse package, these are still loaded into R. We can remove them by unchecking their package. For example, if I uncheck the ggplot2 package you will see the following message : \n&gt; detach(\"package:ggplot2\", unload = TRUE) \nWhen you are starting to become a programmer it might be tempting just to load up EVERYTING, but that is not good practice. When these packages are loaded up, they are taking up memory. This can lead to slower computation times as well as lead to larger files being generated, wasting space. Try to be efficient and load up what you need and avoid bloat.\n\n\nFunctions\n\nThere are two kinds of functions we are going to deal with in class - the ones that are built into R and ones we create ourselves. This lesson will only consider the functions that are built in. \nA FUNCTION is a command that takes in some kind of data, manipulates it, and returns a value. It has the following form : \nFUNCTION_NAME( data ) \nThis will simply print out the result. Remember we could also assign the result to a variable.\nresult &lt;- FUNCTION_NAME ( data)\nFor example, go back to the quiz grades we had listed earlier. What if I wanted to calculated the average (mean) of the quiz scores? I could enter the following :\n\nmean(Quiz1_Scores)\n\n[1] 7.2\n\n\n\nI could assign the result to a variable, such as :\n\n# Calculate the average and store the result in the variable \n# named \"Quiz1_Average\"\n\nQuiz1_Average &lt;- mean(Quiz1_Scores)\n\n# Print out the average\n\nQuiz1_Average\n\n[1] 7.2\n\n\n\nThere are several other built in functions. Here are a few : \nmin( ), max( ), mean( ), median( ), sum( ), range( ), abs( ) \nIf we wanted the highest quiz grade, we could say :\n\n# Find the maximum value :\n\nmax(Quiz1_Scores)\n\n[1] 10\n\n\n\nIf I wanted to know how many values are in my vector, I could say :\n\n# Use the length function :\n\nlength(Quiz1_Scores)\n\n[1] 5\n\n\n\nIf I wanted to pick a random number from 1 - 100, I could type the following\n\nsample(100,1)\n\n[1] 79\n\n\n\nIf I wanted to pick three random numbers (all different) from 1 - 100, we could say this :\n\nsample(100,3)\n\n[1] 72 55  5\n\n\n\nIf I wanted to pick seven random numbers from 1 - 100 where we could have (but not guarantee) duplicate values, I could say this :\n\nsample(100, 7, replace=TRUE)\n\n[1] 46 48 70 84 85 12 94\n\n\n\nTo create a random vector for a range of values, we can use sample function. We just need to pass the range and the sample size inside the sample function. \nFor example, if we want to create a random sample of size 20 for a range of values between 1 to 100 then we can use the command sample(1:100,20) and if the sample size is larger than 100 then we can add replace=TRUE as shown in the below examples.\n\n# Create random sample of 20 values from 1 - 100\n\nx1 &lt;- sample(1:100,20)\n\n# Print out the result\n\nx1\n\n [1]  69  45  60  98  84   9  36  61  79  51  57  59  46   8  44 100  87  32  62\n[20]  89\n\n\n\n\n# Create a sample of 200 values from 1 - 100. Obviously there will be repeats!\n\nx2 &lt;- sample(1:100,200,replace=TRUE)\n\n# Print out the sample\n\nx2\n\n  [1]  37  28  57  16  19  90  31  62  77  39  26  34  12  28  55  38  46  99\n [19]  90  73  93  45   8  60  79  36  43  40  82  42  26  27  42  86  45  67\n [37]  13   7   1  44  40  28  68  92  54  89  25  18  95  97  48  83  24  50\n [55]  54  49  62  23  65  93  42  84  42  45  92   3  52  57  79  39  77  27\n [73]  59  22   4  97  68  52  74  71  15  90  92  37  50  92  33  64  54  29\n [91]  70  80  10  66  59  24   6  73  32  17  32  14  45  12  76   8  21  53\n[109]   2  19  59  11  33   3  83  37  45  22  61  78  85  89  21  38  67  38\n[127]   2  61  24  17  92   6  73  84  21   9  95  88  20  26  85  47  85  59\n[145]  88   6  52  71  52  44  93  11  54  26  79  19  60  42  86  68  30  68\n[163]  79  84  16  30  45   4  69  22   6  30  52  40  74  54  96  88  31  78\n[181]  92  22  37  13  49  80  54  43 100  76  72  42  99  56  37   5  91  30\n[199]   2  21\n\n\n\nThere are far too many built in functions to list. You may have to use a book or The Google to help you find an appropriate one to use. \nLastly, make sure that the data you enter into the function makes sense. For example, what if I tried to find the average of the names of the students we put into the vector “Students”? Let’s remind ourselves what we have saved in this variable :\n\nStudents\n\n[1] \"Alice\"  \"Bob\"    \"Chad\"   \"Debbie\" \"Eric\"  \n\n\nIt shouldn’t make any sense to try to find the average of these because these variables are characters and not numeric. What should happen if we try to take an average of characters instead of numerical values? Check the output below to see! \n\nmean(Students)\n\nWarning in mean.default(Students): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\n\nYeah, this is R pretty much telling us we are not using the function correctly. \n\n\nBoolean (Logical) Variables\nOn a side note, when the error message tells us that a variable is not logical, R is not saying that we are being illogical. Instead, R is referring to another type of variable we might discuss later. These are called boolean or logical variables and all that means is that our variable takes on one of two values : TRUE or FALSE. \nIf I wanted to set the variable f to TRUE and the variable g to FALSE, I could do the following :\n\nf &lt;- TRUE\n\ng &lt;- FALSE\n\nWe could then print these out to see the result :\n\n# Print out f\nf\n\n[1] TRUE\n\n# Print out g\ng\n\n[1] FALSE\n\n\n\n\nBoolean (Logical) Operators\nThere are several operators that we can use to compare values. Here are a few : \n\nAND : &\nOR : |\nNOT : !\nEQUALS : ==\nNOT EQUALS : != \n\nLets create some variables to see how these work. \n\na &lt;- TRUE\n\nb &lt;- FALSE\n\nc &lt;- TRUE\n\nd &lt;- FALSE\n\nNow we can use these variables to see how these operators work. \n\n# The AND operator will return TRUE if both values are TRUE. If one or \n# both are FALSE, then it will return FALSE.\n\na & b\n\n[1] FALSE\n\na&c\n\n[1] TRUE\n\na&d\n\n[1] FALSE\n\nb&c\n\n[1] FALSE\n\nb&d\n\n[1] FALSE\n\nc&d\n\n[1] FALSE\n\n\n\n\n# The OR operator will return TRUE if one or both values are TRUE. If both\n# are FALSE, then it will return FALSE.\n\na|b\n\n[1] TRUE\n\na|c\n\n[1] TRUE\n\na|d\n\n[1] TRUE\n\nb|c\n\n[1] TRUE\n\nb|d\n\n[1] FALSE\n\nc|d\n\n[1] TRUE\n\n\n\n\n# The NOT operator will return the opposite of the value. If the value is TRUE\n# then it will return FALSE. If the value is FALSE, then it will return TRUE.\n\n!a\n\n[1] FALSE\n\n!b\n\n[1] TRUE\n\n\n\n\n# The EQUALS operator will return TRUE if the values are the same. If the values\n# are different, then it will return FALSE.\n\na==b\n\n[1] FALSE\n\na==c\n\n[1] TRUE\n\n\n\n\n# The NOT EQUALS operator will return TRUE if the values are different. If the values\n# are the same, then it will return FALSE.\n\na!=b\n\n[1] TRUE\n\na!=c\n\n[1] FALSE",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Intro to R"
    ]
  },
  {
    "objectID": "Intro_To_R.html#exercises",
    "href": "Intro_To_R.html#exercises",
    "title": "Intro to R",
    "section": "Exercises",
    "text": "Exercises\nThis exercise set will review the basic functions of R. Most of the commands were reviewed in the lesson, but there are a few that you will have to look up to see how to carry out the commands. \nInstructions\n\nWrite R code to solve each of the following problems.\nMake sure to test your code to ensure it works correctly.\nProvide comments in your code to explain your logic where necessary.\n\nProblem 1: Addition\nAdd two numbers, 12 and 34.\n\n# Response\n\n\nProblem 2: Subtraction\nSubtract 25 from 75.\n\n# Response\n\n\nProblem 3: Multiplication\nMultiply 28 by 19.\n\n# Response\n\n\nProblem 4: Division\nDivide 144 by 12.\n\n# Response\n\n\nProblem 5: Exponentiation\nCalculate 9 raised to the power of 7.\n\n# Response\n\n\nProblem 6: Square Root\nFind the square root of 64.\n\n# Response\n\n\nProblem 7: Logarithm\nCalculate the natural logarithm (base e) of 20.\n\n# Response\n\n\nProblem 8: Absolute Value\nFind the absolute value of -15.\n\n# Response\n\n\nProblem 9: Factorial\nCalculate the factorial of 6.\n\n# Response\n\n\nProblem 10: Variables\nCreate a variable x that is assigned the value 10 and another variable y that is assigned the value 5. Add the two variables together.\n\n# Response\n\n\nProblem 11: Variables\nCreate a variable z that is the combination of 11*x + 13*y, where x and y are from the previous problem.\n\n# Response\n\n\nProblem 12: Mean and Median of a Vector\nFind the mean and median of the elements in the vector c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20).\n\n# Response\n\n\nProblem 13: Create a Vector\nCreate a vector (with an appropriate name) that contains the names of 5 of your friends. What is the class of this vector?\n\n# Response\n\n\nProblem 14:\nCreate a vector with 20 random values between 300 and 500. What is the mean of this vector? What is the median of this vector?\n\n# Response\n\n\nProblem 15: Maximum and Minimum\nFind the maximum and minimum values of the vector created in Problem 14.\n\n# Response\n\n\nProblem 16: Vectors\nIn the vector you created in problem 14 :\n\nPrint out the 7th element.\nPrint out the first 10 elements.\nPrint out the last 5 elements.\nAdd the 4th and 18th elements together.\n\n\n# Response\n\n\nProblem 17: Loading Libraries\nLoad the library readr and use the help file to find out what the read_csv function does.\n\n# Response\n\n\nProblem 18: Boolean Vectors\nCreate a boolean vector that contains the values TRUE, FALSE, TRUE, TRUE, FALSE. Print out the vector.\n\n# Response\n\n\nProblem 19: Logical Operators\n\n# Explain the output for the following code :\n\nx &lt;- TRUE\n\ny &lt;- FALSE\n\nz &lt;- TRUE\n\nx & (y | z)\n\n[1] TRUE\n\n\n\nProblem 20: Logical Operators\n\n# Explain the output for the following code :\n\nx &lt;- 5\n\ny &lt;- 10\n\nz &lt;- 15\n\nx &gt; y & y &lt; z\n\n[1] FALSE\n\n\n\nProblem 21: Commenting \nGo back and make sure your code is commented. Make sure you have a comment at the top of the script that includes your name, the class, and the date. Make sure you have comments throughout the script to explain what is happening at each step.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Intro to R"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html",
    "href": "Reading_and_Interpreting_Tables.html",
    "title": "Reading and Interpreting Tables",
    "section": "",
    "text": "table()\nThe first function we will use to summarize categorical data is the table( ) function. This function is used to create a frequency table of the counts of the unique values in a vector. For example, we can use the table( ) function to count the number of customers. We can then use the prop.table( ) function to calculate the proportion of customers.\nWhen we use these commands, we are creating structures that are tables, but not data frames.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html#proportions",
    "href": "Reading_and_Interpreting_Tables.html#proportions",
    "title": "Reading and Interpreting Tables",
    "section": "Proportions",
    "text": "Proportions\nWe can also produce contingency tables that present the proportions (percentages) of each category or combination of categories. To do this we simply feed the frequency tables produced by table( ) to the prop.table( ) function. The following reproduces the previous tables but calculates the proportions rather than counts: \n\n# Calculate the percentages of gender categories\n\n# We will first create a new table so we don't accidentally hurt our previous work.\n\ntable2 &lt;- table(supermarket$Gender)\n\n# After saving the output ( new table) to the variable table2, we will now send\n# this table2 to prop.table( ).\n\nprop.table(table2)\n\n\n        F         M \n0.5099936 0.4900064 \n\n\n\nBased on the output, we can see that there are about 51% of respondents saying they are female (F) and about 49% of the respondents saying they are male (M). \nWe could also create a two-way table by adding another variable. For example, let’s create a table that tallies up the variables Marital Status and Gender. \n\n# We shall create a new table (table3) to analyze. \n\ntable3 &lt;- table(supermarket$`Marital Status`, supermarket$Gender)\n\n# We can now create a table of proportions for these variables.\n\nprop.table(table3)\n\n   \n            F         M\n  M 0.2562060 0.2321644\n  S 0.2537876 0.2578420\n\n\n\nWe can interpret this tables as follows :\n\n25.6% of the respondents identify as Female (F) and Married (M)\n23.2% of the respondents identify as Male (M) and Married (M)\n25.3% of the respondents identify as Female (F) and Single (S)\n25.8% of the respondents identify as Male (M) and Single (S) \n\nNote that we can tell ftable( ) how many decimal place to use when reporting the results. For example, go back to table 1. We can combine several commands together into one :\n\nWe want to run prop.table( ) on table 1\nWe want to limit to 3 decimal places\nWe want to round the results\nWe want to take this result and use ftable( ) \n\n\nftable(round(prop.table(table1), 3))\n\n        BC    CA    DF Guerrero Jalisco    OR Veracruz    WA Yucatan Zacatecas\n                                                                              \nM F  0.014 0.045 0.013    0.005   0.001 0.036    0.010 0.083   0.014     0.034\n  M  0.014 0.049 0.015    0.007   0.000 0.037    0.008 0.083   0.009     0.011\nS F  0.013 0.049 0.012    0.008   0.002 0.043    0.009 0.081   0.012     0.025\n  M  0.017 0.051 0.017    0.007   0.002 0.045    0.006 0.079   0.011     0.022",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html#marginals",
    "href": "Reading_and_Interpreting_Tables.html#marginals",
    "title": "Reading and Interpreting Tables",
    "section": "Marginals",
    "text": "Marginals\nMarginals show the total counts or percentages across columns or rows in a contingency table. For instance, if we go back to table3 which is the cross classification counts for gender by marital status: \n\n\n\n\nThe margins are simply the sums of the rows and the columns. For example, if we look at table3, I might want to know, “How many respondents identify as Single?” This is the sum on the last row, 3568 + 3625 = 7,193. Similarly, the amount of those identifying as Married would be 3602 + 3264 = 6,866. We can calculate these values using the margin.table( ) command. \n\n# FREQUENCY MARGINALS\n# row marginals - totals for each marital status across gender\n\nmargin.table(table3, 1)\n\n\n   M    S \n6866 7193 \n\n# This command takes in the table for which we want to find the margins.\n# The second parameter tells us if we want row (1) or column (2) margins.\n\n We can see that this example verifes the values we calculated above. \nWe could also calculate the column margins by changing the second parameter to 2. It is left to you to verify that these values are correct. \n\n# column marginals - totals for each gender across marital status\n\nmargin.table(table3, 2)\n\n\n   F    M \n7170 6889 \n\n\n\nIf we were more interested in proportions / percentage rather than counts, we could use the prop.table( ) command to calculate these proportions. The first example will calculate the row percentages. \n\n# PERCENTAGE MARGINALS\n\n# row marginals - row percentages across gender\n\nprop.table(table3, margin = 1)\n\n   \n            F         M\n  M 0.5246140 0.4753860\n  S 0.4960378 0.5039622\n\n\n\nWe could easily calcuate the column percentages using the following command. \n\n# column marginals - column percentages across marital status\n\nprop.table(table3, margin = 2)\n\n   \n            F         M\n  M 0.5023710 0.4737988\n  S 0.4976290 0.5262012",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html#exercises",
    "href": "Reading_and_Interpreting_Tables.html#exercises",
    "title": "Reading and Interpreting Tables",
    "section": "Exercises",
    "text": "Exercises\n\nInstall and attch the library for the package “vcd”.\n\n\n# Response\n\n\n\nDo a Google search to describe what we are getting when we load the vcd library.\n\n\n# Put response in comments\n\n\n\nDescribe what is in this data set (with View(Arthritis) ) and explain the variables and factors of each variable.\n\n\n# Response\n\n\n\nShow what is in the 1st to the 17th rows of the frame “Arthritis”\n\n\n# Response\n\n\n\nShow rows 28 to 42 and only columns 2 and 5 of the frame “Arthritis”\n\n\n# Response\n\n\n\nShow patient ID’s 1, 15, 42, and 81 and only the “Treatment” and “Improved” columns of the frame “Arthritis” using a single command. \n\n\n# Response\n\n\n\nShow the summary information for “Arthritis”\n\n\n# Response\n\n\n\nShow the values of the “Treatment” column for “Arthritis”\n\n\n# Response\n\n\n\nShow the levels of the “Treatment” column for “Arthritis”. (Hint : levels command…..)\n\n\n# Response\n\n\n\nUse the length( ) function to find the number of patients in “Arthritis”\n\n\n# Response\n\n\n\nUse the table( ) function to display the tabulated results for the “Improved” column of “Arthritis” (Note the summary( ) function does the same thing). Put the result in the variable “ImprovedTable”.\n\n\n# Response\n\n\n\nUse the prop.table( ) function on ImprovedTable to get a table of proportions\n\n\n# Response\n\n\n\nUse the xtabs( ) function to cross-tabulate “Treatment” versus “Improved” in the “Arthritis” data frame. Call the result “Treat.Improv”.\n\n\n# Response\n\n 14. Add marginal sums to the table using the addmargins( ) function.\n\n# Response\n\n 15. Create 3 tables of proportions: proportion of total, proportion of row sum, and proportion of column sum. Call the 3 tables: P.Table1, P.Table2, and P.Table3\n\n# Response",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html",
    "href": "dplyr_and_Piping.html",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "",
    "text": "piping\nThere are two kinds of ways to use piping. One is with the built in command and the other is with the command that comes with the dplyr package. The built in command is |&gt; and the dplyr command is %&gt;%. They both work in the same way so you can pick your favorite. There are some subtle differences between the two, but for the sake of this course, you will probably not notice.\nOK, we are now ready to start using the dplyr functions. Let’s start by dicussing the piping operator %&gt;%. The piping operator %&gt;% is used to chain together multiple operations in a single line of code. It takes the output of one function and passes it as the first argument to the next function. This makes it easy to read and understand the code. The syntax is bascially\nThis is equivalent to function1(data), in other words, take the data and apply function1 to it. We can chain this together with multiple functions to perform complex data manipulations in a single line of code.\nYou can read this command as “take the data set, apply function1, then take the output of function1 and apply function2, then take the output of function2 and apply function3”. Hopefully, this makes it easy to read and understand the code.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#piping",
    "href": "dplyr_and_Piping.html#piping",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "",
    "text": "data |&gt; function1()\n\n or \n\ndata %&gt;% function1()\n\n\n\n\n\ndata %&gt;% function1() %&gt;% function2() %&gt;% function3()",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#filter",
    "href": "dplyr_and_Piping.html#filter",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "filter( )",
    "text": "filter( )\nThe filter( ) function is used to filter rows based on conditions. You can specify the condition you want to filter by passing it as an argument to the filter( ) function. \nHere are the different comparisons you can use with the filter( ) command. These are called comparison operators. \n\n\n\n\nLet’s create a simple data frame to demonstrate how the filter( ) function works. We will use the data.frame() function to create a data frame with two columns: id and value. The id column will contain the numbers 1 through 15, and the value column will contain the numbers 5, 2, 9, 4, 6, 3, 8, 1, 7, 10, 2, 6, 8, 3, and 5. We will then use the filter( ) function to filter the data frame based on different conditions. \n\n# Sample data frame\ndata &lt;- data.frame(\n  id = 1:15,\n  value = c(5, 2, 9, 4, 6, 3, 8, 1, 7, 10, 2, 6, 8, 3, 5)\n)\n\ndata\n\n   id value\n1   1     5\n2   2     2\n3   3     9\n4   4     4\n5   5     6\n6   6     3\n7   7     8\n8   8     1\n9   9     7\n10 10    10\n11 11     2\n12 12     6\n13 13     8\n14 14     3\n15 15     5\n\n\n\nHere is an example of how to use the filter( ) function to filter the data frame to include only rows where the value is equal to 5.\n\n# Filter rows where value is equal to 5\nfiltered_data_1 &lt;- filter(data, value == 5)\n\n# Print out the results\nfiltered_data_1\n\n  id value\n1  1     5\n2 15     5\n\n\n\nHere are more examples : \n\n# Filter rows where value is not equal to 5\nfiltered_data_2 &lt;- filter(data, value != 5)\n\nfiltered_data_2\n\n   id value\n1   2     2\n2   3     9\n3   4     4\n4   5     6\n5   6     3\n6   7     8\n7   8     1\n8   9     7\n9  10    10\n10 11     2\n11 12     6\n12 13     8\n13 14     3\n\n# Filter rows where value is less than 5\nfiltered_data_3 &lt;- filter(data, value &lt; 5)\n\nfiltered_data_3\n\n  id value\n1  2     2\n2  4     4\n3  6     3\n4  8     1\n5 11     2\n6 14     3\n\n# Filter rows where value is less than or equal to 3\nfiltered_data_4 &lt;- filter(data, value &lt;= 3)\n\nfiltered_data_4\n\n  id value\n1  2     2\n2  6     3\n3  8     1\n4 11     2\n5 14     3\n\n# Filter rows where value is greater than 8\nfiltered_data_5 &lt;- filter(data, value &gt; 8)\n\nfiltered_data_5\n\n  id value\n1  3     9\n2 10    10\n\n# Filter rows where value is greater than or equal to 7\nfiltered_data_6 &lt;- filter(data, value &gt;= 7)\n\nfiltered_data_6\n\n  id value\n1  3     9\n2  7     8\n3  9     7\n4 10    10\n5 13     8\n\n# Filter rows where value is in the set of 1, 5, or 10\nfiltered_data_7 &lt;- filter(data, value %in% c(1, 5, 10))\n\nfiltered_data_7\n\n  id value\n1  1     5\n2  8     1\n3 10    10\n4 15     5\n\n\nGoing back to the mtcars example, if you want to filter the data to include only rows where the number of cylinders is 6, you can use the following command:\n\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Filter the data to include only rows where the number of cylinders is 6\n\ndata %&gt;%\n  filter(cyl == 6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\nAs you can see from the output, the data has been filtered to include only rows where the number of cylinders is 6. This is a simple example, but you can use the filter() function to filter the data based on any condition. \nYou can also use the filter() function from the dplyr package in combination with the is.na( ) and !is.na( ) functions to filter rows based on whether they contain NA (missing) values or not. \nHere are examples of how to use filter( ) with is.na( ) and !is.na( ): \n\n# Create a sample data frame\ndata &lt;- data.frame(\n  id = 1:10,\n  value = c(5, NA, 9, 4, NA, 3, 8, 1, NA, 10)\n)\n\n# Filter rows where the 'value' column has NA values\nfiltered_data_na &lt;- filter(data, is.na(value))\nprint(filtered_data_na)\n\n  id value\n1  2    NA\n2  5    NA\n3  9    NA\n\n# Filter rows where the 'value' column does not have NA values\nfiltered_data_not_na &lt;- filter(data, !is.na(value))\nprint(filtered_data_not_na)\n\n  id value\n1  1     5\n2  3     9\n3  4     4\n4  6     3\n5  7     8\n6  8     1\n7 10    10\n\n\n\nLastly, we can use the filter() function to filter the data based on multiple conditions. You can specify multiple conditions by passing them as arguments to the filter() function. Here are the logical operators you can use to combine multiple conditions: \n\n\n\n\nRecall that the AND operator means we want all of the conditions to be true. The OR operator means we want at least one of the conditions to be true. The NOT operator means we want the condition to be false. \nWe will use this data set for the next few examples. \n\n# Create a sample data frame\ndata &lt;- data.frame(\n  id = 1:15,\n  value = c(3, 5, 7, 2, 8, 9, 10, 1, 4, 6, 3, 8, 1, 7, 10),\n  category = c(\"C\", \"A\", \"B\", \"C\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"B\")\n)\n\ndata\n\n   id value category\n1   1     3        C\n2   2     5        A\n3   3     7        B\n4   4     2        C\n5   5     8        C\n6   6     9        A\n7   7    10        B\n8   8     1        C\n9   9     4        A\n10 10     6        B\n11 11     3        A\n12 12     8        C\n13 13     1        B\n14 14     7        A\n15 15    10        B\n\n\n\nLet’s filter the data to include only rows where the value is greater than 5 AND the category is “A”.\n\n# Filter rows where 'value' is greater than 5 AND 'category' is 'A'\nfiltered_data_and &lt;- filter(data, value &gt; 8 & category == \"A\")\n\nfiltered_data_and\n\n  id value category\n1  6     9        A\n\n\nIn this example, we only had one instance where the value was greater than 8 and the category was “A”. So, we only have one row in the output. \nNext we will filter the data to include only rows where the value is greater than 5 OR the category is “B”. \n\n# Filter rows where 'value' is greater than 5 OR 'category' is 'B'\nfiltered_data_or &lt;- filter(data, value &lt;= 2 | category == \"B\")\n\nfiltered_data_or\n\n  id value category\n1  3     7        B\n2  4     2        C\n3  7    10        B\n4  8     1        C\n5 10     6        B\n6 13     1        B\n7 15    10        B\n\n\nThis example gave us many more rows in the output because we are looking for rows where the value is less than or equal to 2 or the category is “B”. The OR command is less restrictive than the AND command, usually leading to more rows in the output. \nFinally, we will filter the data to include only rows where the value is not equal to 5.\n\n# Filter rows where 'value' is not equal to 5\n\nfiltered_data_not &lt;- filter(data, value != 5)\n\nfiltered_data_not\n\n   id value category\n1   1     3        C\n2   3     7        B\n3   4     2        C\n4   5     8        C\n5   6     9        A\n6   7    10        B\n7   8     1        C\n8   9     4        A\n9  10     6        B\n10 11     3        A\n11 12     8        C\n12 13     1        B\n13 14     7        A\n14 15    10        B\n\n\nHere we have plenty of results where the value is not equal to 5.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#mutate",
    "href": "dplyr_and_Piping.html#mutate",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "mutate( )",
    "text": "mutate( )\nWhen using the mutate() function, we can create new columns based on existing columns in the data set. You do have to be careful if you want to save the new data set with the new column. If you don’t assign the output to a new variable, the new column will not be saved. \nLet’s use the mutate() function to create a new column called gpm that calculates the miles per gallon (mpg) per cylinder (cyl). This will give us a measure of fuel efficiency per cylinder.\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Create a new column called \"gpm\" that calculates the miles per gallon per cylinder\n\ndata %&gt;%\n  mutate(gpm = mpg / cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                         gpm\nMazda RX4           3.500000\nMazda RX4 Wag       3.500000\nDatsun 710          5.700000\nHornet 4 Drive      3.566667\nHornet Sportabout   2.337500\nValiant             3.016667\nDuster 360          1.787500\nMerc 240D           6.100000\nMerc 230            5.700000\nMerc 280            3.200000\nMerc 280C           2.966667\nMerc 450SE          2.050000\nMerc 450SL          2.162500\nMerc 450SLC         1.900000\nCadillac Fleetwood  1.300000\nLincoln Continental 1.300000\nChrysler Imperial   1.837500\nFiat 128            8.100000\nHonda Civic         7.600000\nToyota Corolla      8.475000\nToyota Corona       5.375000\nDodge Challenger    1.937500\nAMC Javelin         1.900000\nCamaro Z28          1.662500\nPontiac Firebird    2.400000\nFiat X1-9           6.825000\nPorsche 914-2       6.500000\nLotus Europa        7.600000\nFord Pantera L      1.975000\nFerrari Dino        3.283333\nMaserati Bora       1.875000\nVolvo 142E          5.350000\n\n\nAt this point we have not saved the new data set with the new column. If you look at the data set, you will see that the new column has not been saved.\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# If we want to save the new data set with the new column, we need to assign the\n# output to a new variable.\n\ndata2 &lt;- data %&gt;%\n  mutate(gpm = mpg / cyl)\n\n# Notice that the output did not get printed out. It was saved to the new\n# variable data2. data2 is a new data set with the new column has been saved.\n\nhead(data2)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb      gpm\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.500000\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.500000\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 5.700000\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 3.566667\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 2.337500\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 3.016667\n\n\n\nWe can now use data2 for further analysis or if we need the new column for any other purpose. \nIt is also possible to use the mutate() function to create multiple new columns at once. You can specify the new columns you want to create by passing them as arguments to the mutate() function. Let’s use the mutate() function to create two new columns: gpm (miles per gallon per cylinder) and hp_per_cyl (horsepower per cylinder).\n\n# Create two new columns: \"gpm\" and \"hp_per_cyl\"\n\ndata %&gt;%\n  mutate(gpm = mpg / cyl,\n         hp_per_cyl = hp / cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                         gpm hp_per_cyl\nMazda RX4           3.500000   18.33333\nMazda RX4 Wag       3.500000   18.33333\nDatsun 710          5.700000   23.25000\nHornet 4 Drive      3.566667   18.33333\nHornet Sportabout   2.337500   21.87500\nValiant             3.016667   17.50000\nDuster 360          1.787500   30.62500\nMerc 240D           6.100000   15.50000\nMerc 230            5.700000   23.75000\nMerc 280            3.200000   20.50000\nMerc 280C           2.966667   20.50000\nMerc 450SE          2.050000   22.50000\nMerc 450SL          2.162500   22.50000\nMerc 450SLC         1.900000   22.50000\nCadillac Fleetwood  1.300000   25.62500\nLincoln Continental 1.300000   26.87500\nChrysler Imperial   1.837500   28.75000\nFiat 128            8.100000   16.50000\nHonda Civic         7.600000   13.00000\nToyota Corolla      8.475000   16.25000\nToyota Corona       5.375000   24.25000\nDodge Challenger    1.937500   18.75000\nAMC Javelin         1.900000   18.75000\nCamaro Z28          1.662500   30.62500\nPontiac Firebird    2.400000   21.87500\nFiat X1-9           6.825000   16.50000\nPorsche 914-2       6.500000   22.75000\nLotus Europa        7.600000   28.25000\nFord Pantera L      1.975000   33.00000\nFerrari Dino        3.283333   29.16667\nMaserati Bora       1.875000   41.87500\nVolvo 142E          5.350000   27.25000\n\n\nDon’t forget to save these new columns if you want to use them later on :\n\n# Save the new data set with the new columns\n\ndata3 &lt;- data %&gt;%\n  mutate(gpm = mpg / cyl,\n         hp_per_cyl = hp / cyl)\n\nhead(data3)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb      gpm\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.500000\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.500000\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 5.700000\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 3.566667\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 2.337500\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 3.016667\n                  hp_per_cyl\nMazda RX4           18.33333\nMazda RX4 Wag       18.33333\nDatsun 710          23.25000\nHornet 4 Drive      18.33333\nHornet Sportabout   21.87500\nValiant             17.50000\n\n\n\nIf needed, we can also use mutate() to change the values in an existing column. For example, if we wanted to add 1 to the cyl variable, we could use the following command:\n\n# Original data set :\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n# Convert the miles per gallon (mpg) column to kilometers per liter (kpl)\n  \ndata &lt;- data %&gt;%\n  mutate(cyl = cyl + 1)\n\n\n# New data set:\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   7  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   7  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   5  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   7  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   9  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   7  225 105 2.76 3.460 20.22  1  0    3    1",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#select",
    "href": "dplyr_and_Piping.html#select",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "select( )",
    "text": "select( )\nThe select() function is used to select columns from the data set. You can specify the columns you want to keep by passing their names as arguments to the select() function. You can also use the : operator to select a range of columns. Let’s use the select() function to select the columns mpg, cyl, and hp from the data set.\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Select the columns mpg, cyl, and hp\n\ndata %&gt;%\n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n\n\nAs you can see from the output, the data set has been filtered to include only the columns mpg, cyl, and hp. You can use the select() function to select any columns you want from the data set.\nIt is also worth noting that we did not save the new data set with the selected columns. If you want to save the new data set with the selected columns, you need to assign the output to a new variable.\n\n# Save the new data set with the selected columns\n\ndata3 &lt;- data %&gt;%\n  select(mpg, cyl, hp)\n\n# Notice that the output did not get printed out. It was saved to the new\n# variable data3. data3 is a new data set with the selected columns.\n\nhead(data3)\n\n                   mpg cyl  hp\nMazda RX4         21.0   6 110\nMazda RX4 Wag     21.0   6 110\nDatsun 710        22.8   4  93\nHornet 4 Drive    21.4   6 110\nHornet Sportabout 18.7   8 175\nValiant           18.1   6 105\n\n\nWe could also use the : operator to select a range of columns. For example, if we wanted to select all columns from mpg to hp, we could use the following command:\n\n# Select all columns from mpg to hp and save the new data set to data4\n\ndata4 &lt;- data %&gt;%\n  select(mpg:hp)\n\nhead(data4)\n\n                   mpg cyl disp  hp\nMazda RX4         21.0   6  160 110\nMazda RX4 Wag     21.0   6  160 110\nDatsun 710        22.8   4  108  93\nHornet 4 Drive    21.4   6  258 110\nHornet Sportabout 18.7   8  360 175\nValiant           18.1   6  225 105\n\n\n\nIf we want to exclude a column, we can use the - operator. For example, if we wanted to select all columns except mpg, we could use the following command:\n\n# Select all columns except mpg and save the new data set to data5\n\ndata5 &lt;- data %&gt;%\n  select(-mpg)\n\nhead(data5)\n\n                  cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant             6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nIf we want to remove a range of columns, we can combine the last two ideas. For instance, if we want to remove the columns from mpg to hp, we could use the following command:\n\n# Remove all columns from mpg to hp and save the new data set to data6\n\ndata6 &lt;- data %&gt;%\n  select(-(mpg:hp))\n\nhead(data6)\n\n                  drat    wt  qsec vs am gear carb\nMazda RX4         3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     3.90 2.875 17.02  0  1    4    4\nDatsun 710        3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 3.15 3.440 17.02  0  0    3    2\nValiant           2.76 3.460 20.22  1  0    3    1",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#arrange",
    "href": "dplyr_and_Piping.html#arrange",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "arrange( )",
    "text": "arrange( )\nThe arrange() function is used to sort the data set based on one or more columns. You can specify the columns you want to sort by passing their names as arguments to the arrange() function. By default, the data set is sorted in ascending order. If you want to sort in descending order, you can use the desc() function. Let’s use the arrange() function to sort the data set by the mpg column in descending order.\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Sort the data set by the mpg column in descending order\n\ndata %&gt;%\n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n\n\nAs we saw above, the output is not saved. If you want to save the sorted data set, you need to assign the output to a new variable.\n\n# Save the sorted data set to data6\n\ndata6 &lt;- data %&gt;%\n  arrange(desc(mpg))\n\nhead(data6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\nWe can sort the data set by multiple columns by passing multiple arguments to the arrange() function. For example, if we wanted to sort the data set by the mpg column in descending order and then by the cyl column in ascending order, we could use the following command:\n\n# Sort the data set by the mpg column in descending order and then by the cyl \n# column in ascending order and save the output to data7\n\ndata7 &lt;- data %&gt;%\n  arrange(desc(mpg), cyl)\n\nhead(data7)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\ntail(data7)\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMaserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\n\n\n\nYou can see from the head( ) and tail( ) commands that the data set has been sorted first by the mpg column in descending order and then by the cyl column in ascending order. So the cars with 4 cylinders come first and they are sorted by the miles per gallon in descending order. Then the cars with 6 cylinders come next and they are also sorted by the miles per gallon in descending order. Finally, the cars with 8 cylinders come last and they are also sorted by the miles per gallon in descending order.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#group_by-and-summarize",
    "href": "dplyr_and_Piping.html#group_by-and-summarize",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "group_by( ) and summarize( )",
    "text": "group_by( ) and summarize( )\nThe group_by() function is used to group the data set by one or more columns. The summarize() function is used to summarize the data within each group. You can specify the summary statistics you want to calculate by passing them as arguments to the summarize() function. Let’s use the group_by() and summarize() functions to calculate the average miles per gallon (mpg) for each number of cylinders (cyl).\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Group the data set by the cyl column and calculate the average mpg for each group\n\ndata %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(avg_mpg = mean(mpg))\n\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\nIn this example, the data was grouped together by cylinders and then the average was calculated to the miles per gallon for each group.\nLet’s verify this result by looking at the cars with 6 cylinders. Let us filter the data set to include only cars with 6 cylinders and then calculate the average miles per gallon for these cars.\n\n# Filter the data set to include only cars with 6 cylinders\n\ncyl6_data &lt;- data %&gt;%\n  filter(cyl == 6)\n\ncyl6_data\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\n\nWe can now calculate the average miles per gallon for these cars.\n\n# Calculate the average miles per gallon for cars with 6 cylinders\n\nmean(cyl6_data$mpg)\n\n[1] 19.74286\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you are coding, it is always good to double check your work with a small subset of the data to make sure that you are getting the results you expect.\n\n\nAs we saw above, the output is not saved as we are just printing out the result. If you want to save the summarized data set, you need to assign the output to a new variable.\n\n# Save the summarized data set to data8\n\ndata8 &lt;- data %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(avg_mpg = mean(mpg))\n\ndata8\n\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\nWhat happens if we have data that is incomplete? In other words, what if we have missing values in our data set? Let’s take a look at the airquality data set, which contains information about air quality measurements in New York City. This data set has missing values, so we need to be careful when calculating summary statistics.\n\n# Load the airquality data set\n\ndata(\"airquality\")\n\n# Print out the first few rows of the data set\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nLet’s calculate the average Solar.R value for each month in the airquality data set. We will use the group_by() and summarize() functions to group the data by the Month column and calculate the average temperature for each month.\n\n# Group the data set by the Month column and calculate the average Temp for each group\n\nairquality %&gt;%\n  group_by(Month) %&gt;%\n  summarize(avg_temp = mean(Solar.R))\n\n# A tibble: 5 × 2\n  Month avg_temp\n  &lt;int&gt;    &lt;dbl&gt;\n1     5      NA \n2     6     190.\n3     7     216.\n4     8      NA \n5     9     167.\n\n\nYou can see that the NA values did not allow us to properly calculate the mean. R does not automatically drop the NA vaules when going through the calculations. We can use the na.rm = TRUE argument to remove the NA values and calculate the mean for the remaining values.\n\n# Group the data set by the Month column and calculate the average Solar.R for each group\n\nairquality %&gt;%\n  group_by(Month) %&gt;%\n  summarize(avg_temp = mean(Solar.R, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  Month avg_temp\n  &lt;int&gt;    &lt;dbl&gt;\n1     5     181.\n2     6     190.\n3     7     216.\n4     8     172.\n5     9     167.\n\n\nThere are many other functions that you can use with summarize() other than just mean(). Here are some of the other functions that you can use:\n\nmean(): Calculate the mean\nmedian(): Calculate the median\nsd(): Calculate the standard deviation\nvar(): Calculate the variance\nmin(): Calculate the minimum value\nmax(): Calculate the maximum value\nn(): Count the number of observations\nsum(): Calculate the sum\nfirst(): Get the first value\nlast(): Get the last value\nnth(): Get the nth value\nn_distinct(): Count the number of distinct values\n\nYou can use these functions to calculate a wide range of summary statistics for your data.\nHere is an example of how to use the summarize() function to find the number of unique values in the Month column of the airquality data set.\n\n# Find the number of unique values in the Month column\n\nairquality %&gt;%\n  summarize(unique_months = n_distinct(Month))\n\n  unique_months\n1             5",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#chaining-operations",
    "href": "dplyr_and_Piping.html#chaining-operations",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "Chaining Operations",
    "text": "Chaining Operations\nOne of the powerful features of the dplyr package is the ability to chain operations together using the piping operator %&gt;%. This allows you to perform multiple data manipulation steps in a single line of code. Let’s walk through an example to see how this works.\nSuppose we want to filter the mtcars data set to include only cars with 6 cylinders, select the mpg, cyl, and hp columns, and then arrange the data by the mpg column in descending order. We can do this in a single line of code using the piping operator %&gt;%.\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Filter the data to include only cars with 6 cylinders, select the mpg, cyl, \n# and hp columns, and arrange the data by the mpg column in descending order\n\ndata %&gt;%\n  filter(cyl == 6) %&gt;%\n  select(mpg, cyl, hp) %&gt;%\n  arrange(desc(mpg))\n\n                mpg cyl  hp\nHornet 4 Drive 21.4   6 110\nMazda RX4      21.0   6 110\nMazda RX4 Wag  21.0   6 110\nFerrari Dino   19.7   6 175\nMerc 280       19.2   6 123\nValiant        18.1   6 105\nMerc 280C      17.8   6 123\n\n\nAnother example is to filter the mtcars data set to include only cars with 6 cylinders, create a new column power_to_weight that calculates the ratio of hp to wt, and then select the mpg, cyl, hp, and power_to_weight columns.\n\n# reinitialize the data set\n\ndata &lt;- mtcars\n\n# Filter the data to include only cars with 6 cylinders, create a new column\n# power_to_weight that calculates the ratio of hp to wt, and select the mpg, cyl,\n# hp, and power_to_weight columns\n\ndata %&gt;%\n  filter(cyl == 6) %&gt;%\n  mutate(power_to_weight = hp / wt) %&gt;%\n  select(mpg, cyl, hp, power_to_weight)\n\n                mpg cyl  hp power_to_weight\nMazda RX4      21.0   6 110        41.98473\nMazda RX4 Wag  21.0   6 110        38.26087\nHornet 4 Drive 21.4   6 110        34.21462\nValiant        18.1   6 105        30.34682\nMerc 280       19.2   6 123        35.75581\nMerc 280C      17.8   6 123        35.75581\nFerrari Dino   19.7   6 175        63.17690",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#conclusion",
    "href": "dplyr_and_Piping.html#conclusion",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to clean data using the dplyr package. We saw several of the key functions in dplyr, including filter(), mutate(), select(), arrange(), group_by(), and summarize(). We also learned how to chain operations together using the piping operator %&gt;%. These functions are powerful tools for data manipulation in R and can help you clean and transform your data quickly and efficiently.\nFor more information on the dplyr package, check out the documentation at https://dplyr.tidyverse.org/.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "dplyr_and_Piping.html#exercises",
    "href": "dplyr_and_Piping.html#exercises",
    "title": "Beginning Data Cleaning with dplyr and piping",
    "section": "Exercises",
    "text": "Exercises\nInstructions: Solve each of the following problems using the commands we saw earlier from the dplyr package :\n\nchaining operations with the %&gt;% (piping) operator\nfilter(): Filter rows based on conditions\nmutate(): Create new columns\nselect(): Select columns\narrange(): Arrange rows\ngroup_by(): Group data\nsummarize(): Summarize data\n\nEach of the solutions should be one piped command. Some may be a single pipe while others may take multiple pipes.\n\nProblem 1: Filtering Rows\nUse the mtcars dataset to filter the rows where the mpg (miles per gallon) is greater than 20. Save the result to filtered_mtcars.\n\n\nCode\n# Solution\n\n# If needed, load up dplyr\n\n# library(dplyr)\n\nfiltered_mtcars &lt;- mtcars %&gt;%\n  filter(mpg &gt; 20)\n\nfiltered_mtcars\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\nProblem 2: Selecting Columns\nUse the mtcars dataset to select the columns mpg, cyl, and hp. Save the result to selected_mtcars.\n\n\nCode\n# Solution\n\nselected_mtcars &lt;- mtcars %&gt;%\n  select(mpg, cyl, hp)\n\nselected_mtcars\n\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n\n\n\nProblem 3: Arranging Rows\nUse the mtcars dataset to arrange the rows in descending order of hp (horsepower). Save the result to arranged_mtcars.\n\n\nCode\n# Solution\n\narranged_mtcars &lt;- mtcars %&gt;%\n  arrange(desc(hp))\n\narranged_mtcars\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n\n\n\n\nProblem 4: Creating New Columns\nUse the mtcars dataset to create a new column power_to_weight which is the ratio of hp (horsepower) to wt (weight). Save the result to mutated_mtcars.\n\n\nCode\n# Solution\n\nmutated_mtcars &lt;- mtcars %&gt;%\n  mutate(power_to_weight = hp / wt)\n\nmutated_mtcars\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    power_to_weight\nMazda RX4                  41.98473\nMazda RX4 Wag              38.26087\nDatsun 710                 40.08621\nHornet 4 Drive             34.21462\nHornet Sportabout          50.87209\nValiant                    30.34682\nDuster 360                 68.62745\nMerc 240D                  19.43574\nMerc 230                   30.15873\nMerc 280                   35.75581\nMerc 280C                  35.75581\nMerc 450SE                 44.22604\nMerc 450SL                 48.25737\nMerc 450SLC                47.61905\nCadillac Fleetwood         39.04762\nLincoln Continental        39.63864\nChrysler Imperial          43.03087\nFiat 128                   30.00000\nHonda Civic                32.19814\nToyota Corolla             35.42234\nToyota Corona              39.35091\nDodge Challenger           42.61364\nAMC Javelin                43.66812\nCamaro Z28                 63.80208\nPontiac Firebird           45.51365\nFiat X1-9                  34.10853\nPorsche 914-2              42.52336\nLotus Europa               74.68605\nFord Pantera L             83.28076\nFerrari Dino               63.17690\nMaserati Bora              93.83754\nVolvo 142E                 39.20863\n\n\n\n\nProblem 5: Summarizing Data\nUse the mtcars dataset to calculate the mean mpg (miles per gallon). Save the result to summary_mtcars.\n\n\nCode\n# Solution\n\nsummary_mtcars &lt;- mtcars %&gt;%\n  summarise(mean_mpg = mean(mpg))\n\nsummary_mtcars\n\n\n  mean_mpg\n1 20.09062\n\n\n\n\nProblem 6: Grouping and Summarizing Data\nUse the mtcars dataset to calculate the mean mpg (miles per gallon) for each number of cylinders (cyl). Save the result to grouped_summary_mtcars.\n\n\nCode\n# Solution\n\ngrouped_summary_mtcars &lt;- mtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean_mpg = mean(mpg))\n\ngrouped_summary_mtcars\n\n\n# A tibble: 3 × 2\n    cyl mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     4     26.7\n2     6     19.7\n3     8     15.1\n\n\n\n\nProblem 7: Filtering, Selecting, and Arranging\nUse the mtcars dataset to filter the rows where mpg is greater than 20, select the columns mpg and hp, and arrange the rows in ascending order of hp. Save the result to filtered_selected_arranged_mtcars.\n\n\nCode\n# Solution\n\nfiltered_selected_arranged_mtcars &lt;- mtcars %&gt;%\n  filter(mpg &gt; 20) %&gt;%\n  select(mpg, hp) %&gt;%\n  arrange(hp)\n\nfiltered_selected_arranged_mtcars\n\n\n                mpg  hp\nHonda Civic    30.4  52\nMerc 240D      24.4  62\nToyota Corolla 33.9  65\nFiat 128       32.4  66\nFiat X1-9      27.3  66\nPorsche 914-2  26.0  91\nDatsun 710     22.8  93\nMerc 230       22.8  95\nToyota Corona  21.5  97\nVolvo 142E     21.4 109\nMazda RX4      21.0 110\nMazda RX4 Wag  21.0 110\nHornet 4 Drive 21.4 110\nLotus Europa   30.4 113\n\n\n\n\nProblem 8: Creating Multiple New Columns\nUse the mtcars dataset to create two new columns: power_to_weight (ratio of hp to wt) and mpg_per_cyl (ratio of mpg to cyl). Save the result to mutated_mtcars.\n\n\nCode\n# Solution\n\nmutated_mtcars &lt;- mtcars %&gt;%\n  mutate(power_to_weight = hp / wt,\n         mpg_per_cyl = mpg / cyl)\n\nmutated_mtcars\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    power_to_weight mpg_per_cyl\nMazda RX4                  41.98473    3.500000\nMazda RX4 Wag              38.26087    3.500000\nDatsun 710                 40.08621    5.700000\nHornet 4 Drive             34.21462    3.566667\nHornet Sportabout          50.87209    2.337500\nValiant                    30.34682    3.016667\nDuster 360                 68.62745    1.787500\nMerc 240D                  19.43574    6.100000\nMerc 230                   30.15873    5.700000\nMerc 280                   35.75581    3.200000\nMerc 280C                  35.75581    2.966667\nMerc 450SE                 44.22604    2.050000\nMerc 450SL                 48.25737    2.162500\nMerc 450SLC                47.61905    1.900000\nCadillac Fleetwood         39.04762    1.300000\nLincoln Continental        39.63864    1.300000\nChrysler Imperial          43.03087    1.837500\nFiat 128                   30.00000    8.100000\nHonda Civic                32.19814    7.600000\nToyota Corolla             35.42234    8.475000\nToyota Corona              39.35091    5.375000\nDodge Challenger           42.61364    1.937500\nAMC Javelin                43.66812    1.900000\nCamaro Z28                 63.80208    1.662500\nPontiac Firebird           45.51365    2.400000\nFiat X1-9                  34.10853    6.825000\nPorsche 914-2              42.52336    6.500000\nLotus Europa               74.68605    7.600000\nFord Pantera L             83.28076    1.975000\nFerrari Dino               63.17690    3.283333\nMaserati Bora              93.83754    1.875000\nVolvo 142E                 39.20863    5.350000\n\n\n\n\nProblem 9: Counting Rows\nUse the mtcars dataset to count the number of cars for each number of cylinders (cyl). Save the result to counted_mtcars.\n\n\nCode\n# Solution\n\ncounted_mtcars &lt;- mtcars %&gt;%\n  count(cyl)\n\ncounted_mtcars\n\n\n  cyl  n\n1   4 11\n2   6  7\n3   8 14\n\n\n\n\nProblem 10: Filtering and Summarizing\nUse the airquality dataset to filter the rows where Month is 5 (May), and calculate the mean Temp (temperature). Save the result to filtered_summary_airquality.\n\n\nCode\n# Solution\n\ndata(\"airquality\")\n\nfiltered_summary_airquality &lt;- airquality %&gt;%\n  filter(Month == 5) %&gt;%\n  summarise(mean_temp = mean(Temp, na.rm = TRUE))\n\nfiltered_summary_airquality\n\n\n  mean_temp\n1  65.54839\n\n\n\n\nProblem 11: Grouping and Creating New Columns\nUse the airquality dataset to group the data by Month and create a new column mean_temp which is the mean Temp (temperature) for each month. Save the result to grouped_mutated_airquality.\n\n\nCode\n# Solution\n\ngrouped_mutated_airquality &lt;- airquality %&gt;%\n  group_by(Month) %&gt;%\n  mutate(mean_temp = mean(Temp, na.rm = TRUE))\n\ngrouped_mutated_airquality\n\n\n# A tibble: 153 × 7\n# Groups:   Month [5]\n   Ozone Solar.R  Wind  Temp Month   Day mean_temp\n   &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1    41     190   7.4    67     5     1      65.5\n 2    36     118   8      72     5     2      65.5\n 3    12     149  12.6    74     5     3      65.5\n 4    18     313  11.5    62     5     4      65.5\n 5    NA      NA  14.3    56     5     5      65.5\n 6    28      NA  14.9    66     5     6      65.5\n 7    23     299   8.6    65     5     7      65.5\n 8    19      99  13.8    59     5     8      65.5\n 9     8      19  20.1    61     5     9      65.5\n10    NA     194   8.6    69     5    10      65.5\n# ℹ 143 more rows\n\n\n\n\nProblem 12: Filtering, Grouping, and Summarizing\nUse the airquality dataset to filter the rows where Temp is greater than 80, group by Month, and calculate the mean Ozone level for each month. Save the result to filtered_grouped_summary_airquality.\n\n\nCode\n# Solution\n\nfiltered_grouped_summary_airquality &lt;- airquality %&gt;%\n  filter(Temp &gt; 80) %&gt;%\n  group_by(Month) %&gt;%\n  summarise(mean_ozone = mean(Ozone, na.rm = TRUE))\n\nfiltered_grouped_summary_airquality\n\n\n# A tibble: 5 × 2\n  Month mean_ozone\n  &lt;int&gt;      &lt;dbl&gt;\n1     5       45  \n2     6       40.5\n3     7       65.4\n4     8       72.9\n5     9       57  \n\n\n\n\nProblem 13: Selecting and Summarizing\nUse the trees dataset to select the Height and Volume columns and calculate the mean Height and Volume. Save the result to selected_summary_trees.\n\n\nCode\n# Solution\n\ndata(\"trees\")\n\nselected_summary_trees &lt;- trees %&gt;%\n  select(Height, Volume) %&gt;%\n  summarise(mean_height = mean(Height),\n            mean_volume = mean(Volume))\n\nselected_summary_trees\n\n\n  mean_height mean_volume\n1          76    30.17097\n\n\n\n\nProblem 14: Filtering, Mutating, and Arranging\nUse the trees dataset to filter the rows where Height is greater than 75, create a new column volume_to_height which is the ratio of Volume to Height, and arrange the rows in descending order of volume_to_height. Save the result to filtered_mutated_arranged_trees.\n\n\nCode\n# Solution\n\nfiltered_mutated_arranged_trees &lt;- trees %&gt;%\n  filter(Height &gt; 75) %&gt;%\n  mutate(volume_to_height = Volume / Height) %&gt;%\n  arrange(desc(volume_to_height))\n\nfiltered_mutated_arranged_trees\n\n\n   Girth Height Volume volume_to_height\n1   20.6     87   77.0        0.8850575\n2   17.9     80   58.3        0.7287500\n3   17.3     81   55.4        0.6839506\n4   17.5     82   55.7        0.6792683\n5   18.0     80   51.5        0.6437500\n6   18.0     80   51.0        0.6375000\n7   16.3     77   42.6        0.5532468\n8   14.0     78   34.5        0.4423077\n9   12.9     85   33.8        0.3976471\n10  14.2     80   31.7        0.3962500\n11  13.3     86   27.4        0.3186047\n12  11.3     79   24.2        0.3063291\n13  11.1     80   22.6        0.2825000\n14  11.4     76   21.4        0.2815789\n15  11.4     76   21.0        0.2763158\n16  10.8     83   19.7        0.2373494\n17  10.7     81   18.8        0.2320988\n\n\n\n\nProblem 15: Combining Multiple Steps\nUse the trees dataset to filter the rows where Height is greater than 70, select the Height and Volume columns, create a new column volume_to_height which is the ratio of Volume to Height, and calculate the mean volume_to_height. Save the result to combined_trees.\n\n\nCode\n# Solution\n\n\ncombined_trees &lt;- trees %&gt;%\n  filter(Height &gt; 70) %&gt;%\n  select(Height, Volume) %&gt;%\n  mutate(volume_to_height = Volume / Height) %&gt;%\n  summarise(mean_volume_to_height = mean(volume_to_height))\ncombined_trees\n\n\n  mean_volume_to_height\n1             0.4262962",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Beginning Data Cleaning with dplyr and piping"
    ]
  },
  {
    "objectID": "Additional_Data_Cleaning_Functions.html",
    "href": "Additional_Data_Cleaning_Functions.html",
    "title": "Additional Data Cleaning Functions",
    "section": "",
    "text": "clean_names( )\nThe clean_names function is part of the janitor library.\nThe clean_names function is used to clean column (variable) names. Sometimes the names of the columns are not in the format that we would like. Maybe we want them to be in all lower case, or maybe we want to replace spaces with underscores. The clean_names function can be used to clean the column names. Here are the different options you can use with the clean_names function:\nConsider the following data frame:\n# create a data frame\n\ndf &lt;- data.frame(\n  \"First Name\" = c(\"MikE\", \"JOHN\", \"sue\"),\n  \"Last Name\" = c(\"LeVaN\", \"Doe\", \"SmiTH\"),\n  \"Age\" = c(30, 40, 50)\n)\n\ndf\n\n  First.Name Last.Name Age\n1       MikE     LeVaN  30\n2       JOHN       Doe  40\n3        sue     SmiTH  50\nIf we use the default clean_names function, the column names will be converted to lower case and spaces will be replaced with underscores.\nlibrary(janitor)\n\ndf_clean &lt;- clean_names(df)\n\ndf_clean\n\n  first_name last_name age\n1       MikE     LeVaN  30\n2       JOHN       Doe  40\n3        sue     SmiTH  50\nNotice that we are changing the variable names, and not the actual data itself. We will do that later in this section.\nclean_names() has a few arguments that can be used to customize the cleaning process. For example, we can use the case argument to specify whether we want the column names to be in lower case, upper case, or title case. We can also use the snake_case argument to specify whether we want to replace spaces with underscores.\ndf_upper &lt;- clean_names(df, \"all_caps\") \n\ndf_upper\n\n  FIRST_NAME LAST_NAME AGE\n1       MikE     LeVaN  30\n2       JOHN       Doe  40\n3        sue     SmiTH  50\nIf we wanted to add the prefix my_ to each column name, we could do the following:\ndf_prefix &lt;- clean_names(df, prefix = \"my_\")\n\ndf_prefix\n\n  my_first_name my_last_name my_age\n1          MikE        LeVaN     30\n2          JOHN          Doe     40\n3           sue        SmiTH     50\nAt this point the clean_names() function can only perform one operation at a time. If you want to perform multiple operations, you will need to use the clean_names() function multiple times.\ndf_multiple &lt;- clean_names(df, prefix=\"my_\")\n\ndf_multiple2 &lt;- clean_names(df_multiple, case = \"all_caps\")\n\ndf_multiple2\n\n  MY_FIRST_NAME MY_LAST_NAME MY_AGE\n1          MikE        LeVaN     30\n2          JOHN          Doe     40\n3           sue        SmiTH     50\nWhy is it important to clean the column names? It is important to clean the column names because it makes it easier to work with the data. For example, if we want to select a column from the data frame, we can use the $ operator. If the column name has spaces, we will need to use the backticks to select the column. This can be cumbersome. It is also important to clean the column names because it makes the data easier to read, as well as writing code that will work with standardized data.\nWe are now going to see how to clean the data itself.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Additional Data Cleaning Functions"
    ]
  },
  {
    "objectID": "Additional_Data_Cleaning_Functions.html#str_replace",
    "href": "Additional_Data_Cleaning_Functions.html#str_replace",
    "title": "Additional Data Cleaning Functions",
    "section": "str_replace( )",
    "text": "str_replace( )\nThe str_replace function is used to replace a pattern in a string with another pattern. The str_replace function is part of the stringr library.\n\nlibrary(stringr)\n\n# create a vector of strings\nx &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\")\n\n# replace \"a\" with \"z\"\n\nstr_replace(x, \"a\", \"z\")\n\n[1] \"zpple\"  \"bznana\" \"cherry\" \"dzte\"  \n\n\n\nThe str_replace_all function can also be used to replace multiple patterns at once if we create a vector of the patterns we want to change.\n\n# Create a sample string\nstr &lt;- \"The quick brown fox jumps over the lazy dog.\"\n\n# Create a named vector of patterns and replacements\npatterns &lt;- c(\"quick\" = \"slow\", \"brown\" = \"black\", \"lazy\" = \"energetic\")\n\n# Perform multiple replacements at once\nresult &lt;- str_replace_all(str, patterns)\nprint(result)\n\n[1] \"The slow black fox jumps over the energetic dog.\"\n\n# Output: \"The slow black fox jumps over the energetic dog.\"\n\nThe str_replace function can also be used to replace patterns that are not characters. For example, we can use the str_replace function to replace numbers.\n\n# create a vector of strings\n\ny &lt;- c(\"apple1\", \"banana2\", \"cherry3\", \"date4\")\n\n# replace numbers with \"z\"\n\nstr_replace(y, \"[0-9]\", \"z\")\n\n[1] \"applez\"  \"bananaz\" \"cherryz\" \"datez\"",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Additional Data Cleaning Functions"
    ]
  },
  {
    "objectID": "Additional_Data_Cleaning_Functions.html#tolower-and-toupper",
    "href": "Additional_Data_Cleaning_Functions.html#tolower-and-toupper",
    "title": "Additional Data Cleaning Functions",
    "section": "tolower( ) and toupper( )",
    "text": "tolower( ) and toupper( )\nThe tolower() and toupper() functions can be used to convert strings to lower case and upper case, respectively. This is useful when we want to standardize the case of the strings in a data frame or to make the data easier to read.\n\n# create a vector of strings\n\nz &lt;- c(\"Apple\", \"Banana\", \"Cherry\", \"Date\")\n\n# convert to lower case\n\ntolower(z)\n\n[1] \"apple\"  \"banana\" \"cherry\" \"date\"  \n\n# convert to upper case\n\ntoupper(z)\n\n[1] \"APPLE\"  \"BANANA\" \"CHERRY\" \"DATE\"  \n\n\n\nNote that this command is only being used for a single vector. If we wanted to convert all of the columns in a data frame to lower case, we would need to use the lapply() function.\n\n# create a data frame\n\ndf2 &lt;- data.frame(\n  \"First Name\" = c(\"MikE\", \"JOHN\", \"sue\"),\n  \"Last Name\" = c(\"LeVaN\", \"Doe\", \"SmiTH\"),\n  \"Age\" = c(30, 40, 50)\n)\n\ndf2_cleaned &lt;- lapply(df2, tolower)\n\n\nIf you notice the result, we have also created a problem for ourselves. The column “Age” now has quotation marks around it. In fact, take a look at the class of df2_cleaned.\n\nclass(df2_cleaned)\n\n[1] \"list\"\n\n\n The data frame is now a list! Really, it is a list of characters. This is because the lapply() function converts the entire data frame to a character.\nWe will see how to convert this back to a data frame in the next section.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Additional Data Cleaning Functions"
    ]
  },
  {
    "objectID": "Additional_Data_Cleaning_Functions.html#type-conversion",
    "href": "Additional_Data_Cleaning_Functions.html#type-conversion",
    "title": "Additional Data Cleaning Functions",
    "section": "Type Conversion",
    "text": "Type Conversion\nThere are times when data that should be numeric is stored as a character. This can happen when the data is imported from a file or when the data is manually entered. When this is the case, we can not perform mathematical operations on the data. Instead of reentering all of the data, we need to use R to convert this for us. \nWe can use the as.numeric() function to convert a character to a numeric.\n\n# create a vector of strings that we meant to type in as numbers :\n\nnum &lt;- c(\"1\", \"2\", \"3\", \"4\")\n\n Because we used quotes around the numbers, they are stored as characters. That means we can not perform mathematical operations on them. For example, what if we wanted to add the second and third elements of the vector together?\n\nnum[2] + num[3] \n\nError in num[2] + num[3]: non-numeric argument to binary operator\n\n\n\nYou can see from the error message that R considers these to be non-numeric so it can not perform the operation. If you are unsure of the class of the data, you can use the class() function to check.\n\n# We can check the class of the data using the class() function.\n\nclass(num) \n\n[1] \"character\"\n\n\n\nThis verifies that the data is stored as a character. We can convert this to a numeric using the as.numeric() function. Don’t forget to assign the result to a new object.\n\nnum2 &lt;- as.numeric(num)\n\n# We can check the new type :\n\nclass(num2)\n\n[1] \"numeric\"\n\n# And verify by looking at the vector.\n\nnum2\n\n[1] 1 2 3 4\n\n\n\nNow that the vector is stored as a numeric, we can perform mathematical operations on it.\n\nnum2[2] + num2[3]\n\n[1] 5\n\n\n\nFrom the earlier example, we had a data frame that was turned into a list. We can convert this back to a data frame using the as.data.frame() function.\n\ndf2_cleaned &lt;- as.data.frame(df2_cleaned)\n\nclass(df2_cleaned)\n\n[1] \"data.frame\"\n\n\n\nAs you can see, the data frame is now a data frame again. However, the variable “Age” is still a character. We can convert this to a numeric using the as.numeric()\n\ndf2_cleaned$Age &lt;- as.numeric(df2_cleaned$Age)\n\nclass(df2_cleaned$Age)\n\n[1] \"numeric\"\n\n\n\nThere are several different conversions we can make. For example, we can convert a character to a factor using the as.factor() function. We can convert a factor to a character using the as.character() function. We can convert a factor to a numeric using the as.numeric() function.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful when performing data cleaning. Make sure your data types are correct before performing operations on them. If you are unsure of the data type, use the class() function to check.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Additional Data Cleaning Functions"
    ]
  },
  {
    "objectID": "Additional_Data_Cleaning_Functions.html#exercises",
    "href": "Additional_Data_Cleaning_Functions.html#exercises",
    "title": "Additional Data Cleaning Functions",
    "section": "Exercises",
    "text": "Exercises\nInstructions \nFor this assignment, you will be provided with a sample dataset. You are required to perform various data cleaning tasks using the specified commands. Complete each problem and provide the cleaned dataset as the solution. \nBe careful to keep track of the changes you are making throughout the exercise set. The changes you make WILL affect how you code the next problem. \nSample Dataset \n\n# Sample data frame\ndata &lt;- data.frame(\n  \"First Name\" = c(\"John\", \"Jane\", \"Doe\", \"Alice\", \"Bob\"),\n  \"Last Name\" = c(\"Smith\", \"Doe\", \"Johnson\", \"Brown\", \"Davis\"),\n  \"AGE\" = c(28, 34, 45, 23, 37),\n  \"Income (USD)\" = c(\"60000\", \"75000\", \"50000\", \"80000\", \"55000\"),\n  \"Join Date\" = c(\"2022-01-15\", \"2021-05-20\", \"2023-07-30\", \"2022-11-05\", \"2021-09-17\")\n)\n\n\nProblem 1 Clean Column Names\nUse the clean_names() function to standardize the column names of the dataset to snake_case.\n\n\nCode\nlibrary(janitor)\n\ndata_cleaned &lt;- clean_names(data, case = \"snake\")\nprint(data_cleaned)\n\n\n  first_name last_name age income_usd  join_date\n1       John     Smith  28      60000 2022-01-15\n2       Jane       Doe  34      75000 2021-05-20\n3        Doe   Johnson  45      50000 2023-07-30\n4      Alice     Brown  23      80000 2022-11-05\n5        Bob     Davis  37      55000 2021-09-17\n\n\n\n\nProblem 2 Replace Substrings\nFor data_cleaned, use the str_replace() function to replace the substring “Doe” with “Smith” in the “Last Name” column.\n\n\nCode\nlibrary(stringr)\n\ndata_cleaned$last_name &lt;- str_replace_all(data_cleaned$last_name, \"Doe\", \"Smith\")\n\nprint(data_cleaned)\n\n\n  first_name last_name age income_usd  join_date\n1       John     Smith  28      60000 2022-01-15\n2       Jane     Smith  34      75000 2021-05-20\n3        Doe   Johnson  45      50000 2023-07-30\n4      Alice     Brown  23      80000 2022-11-05\n5        Bob     Davis  37      55000 2021-09-17\n\n\n\n\nProblem 3 Convert to Lower Case\nNext, use the tolower() function to convert all the values in the “First Name” column to lower case.\n\n\nCode\ndata_cleaned$first_name &lt;- tolower(data_cleaned$first_name)\n\nprint(data_cleaned)\n\n\n  first_name last_name age income_usd  join_date\n1       john     Smith  28      60000 2022-01-15\n2       jane     Smith  34      75000 2021-05-20\n3        doe   Johnson  45      50000 2023-07-30\n4      alice     Brown  23      80000 2022-11-05\n5        bob     Davis  37      55000 2021-09-17\n\n\n\n\nProblem 4 Convert to Upper Case\nContinuing, use the toupper() function to convert all the values in the “Last Name” column to upper case.\n\n\nCode\ndata_cleaned$last_name &lt;- toupper(data_cleaned$last_name)\n\nprint(data_cleaned)\n\n\n  first_name last_name age income_usd  join_date\n1       john     SMITH  28      60000 2022-01-15\n2       jane     SMITH  34      75000 2021-05-20\n3        doe   JOHNSON  45      50000 2023-07-30\n4      alice     BROWN  23      80000 2022-11-05\n5        bob     DAVIS  37      55000 2021-09-17\n\n\n\n\nProblem 5 Clean Column Names to All Caps\nUse the clean_names() function to standardize the column names of the dataset to ALL_CAPS.\n\n\nCode\ndata_cleaned &lt;- clean_names(data_cleaned, case = \"all_caps\")\n\nprint(data_cleaned)\n\n\n  FIRST_NAME LAST_NAME AGE INCOME_USD  JOIN_DATE\n1       john     SMITH  28      60000 2022-01-15\n2       jane     SMITH  34      75000 2021-05-20\n3        doe   JOHNSON  45      50000 2023-07-30\n4      alice     BROWN  23      80000 2022-11-05\n5        bob     DAVIS  37      55000 2021-09-17\n\n\n\n\nProblem 6 Multiple Substring Replacements\nUse the str_replace_all() function to perform multiple replacements in the “Last Name” column, replacing “Smith” with “Johnson” and “Brown” with “White”.\n\n\nCode\npatterns &lt;- c(\"SMITH\" = \"JOHNSON\", \"BROWN\" = \"WHITE\")\n\ndata_cleaned$LAST_NAME &lt;- str_replace_all(data_cleaned$LAST_NAME, patterns)\n\nprint(data_cleaned)\n\n\n  FIRST_NAME LAST_NAME AGE INCOME_USD  JOIN_DATE\n1       john   JOHNSON  28      60000 2022-01-15\n2       jane   JOHNSON  34      75000 2021-05-20\n3        doe   JOHNSON  45      50000 2023-07-30\n4      alice     WHITE  23      80000 2022-11-05\n5        bob     DAVIS  37      55000 2021-09-17\n\n\n\n\nProblem 7 Convert to Lower Case for All Columns\nUse the tolower() function to convert all the values in the entire dataset to lower case. Don’t forget to change the variables that are supposed to be numeric back to their proper form.\n\n\nCode\ndata_cleaned &lt;- as.data.frame(lapply(data_cleaned, tolower))\n\n# Correct the Data Type for Age, Income, and Join Date\n\ndata_cleaned$AGE &lt;- as.numeric(data_cleaned$AGE)\n\ndata_cleaned$INCOME_USD &lt;- as.numeric(data_cleaned$INCOME_USD)\n\ndata_cleaned$JOIN_DATE &lt;- as.Date(data_cleaned$JOIN_DATE)\n\n# Check the result\n\nprint(data_cleaned)\n\n\n  FIRST_NAME LAST_NAME AGE INCOME_USD  JOIN_DATE\n1       john   johnson  28      60000 2022-01-15\n2       jane   johnson  34      75000 2021-05-20\n3        doe   johnson  45      50000 2023-07-30\n4      alice     white  23      80000 2022-11-05\n5        bob     davis  37      55000 2021-09-17\n\n\n\n\nProblem 8 Convert to Upper Case for All Columns\nUse the toupper() function to convert all the values in the entire dataset to upper case. Don’t forget to change the variables that are supposed to be numeric back to their proper form.\n\n\nCode\ndata_cleaned &lt;- as.data.frame(lapply(data_cleaned, toupper))\n\n# Correct the Data Type for Age, Income, and Join Date\n\ndata_cleaned$AGE &lt;- as.numeric(data_cleaned$AGE)\n\ndata_cleaned$INCOME_USD &lt;- as.numeric(data_cleaned$INCOME_USD)\n\ndata_cleaned$JOIN_DATE &lt;- as.Date(data_cleaned$JOIN_DATE)\n\n# Check the result\n\nprint(data_cleaned)\n\n\n  FIRST_NAME LAST_NAME AGE INCOME_USD  JOIN_DATE\n1       JOHN   JOHNSON  28      60000 2022-01-15\n2       JANE   JOHNSON  34      75000 2021-05-20\n3        DOE   JOHNSON  45      50000 2023-07-30\n4      ALICE     WHITE  23      80000 2022-11-05\n5        BOB     DAVIS  37      55000 2021-09-17",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Additional Data Cleaning Functions"
    ]
  },
  {
    "objectID": "Barplots_And_Histograms.html",
    "href": "Barplots_And_Histograms.html",
    "title": "Barplots and Histograms",
    "section": "",
    "text": "Barplots\nDocumentation for the bar chart / plot command can be found at the following link:\nbar plot / chart documentation\nAs we can see in the documentation :\nThere are two types of bar charts: geom_bar( ) and geom_col( ). geom_bar( ) makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col( ) instead.\nYou should read through this as it shows you all of the different options you have when creating a barplot.\nThere are two approaches one can use when trying to create a barplot.\nThe first is to let ggplot do most of the work for you. In this case,\nThe second approach is to do some work before calling the ggplot function. Here we would\nNote : geom_col( ) yields the same results as geom_bar(stat=\"identity\").\nLet’s revisit the data set we discussed in our previous Categorical Variables section. Let’s go back to the Supermarkets Transactions data set to create a few barplots.\nLet’s first read in the data :\n# Read in the data\n\nsupermarket &lt;- read_excel(\"./Supermarket Transactions.xlsx\")\n\n# Verify the data\n\nhead(supermarket,2)\n\n# A tibble: 2 × 16\n  Transaction `Purchase Date`     `Customer ID` Gender `Marital Status`\n        &lt;dbl&gt; &lt;dttm&gt;                      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;           \n1           1 2011-12-18 00:00:00          7223 F      S               \n2           2 2011-12-20 00:00:00          7841 M      M               \n# ℹ 11 more variables: Homeowner &lt;chr&gt;, Children &lt;dbl&gt;, `Annual Income` &lt;chr&gt;,\n#   City &lt;chr&gt;, `State or Province` &lt;chr&gt;, Country &lt;chr&gt;,\n#   `Product Family` &lt;chr&gt;, `Product Department` &lt;chr&gt;,\n#   `Product Category` &lt;chr&gt;, `Units Sold` &lt;dbl&gt;, Revenue &lt;dbl&gt;\nIn the previous section, we looked at a table that broke down the customers by gender:\nTable1 &lt;- table(supermarket$Gender)\n\nTable1\n\n\n   F    M \n7170 6889\nWe can use ggplot to easily make a barplot of the Gender data. We will create a barplot that has two bars, one for each gender listed in the data set. Note that in the data set, the variable is listed with a capitol letter. The variable we want to use is Gender, and not gender. A small typo such as that can mess up your entire code.\nWe will start off the ggplot command by telling the function which data set we are using, and then we will use the aes function to tell ggplot which variable we want to use for the x-axis. We will then use the geom_bar function to create the barplot. Finally, we will use the labs function to add a title to the plot, and to label the x and y axes.\nlibrary(ggplot2)\n\n# here is how we can create a barplot of the count of  male and female gender from the supermarket data set\n\nggplot(supermarket, aes(x = Gender)) +\n  geom_bar() +\n  labs(title = \"Number of Shoppers by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\nIf we wanted to make this look nicer, we could add some color to the bars. The following example will change the color of the bars to skyblue, and the outline of the bars to black. The fill argument is used to change the color of the bars, and the color argument is used to change the color of the outline of the bars.\nggplot(supermarket, aes(x = Gender)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Number of People by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\nIf we are feeling really fancy, we can have the bars with different colors. We will add another layer using the scale_fill_model function. This function will allow us to specify the colors we want to use for each category. In this case, we will use pink for Females and blue for Males. Al you need to do is create a vector that contains the colors you wish to use.\nWe will also move the fill argument from the geom_bar function to the aes function. This will allow us to use the fill argument in the scale_fill_manual function.\nggplot(supermarket, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"F\" = \"pink\", \"M\" = \"blue\")) +\n  labs(title = \"Number of People by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\nWe could also add the numbers of each count to the bars. This will help the reader know the exact values from each category.\nggplot(supermarket, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"F\" = \"pink\", \"M\" = \"blue\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +\n  labs(title = \"Number of People by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\ngeom_text(stat = 'count', aes(label = ..count..), vjust = -0.5) is added to display the counts as text on each bar.\nstat = 'count' ensures that the text is based on the count of each category.\nlabel = after_stat(count) specifies that the label should be the count.\nvjust = -0.5 adjusts the vertical position of the text slightly above the bar. Adjust this value as needed to position the text correctly.\nWe could move the text to the middle of the bars, by playing around with vjust. I found vjust = 15 is a good value to center the text on the bars in this case. It can change depending on your data sets.\nggplot(supermarket, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"F\" = \"pink\", \"M\" = \"blue\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = 15, color = \"white\") +\n  labs(title = \"Number of People by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\nWe could also change the font size of the text. The following example will change the font size to 5.\nggplot(supermarket, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"F\" = \"pink\", \"M\" = \"blue\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = 15, color = \"white\", size = 5) +\n  labs(title = \"Number of People by Gender\",\n       x = \"Gender\",\n       y = \"Count\")\nConsider the case to where we have more than two variables we want to graph. For example, in the Product Family category, we have three different factors : Drink, Food, and Non-Consumable. We can create a barplot that shows the count of each of these categories.\nNotice that the fill argument tells ggplot which variable to use for the color of the bars. We will use the scale_fill_manual function to specify the colors we want to use for each category.\nggplot(supermarket, aes(x = `Product Family`, fill = `Product Family`)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"Food\" = \"green\", \"Drink\" = \"blue\", \"Non-Consumable\" = \"red\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = 2, color = \"white\", size = 5) +\n  labs(title = \"Count of Product Family Categories\",\n       x = \"Product Family\",\n       y = \"Count\")\nLastly, we could create a barplot where the bars are stacked on top of each other.\nggplot(supermarket, aes(x = \"\", fill = `Product Family`)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"Food\" = \"green\", \"Drink\" = \"blue\", \"Non-Consumable\" = \"red\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), position = position_stack(vjust = 0.5), color = \"white\", size = 5) +\n  labs(title = \"Count of Product Family Categories\",\n       x = \"Product Family\",\n       y = \"Count\")\nThese can sometimes look better if drawn horizontally. We can do this by flipping the axis using the coord_flip function.\nggplot(supermarket, aes(x = \"\", fill = `Product Family`)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"Food\" = \"green\", \"Drink\" = \"blue\", \"Non-Consumable\" = \"red\")) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), position = position_stack(vjust = 0.5), color = \"white\", size = 5) +\n  labs(title = \"Count of Product Family Categories\",\n       x = \"Product Family\",\n       y = \"Count\") +\n  coord_flip()",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Barplots and Histograms"
    ]
  },
  {
    "objectID": "Barplots_And_Histograms.html#histograms",
    "href": "Barplots_And_Histograms.html#histograms",
    "title": "Barplots and Histograms",
    "section": "Histograms",
    "text": "Histograms\nHistograms are used to display the distribution of a continuous variable. \nDocumentation for the histogram command can be found at the following link:  histogram documentation \nHere are some of the ideas R uses to create histograms :\n\nYou need an x-axis variable that is continuous, but you do not need a y-axis variable. The y-axis will be created by R.\nR will create bins for the data. The default number of bins is 30. You can change this number if you want. A bin is a range of values. For example, if you have a bin width of 5, the first bin will be from 0 to 5, the second bin will be from 5 to 10, the third bin will be from 10 to 15, and so on.\nThe height of the bars will be the count of the number of values in each bin.\n\nSome of this work is done before the aesthetic mapping. ggplot creates and counts bins before the aesthetic mapping. This is why you do not need to specify a y-axis variable.\n\n\nThere are several arguments that can be used to customize the histogram and you should review the documentation to see all of the options available. \nLet’s create a histogram of the Revenue variable from the supermarket data set. We will use the geom_histogram( ) function to create the histogram. We will also use the labs function to add a title to the plot, and to label the x and y axes. \nThe following is a basic default histogram before any customization:\n\nggplot(supermarket, aes(x = `Revenue`)) +\n  geom_histogram() +\n  labs(title = \"Revenue\",\n       x = \"Revenue\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nWe will see how we can customize this histogram. We will add color to the bars, change the outline of the bars, and add a fill to the bars. We will also add a title to the plot, and label the x and y axes. \nHowever, one thing to note is that the geom_histogram function has a binwidth argument that can be used to change the width of the bars. The default value is binwidth = 30. This means that the bars will be 30 units wide. If we wanted to make the bars 10 units wide, we would use binwidth = 10. If we wanted to make the bars 5 units wide, we would use binwidth = 5. The smaller the number, the more bars we will have. The larger the number, the fewer bars we will have. A bad choice here could make the histogram look bad. You will need to decide which value best represents your data.\n\nggplot(supermarket, aes(x = `Revenue`)) +\n  geom_histogram(fill = \"skyblue\", color = \"black\", binwidth = 30) +\n  labs(title = \"Revenue\",\n       x = \"Revenue\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nThis histogram has too few bars. This does not give us a good picture of the distribution of the data. We can make a histogram where the bin width is 5. Maybe this will give us a better picture of the distribution of the data.\n\nggplot(supermarket, aes(x = `Revenue`)) +\n  geom_histogram(fill = \"skyblue\", color = \"black\", binwidth = 5) +\n  labs(title = \"Revenue\",\n       x = \"Revenue\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nThis is better. We can see that the data is skewed to the right. We can also see that there are a few outliers. If we were doing some data analysis, we could remove the outliers and see if the data is normally distributed. We could also try to transform the data to see if it becomes normally distributed. We will not do that here, but it is something to consider when looking at data. \nAlso notice the locations of the bars. The bars are centered on the x-axis values. This means that the bars are centered on the values 0, 5, 10, 15, 20, and so on. So the first bar has a width of 5 centered at 0. This means the first bar starts at -2.5 and goes to 2.5. The second bar starts at 2.5 and goes to 7.5. The third bar starts at 7.5 and goes to 12.5. And so on. This is not always the best way to display the data.  If we want the bars to start at 0 (or some other boundary) we can use the boundary argument. The default value is boundary = 0. This means that the bars will start at 0. If we wanted the bars to start at 5, we would use boundary = 5. If we wanted the bars to start at -5, we would use boundary = -5.\n\nggplot(supermarket, aes(x = `Revenue`)) +\n  geom_histogram(fill = \"skyblue\", color = \"black\", binwidth = 5, boundary=0) +\n  labs(title = \"Revenue\",\n       x = \"Revenue\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nAs we did with the baplot, we can add values to the histogram bars. This is a tad tricky, and is not always the best way to display the data. Many journals will include a table outlining the different intervals and counts. \nAn additional layer that can be added to a histogram to help see the distribution of the data is a density plot. The geom_density function can be used to add a density plot to the histogram. The density plot will show the distribution of the data. The density plot is a smoothed version of the histogram. The density plot is useful when the data is not normally distributed. The density plot will show the distribution of the data, and will show if the data is skewed to the left or right, or if the data is bimodal. \nThe following example will add a density plot to the histogram. The geom_density function is used to add the density plot. The fill argument is used to change the color of the density plot. The color argument is used to change the outline of the density plot. The size argument is used to change the size of the density plot. The alpha argument is used to change the transparency of the density plot. The linetype argument is used to change the line type of the density plot. The position argument is used to change the position of the density plot. The adjust argument is used to change the adjustment of the density plot. \nThe difficult part is that you will need to alter the y value in the aes function to get the density plot to show up. The y value is the height of the density plot. You will need to play around with this value to get the density plot to show up. The following example uses y = after_stat(density)*70000.\n\nggplot(supermarket, aes(x = `Revenue`)) +\n  geom_histogram(fill = \"skyblue\", color = \"black\", binwidth = 5, boundary=0) +\n  geom_density(aes(y=after_stat(density)*70000), fill = \"red\", color = \"black\", \n               linewidth = 1, alpha = 0.25, linetype = \"dashed\", \n               position = \"stack\", adjust = 1) +\n  labs(title = \"Revenue\",\n       x = \"Revenue\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nFor more information on how to create nice histograms, you can read the following :\nhttps://www.appsilon.com/post/ggplot2-histograms",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Barplots and Histograms"
    ]
  },
  {
    "objectID": "Barplots_And_Histograms.html#exercises",
    "href": "Barplots_And_Histograms.html#exercises",
    "title": "Barplots and Histograms",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will be working with a small dataset and using R to create basic bar plots and histograms. This exercise will help you get familiar with data visualization techniques, which are essential for data analysis and interpretation.\n\nSkills Checked\n\nLoad the Dataset: Load the provided dataset into R.\nBar Plot: Create a bar plot to visualize the data.\nHistogram: Create a histogram to visualize the data.\n\n\n\nBarplot Dataset 1\nThe following is a dataset that contains information about the number of students enrolled in different courses at a university. Copy the following dataset into a CSV file named courses.csv and upload it to your working directory. Create an appropriate barchart depicting the data.\n\n\n\nCourse\nNumber of Students\n\n\n\n\nMathematics\n45\n\n\nPhysics\n30\n\n\nChemistry\n25\n\n\nBiology\n50\n\n\nComputer_Science\n40\n\n\nHistory\n35\n\n\nEnglish\n20\n\n\nEconomics\n30\n\n\n\n\n\nBarplot Dataset 2\nThe following is a dataset that contains sales information about the number of electronic sales at your company. Copy the following dataset into a CSV file named sales.csv and upload it to your working directory. Create an appropriate barchart depicting the data.\n\n\n\nProduct\nSales\n\n\n\n\nLaptops\n120\n\n\nSmartphones\n200\n\n\nTablets\n80\n\n\nAccessories\n150\n\n\nWearables\n90\n\n\nDesktops\n70\n\n\nCameras\n60\n\n\nPrinters\n40\n\n\n\n\n\nBarplot Dataset 3\nThe following is a dataset that contains monthly rainfall (in millimeters) information for various cities across the United States. Copy the following dataset into a CSV file named rainfall.csv` and upload it to your working directory. Create an appropriate barchart depicting the data.\n\n\n\nCity\nMonthly_Rainfall_mm\n\n\n\n\nNew_York\n120\n\n\nLos_Angeles\n20\n\n\nChicago\n80\n\n\nHouston\n90\n\n\nPhoenix\n15\n\n\nPhiladelphia\n100\n\n\nSan_Antonio\n70\n\n\nSan_Diego\n30\n\n\nDallas\n85\n\n\nSan_Jose\n25\n\n\n\n\n\nBarplot Dataset 4\nThe following is a dataset that contains monthly transatlantic airtravel, in thousands of passengers, for 1958-1960. There are 4 fields, “Month”, “1958”, “1959” and “1960” and 12 records, “JAN” through “DEC”. \nCreate a bar chart that shows the average number of passengers for each month across the years 1958-1960. \nData : https://people.sc.fsu.edu/~jburkardt/data/csv/airtravel.csv \nYou will need to downlaod the data set and load it into R.\n\n\nBarplot Dataset 5\nGo to Kaggle and download the dat set “US Christmas Tree Sales Data” from the following link: \nhttps://www.kaggle.com/datasets/thedevastator/us-christmas-tree-sales-data/data \nCreate a bar chart that shows the number of trees sold (per million) for each year from 2010-2016.\n\ntree_data &lt;- read_csv(\"./US_Christmas_Tree_Sales_2010_to_2016.csv\")\n\n\n\nHistogram Dataset 1\nThe following is a dataset (Ages.csv) that contains information about the ages of participants at a local community center. Copy the dataset into a CSV file named ages_data.csv and upload it to your working directory. Create a histogram with 7 bins depicting the data.\n\nages_data &lt;- read_csv(\"./Ages.csv\")\n\nages_data\n\n# A tibble: 50 × 1\n     Age\n   &lt;dbl&gt;\n 1    23\n 2    27\n 3    31\n 4    35\n 5    29\n 6    42\n 7    38\n 8    45\n 9    52\n10    36\n# ℹ 40 more rows\n\n\n\n\nHistogram Dataset 2\nThe following (Lexington_Temperature_Data.csv) is a dataset that contains information about the daily temperature in Celsius for Lexington the first 50 days of spring. Copy the dataset into a CSV file named Lex_temps.csv and upload it to your working directory. Create a histogram with 4 bins depicting the data.\n\nLex_temps &lt;- read_csv(\"./Lexington_Temperature_Data.csv\")\n\nLex_temps\n\n# A tibble: 50 × 2\n     Day Temperature\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1     1        15.2\n 2     2        16.5\n 3     3        14.8\n 4     4        17.1\n 5     5        15.9\n 6     6        16.2\n 7     7        15  \n 8     8        14.7\n 9     9        16.8\n10    10        15.4\n# ℹ 40 more rows\n\n\n\n\nHistogram Dataset 3\nThe following is a data set (kentucky_mens_basketball_wins.csv) that contains information about the number of wins for the University of Kentucky’s men’s basketball team from 1980 - 2024. Copy the dataset into a CSV file named UK_wins and upload it to your working directory. Create a histogram with 10 bins depicting the data on the amount of wins per year.\n\nUK_wins &lt;- read_csv(\"./kentucky_mens_basketball_wins.csv\")\n\nUK_wins\n\n# A tibble: 45 × 2\n    Year Games_Won\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1980        29\n 2  1981        22\n 3  1982        22\n 4  1983        13\n 5  1984        29\n 6  1985        18\n 7  1986        32\n 8  1987        18\n 9  1988        27\n10  1989        13\n# ℹ 35 more rows\n\n\n\n\nHistogram Dataset 4\nThe following is a data set (Fire_Arm_Deaths_1990_to_2022.xlsx) that contains information about the number of firearm deaths in the US from 1990 - 2024. Copy the dataset into a CSV file named Fire_Arm_Deaths and upload it to your working directory. Create a histogram with 10 bins depicting the data on the amount of deaths per year.\n\nlibrary(readxl)\n\nFire_Arm_Deaths &lt;- read_csv(\"./Fire_Arm_Deaths_1990_to_2022.csv\")\n\nFire_Arm_Deaths\n\n# A tibble: 33 × 2\n    Year Deaths\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2022  48204\n 2  2021  48830\n 3  2020  45222\n 4  2019  39707\n 5  2018  39740\n 6  2017  39773\n 7  2016  38658\n 8  2015  36252\n 9  2014  33594\n10  2013  33636\n# ℹ 23 more rows\n\n\n\n\nHistogram Dataset 5\nThe data set (gdp-per-capita-us-dollars-2020.csv) contains information about the GDP per capita in US dollars for various countries in 2020. Download the data set from here and save it to yourworking directory. Copy the dataset into a variable named gdp_per_capita.\n\ngdp_per_capita &lt;- read_csv(\"./gdp-per-capita-us-dollars-2020.csv\")\n\nhead(gdp_per_capita)\n\n# A tibble: 6 × 2\n  `Country Name`       `GDP Per Capita (Current US $ , 2020)`\n  &lt;chr&gt;                                                 &lt;dbl&gt;\n1 Afghanistan                                             517\n2 Angola                                                 1776\n3 Albania                                                5246\n4 Algeria                                                3307\n5 United Arab Emirates                                  36285\n6 Argentina                                              8579\n\n\n\nCreate a histogram, with equal class widths, with the first class being (0 , 10000 ] and covers all data values.\nCreate a histogram, with equal class widths, with the first class being (0 , 5000] with the last class being (55000 , 60000]\nCreate a histogram, with equal class widths, with the first class being (0 , 1000] with the last class being (19000 , 20000]\nCompare all three histograms. Which one gives more information as to how the countries with low GDP are distributed?",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Barplots and Histograms"
    ]
  },
  {
    "objectID": "Line_Graphs_and_Pie_Charts.html",
    "href": "Line_Graphs_and_Pie_Charts.html",
    "title": "Line Graphs and Pie Charts",
    "section": "",
    "text": "Line Graphs\nA line graph (or line chart) is used to display data points connected by a continuous line. As we mentioned above, is especially useful for showing trends over time.\nWhat Does a Line Graph Do?\nWhen to Use a Line Graph?\nIn order to make a line graph in R, you need to make sure the following libraries are loaded up :\nWe will use a combination of ggplot( ) and the geometry function geom_line( ) to create a line graph. the geom_line( ) function connects the data points in the order of the x-axis variable.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Line Graphs and Pie Charts"
    ]
  },
  {
    "objectID": "Line_Graphs_and_Pie_Charts.html#line-graphs",
    "href": "Line_Graphs_and_Pie_Charts.html#line-graphs",
    "title": "Line Graphs and Pie Charts",
    "section": "",
    "text": "Shows trends and patterns : It helps visualize how a variable changes over time (e.g., stock prices, temperature changes, sales growth).\nCompares multiple data series : You can plot multiple lines to compare different categories or groups.\nIdentifies peaks and dips : It makes it easy to see the highest and lowest values in a dataset.\nHelps in forecasting : By analyzing past trends, a line graph can give insights into possible future patterns. \n\n\n\nWhen data is continuous (e.g., time-based data like months, years, or days).\nWhen you want to track changes over time.\nWhen comparing trends across different categories.\n\n\n\ndplyr\nggplot2\n\n\n\nUnemployment Rate Example\nLet’s take a look at an example of a line graph that shows the unemployment rate over time using the built-in economics dataset in R.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load built-in economics dataset\n\ndata(\"economics\")\n\n# Create a time series line plot of unemployment rate\n\neconomics %&gt;%\n  ggplot(aes(x = date, y = unemploy)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Unemployment Over Time\",\n       x = \"Year\",\n       y = \"Unemployment (in thousands)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis graph shows the unemployment rate over time. We can see that the rate fluctuates over time, with some periods of high unemployment and some periods of low unemployment. This graph can help us understand how the unemployment rate has changed over time and identify any trends or patterns. \nWhile line graphs are generally used as a way to show trends over time, they can also be used to compare multiple data series. Let’s look at an example of a line graph that compares the price of a diamond to the size (carat) of the diamond. \n\n\nDiamond Price vs Carat Size Example\nLet’s consider the diamonds dataset that comes with the ggplot2 package. This dataset contains information about the price, carat, and cut of diamonds. We will use this dataset to create a line graph that shows the comparison of average price versus carat size.\n\n# Load the libraries\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create a line graph of the average price of diamonds over time\n# Aggregate data: Get the average price for each carat size\n\navg_price_per_carat &lt;- aggregate(price ~ carat, data = diamonds, FUN = mean)\n\n# Create the line graph\nggplot(avg_price_per_carat, aes(x = carat, y = price)) +\n  geom_line(color = \"blue\", linewidth = 1) + \n  labs(title = \"Diamond Price vs. Carat Size\",\n       x = \"Carat Size\",\n       y = \"Average Price (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhile certainly not a surprise, we can interpret from this graph that as the number of carats in the diamond increases, the price of the diamond also increases. It is curious as to why there is a dip in the price of diamonds of size around 3 carats. This could be due to a variety of factors such as the quality of the diamond, the cut, or the color. \nLet’s walk through some examples of how we can think about how to create line graphs. Try to follow along by creating the graphs in your R console. \n\n\nBasic Line Graph\nObjective: Create a basic line graph using ggplot2. \nInstructions: \n\nLoad the ggplot2 library.\nCreate a simple data frame with two columns: year (from 2000 to 2020) and value (random numbers).\nUse ggplot to create a line graph where the x-axis represents the year and the y-axis represents the value.\n\nExample:\n\nlibrary(ggplot2)\n\n# Create data frame\ndf &lt;- data.frame(\n  year = 2000:2020,\n  value = runif(21, min = 0, max = 100)\n)\n\n# Create line graph\nggplot(df, aes(x = year, y = value)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nAdding Titles and Labels\nObjective: Enhance the basic line graph by adding titles and axis labels. \nInstructions:\n\nAdd a main title, x-axis label, and y-axis label to the line graph.\n\nExample:\n\nggplot(df, aes(x = year, y = value)) +\n  geom_line() +\n  ggtitle(\"Value Over Years\") +\n  xlab(\"Year\") +\n  ylab(\"Value\")\n\n\n\n\n\n\n\n\n\n\nStyling the Line\nObjective: Customize the appearance of the line in the graph. \nInstructions:\n\nChange the line color to blue and make it thicker.\n\nExample:\n\nggplot(df, aes(x = year, y = value)) +\n  geom_line(color = \"blue\", size = 1.5) +\n  ggtitle(\"Value Over Years\") +\n  xlab(\"Year\") +\n  ylab(\"Value\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nAdding Points to the Line Graph\nObjective: Add points to highlight data values on the line graph. \nInstructions:\n\nAdd points on top of the line graph to indicate the actual data values.\n\nExample:\n\nggplot(df, aes(x = year, y = value)) +\n  geom_line(color = \"blue\", size = 1.5) +\n  geom_point(color = \"red\", size = 3) +\n  ggtitle(\"Value Over Years\") +\n  xlab(\"Year\") +\n  ylab(\"Value\")\n\n\n\n\n\n\n\n\n\n\nFaceting the Line Graph\nObjective: Create multiple line graphs using faceting. \nInstructions:\n\nCreate a new data frame with three columns: year (from 2000 to 2020), value (random numbers), and category (categorical variable with two levels: “A” and “B”).\nUse ggplot to create a line graph with facets for each category.\n\nExample:\n\n# Create data frame with categories\ndf &lt;- data.frame(\n  year = rep(2000:2020, 2),\n  value = runif(42, min = 0, max = 100),\n  category = rep(c(\"A\", \"B\"), each = 21)\n)\n\n# Create faceted line graph\nggplot(df, aes(x = year, y = value, color = category)) +\n  geom_line() +\n  facet_wrap(~ category) +\n  ggtitle(\"Value Over Years by Category\") +\n  xlab(\"Year\") +\n  ylab(\"Value\")\n\n\n\n\n\n\n\n\nNow that we have seen a few examples of how to create a line graph, let’s turn our attention towards pie charts.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Line Graphs and Pie Charts"
    ]
  },
  {
    "objectID": "Line_Graphs_and_Pie_Charts.html#pie-charts",
    "href": "Line_Graphs_and_Pie_Charts.html#pie-charts",
    "title": "Line Graphs and Pie Charts",
    "section": "Pie Charts",
    "text": "Pie Charts\nA pie chart is a circular statistical graphic that is divided into slices to illustrate numerical proportions. The size of each slice is proportional to the quantity it represents. Pie charts are useful for showing the relative proportions of different categories or groups in a dataset. \nThere are several different ways on could make a pie chart. While we will use the traditinoal version of a pie chart, there are several pretty cool methods you could use to present the data is a more circularish way. You can check them out at the following link : \n Different Pie Chart Methods  \n\nWhat Does a Pie Chart Do?\n\nShows proportions : It helps visualize the distribution of data across different categories.\nCompares parts to the whole : It shows how each category contributes to the total.\nHighlights differences : It makes it easy to see which categories are larger or smaller.\nSimplifies complex data : It presents data in a simple and easy-to-understand format. \n\nWhen to Use a Pie Chart?\n\nWhen you want to show the relative proportions of different categories.\nWhen you want to compare parts to the whole.\nWhen you have a small number of categories (3-7) to display. \n\nUnfortunatley ggplot2 doesn’t have a dedicated geometry for pie charts; instead, you create them by transforming a stacked bar chart using coord_polar( ). \nHere’s how you can achieve this: \nPie charts are essentially stacked bar charts viewed in polar coordinates. You can follow these steps to create a pie chart in ggplot2: \n\nUse geom_bar( ) or geom_col( ) to create the stacked bar chart, mapping your data to the y aesthetic (for the bar heights) and fill aesthetic (for the colors of the slices).\nUse coord_polar( ) to transform the rectangular bar chart into a circular pie chart. \n\nYou can customize the pie chart further by adjusting the colors, labels, and other aesthetics using ggplot2’s various functions. \n\nDiamonds Pie Chart Example\nLet’s create a pie chart that shows the distribution of diamond cuts (Fair, Good, Very Good, Premium, Ideal)in the diamonds dataset. \nLet’s first take a quick look at the data set by using the count( ) function to get the number of diamonds for each cut. \n\ndiamonds %&gt;% count(cut)\n\n# A tibble: 5 × 2\n  cut           n\n  &lt;ord&gt;     &lt;int&gt;\n1 Fair       1610\n2 Good       4906\n3 Very Good 12082\n4 Premium   13791\n5 Ideal     21551\n\n\n\nThis output creates a table that has two columns: cut and n. The cut column contains the different types of diamond cuts, and the n column contains the number of diamonds for each cut. Notice that we did not save this output to a different variable. We could do this, but it is not needed if we are going to simply pipe this output to the next step of the process. \nAt this point we could make a normal bar chart as follows :\n\ndiamonds %&gt;%\n  count(cut) %&gt;%\n  ggplot(aes(x = cut, y = n, fill = cut)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  labs(title = \"Distribution of Diamond Cuts\",\n       fill = \"Cut\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nWhat we want is a stacked bar chart, so we don’t want the x-axis to be the cut of the diamond. Instead, we want the x-axis to be an empty string (x = \"\"). This will create a bar chart with only one bar. We will take this output and create a stacked barplot of the distribution of diamond cuts using another pipe and sending the output to ggplot and then to geom_bar. We finish off the bar chart by adding some labels, fills, and a theme.\n\ndiamonds %&gt;%\n  count(cut) %&gt;%\n  ggplot(aes(x = \"\", y = n, fill = cut)) +\n#  geom_bar(stat = \"identity\", width = 1) +\n  geom_col() +\n    labs(title = \"Distribution of Diamond Cuts\",\n       fill = \"Cut\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nWe are now ready to turn this into a Pie Chart! By adding the coord_polar function, we transform the bar chart into a pie chart. \nHere is the code to create a pie chart of the distribution of diamond cuts: \n\n# Load necessary libraries\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a pie chart of the distribution of diamond cuts\n\n# We will create the bar chart as we did above, but this time we will use\n# coord_polar to make it circular.\n\n# Lastly we will add some labels and some colors\n\n# The fill color is the cut of the diamond and the sizes of the pieces of the \n# pie are based on the number of diamonds\n\ndiamonds %&gt;%\n  count(cut) %&gt;%\n  ggplot(aes(x = \"\", y = n, fill = cut)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Distribution of Diamond Cuts\",\n       fill = \"Cut\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\nCars by Cylinder Pie Chart Example\nLet’s look at one more example by revisiting the mtcars dataset. We will create a pie chart that shows the distribution of car models by the number of cylinders. \n\n# Load necessary libraries\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# load the mtcars data set\n\ndata(\"mtcars\")\n\n# Create a pie chart of the Count of car models by number of cylinders. \n\n# We will use the count function to get the number of cars for each \n# combination of model and cylinders\n\n# We will then use ggplot to create the pie chart by making a bar plot and\n# using coord_polar to make it circular\n\n# Lastly we will add some labels and some colors where the fill color is the \n# number of cylinders\n\nmtcars %&gt;%\n  count(cyl) %&gt;%\n  ggplot(aes(x=\"\", y=n, fill=factor(cyl))) +\n  geom_bar(stat=\"identity\", width=1) +\n  coord_polar(\"y\") +\n  geom_text(aes(label = n),\n        position = position_stack(vjust = 0.5),\n        color = \"white\", size = 5) +\n  labs(title=\"Count of Cars by Cylinder\",\n       fill=\"Cylinders\")\n\n\n\n\n\n\n\n\nNote that we can remove that “outer ring” by adding theme_void( ) to the end of the code. This will remove the axis labels and the grid lines. \n\nmtcars %&gt;%\n  count(cyl) %&gt;%\n  ggplot(aes(x=\"\", y=n, fill=factor(cyl))) +\n  geom_bar(stat=\"identity\", width=1) +\n  coord_polar(\"y\") +\n  geom_text(aes(label = n),\n        position = position_stack(vjust = 0.5),\n        color = \"white\", size = 5) +\n  labs(title=\"Count of Cars by Cylinder\",\n       fill=\"Cylinders\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAs we did with the last Line Graph example, let’s walk through some ideas on how to create a pie chart. \n\n\nBasic Pie Chart\nTask: Create a basic pie chart using the ggplot2 library in R. \nSteps:\n\nInstall and load the ggplot2 library.\nCreate a simple data frame with two columns: category and value.\nUse ggplot2 to create a pie chart from the data frame.\n\nSolution:\n\n# Install and load ggplot2\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 20, 30, 40)\n)\n\n# Create pie chart\nggplot(data, aes(x = \"\", y = value, fill = category)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nAdding Labels to the Pie Chart\nTask: Add labels to the pie chart to show the percentage of each category.\nSteps:\n\nModify the data frame to include percentage calculations.\nAdd labels to the pie chart using geom_text. \n\nSolution:\n\n# Create data frame with percentage\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 20, 30, 40)\n)\ndata$percentage &lt;- round(data$value / sum(data$value) * 100, 1)\n\n# Create pie chart with labels\nggplot(data, aes(x = \"\", y = value, fill = category)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(label = paste0(percentage, \"%\")), position = position_stack(vjust = 0.5)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nCustomizing Colors\nTask: Customize the colors of the pie chart slices. \nSteps:\n\nChoose a color palette.\nApply the color palette to the pie chart using scale_fill_manual. \n\nSolution:\n\n# Create data frame with percentage\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 20, 30, 40)\n)\n\n# Custom colors\ncolors &lt;- c(\"A\" = \"red\", \"B\" = \"blue\", \"C\" = \"green\", \"D\" = \"purple\")\n\n# Create pie chart with custom colors\nggplot(data, aes(x = \"\", y = value, fill = category)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = colors) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nAdding a Title and Legend\nTask: Add a title and customize the legend of the pie chart. \nSteps:\n\nUse ggtitle to add a title to the pie chart.\nCustomize the legend using theme.\n\nSolution:\n\n# Create data frame with percentage\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 20, 30, 40)\n)\n\n# Create pie chart with title and legend\nggplot(data, aes(x = \"\", y = value, fill = category)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  ggtitle(\"Pie Chart Example\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Line Graphs and Pie Charts"
    ]
  },
  {
    "objectID": "Line_Graphs_and_Pie_Charts.html#exercises",
    "href": "Line_Graphs_and_Pie_Charts.html#exercises",
    "title": "Line Graphs and Pie Charts",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will create line graphs and pie charts using the ggplot2 library in R. You will use built-in datasets from RStudio to visualize different types of data. Ensure you follow the instructions for each problem carefully.\n\nProblem 1: Line Graph of pressure Dataset\nTask: Create a line graph of the pressure dataset, which shows the relationship between temperature and pressure.\nSteps: \n\nLoad the ggplot2 library.\nUse the pressure dataset.\nCreate a line graph with temperature on the x-axis and pressure on the y-axis.\nAdd appropriate labels to the axes and a title to the graph.\n\nSolution:\n\n\nCode\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Create line graph\nggplot(pressure, aes(x = temperature, y = pressure)) +\n  geom_line() +\n  labs(title = \"Temperature vs Pressure\",\n       x = \"Temperature\",\n       y = \"Pressure\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 2: Line Graph of AirPassengers Dataset\nTask: Create a line graph of the AirPassengers dataset, which shows the number of air passengers over time.\nSteps: \n\nLoad the ggplot2 library.\nUse the AirPassengers dataset.\nConvert the AirPassengers time series object to a data frame.\nCreate a line graph with time on the x-axis and the number of passengers on the y-axis.\nAdd appropriate labels to the axes and a title to the graph.\n\nSolution:\n\n\nCode\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Convert AirPassengers to data frame\nairpassengers_df &lt;- data.frame(\n  time = time(AirPassengers),\n  passengers = as.numeric(AirPassengers)\n)\n\n# Create line graph\nggplot(airpassengers_df, aes(x = time, y = passengers)) +\n  geom_line() +\n  labs(title = \"Number of Air Passengers Over Time\",\n       x = \"Time\",\n       y = \"Number of Passengers\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 3: Line Graph of lynx Dataset\nTask: Create a line graph of the lynx dataset, which shows the annual numbers of lynx trappings from 1821–1934 in Canada.\nSteps: \n\nLoad the ggplot2 library.\nUse the lynx dataset.\nConvert the lynx time series object to a data frame.\nCreate a line graph with time on the x-axis and the number of lynx trapped on the y-axis.\nAdd appropriate labels to the axes and a title to the graph.\n\nSolution:\n\n\nCode\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Convert lynx to data frame\nlynx_df &lt;- data.frame(\n  year = time(lynx),\n  trappings = as.numeric(lynx)\n)\n\n# Create line graph\nggplot(lynx_df, aes(x = year, y = trappings)) +\n  geom_line() +\n  labs(title = \"Annual Numbers of Lynx Trappings\",\n       x = \"Year\",\n       y = \"Number of Lynx Trapped\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 4: Line Graph of economics Dataset\nTask: Create a line graph of the economics dataset, which shows the unemployment rate over time.\nSteps: \n\nLoad the ggplot2 library.\nUse the economics dataset.\nCreate a line graph with date on the x-axis and unemployment rate (unemploy/pop * 100) on the y-axis.\nAdd appropriate labels to the axes and a title to the graph.\n\nSolution:\n\n\nCode\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Create line graph\nggplot(economics, aes(x = date, y = unemploy / pop * 100)) +\n  geom_line() +\n  labs(title = \"Unemployment Rate Over Time\",\n       x = \"Date\",\n       y = \"Unemployment Rate (%)\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 5: Line Graph of co2 Dataset\nTask: Create a line graph of the co2 dataset, which shows the concentration of atmospheric carbon dioxide over time.\nSteps: \n\nLoad the ggplot2 library.\nUse the co2 dataset.\nConvert the co2 time series object to a data frame.\nCreate a line graph with time on the x-axis and CO2 concentration on the y-axis.\nAdd appropriate labels to the axes and a title to the graph.\n\nSolution:\n\n\nCode\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Convert co2 to data frame\nco2_df &lt;- data.frame(\n  time = time(co2),\n  concentration = as.numeric(co2)\n)\n\n# Create line graph\nggplot(co2_df, aes(x = time, y = concentration)) +\n  geom_line() +\n  labs(title = \"Atmospheric CO2 Concentration Over Time\",\n       x = \"Time\",\n       y = \"CO2 Concentration (ppm)\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 6: Pie Chart of Species Distribution in the Iris Dataset\nObjective: Create a pie chart to visualize the distribution of different species in the Iris dataset.\nSteps: \n\nLoad the ggplot2 library.\nUse the built-in iris dataset.\nCreate a data frame that counts the number of occurrences of each species.\nUse ggplot2 to create a pie chart displaying the species distribution.\n\nHints: - Use table to count the occurrences of each species. - Use geom_bar and coord_polar to create the pie chart.\nSolution:\n\n\nCode\n# Load ggplot2\nlibrary(ggplot2)\n\n# Load iris dataset\ndata(iris)\n\n# Count species occurrences\nspecies_count &lt;- as.data.frame(table(iris$Species))\n\n# Create pie chart\nggplot(species_count, aes(x = \"\", y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") + \n  geom_text(aes(label = Freq),\n        position = position_stack(vjust = 0.5),\n        color = \"white\", size = 5) +\n  theme_void() +\n  ggtitle(\"Distribution of Species in the Iris Dataset\") +\n  labs(fill = \"Species\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 7: Pie Chart of Gear Distribution in the Cars Dataset\nObjective: Create a pie chart to visualize the distribution of different gears in the cars dataset.\nSteps: \n\nLoad the ggplot2 library.\nUse the built-in cars dataset.\nCreate a data frame that counts the number of occurrences of each gear type.\nUse ggplot2 to create a pie chart displaying the gear distribution.\n\nHints: - Use table to count the occurrences of each gear type. - Use geom_bar and coord_polar to create the pie chart.\nSolution:\n\n\nCode\n# Load ggplot2\nlibrary(ggplot2)\n\n# Load cars dataset\ndata(mtcars)\n\n# Count gear occurrences\ngear_count &lt;- as.data.frame(table(mtcars$gear))\n\n# Create pie chart\n\nggplot(gear_count, aes(x = \"\", y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") + \n  geom_text(aes(label = Freq),\n        position = position_stack(vjust = 0.5),\n        color = \"white\", size = 5) +\n  theme_void() +\n  ggtitle(\"Distribution of Gears in the Cars Dataset\") +\n  labs(fill = \"Gears\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 8: Pie Chart of Titanic Class Distribution in the Titanic Dataset\nObjective: Create a pie chart to visualize the distribution of different classes in the Titanic dataset.\nSteps: \n\nLoad the ggplot2 library.\nUse the built-in Titanic dataset.\nCreate a data frame that counts the number of occurrences of each class.\nUse ggplot2 to create a pie chart displaying the class distribution.\n\nHints: - Use table to count the occurrences of each class. - Use geom_bar and coord_polar to create the pie chart.\nSolution:\n\n\nCode\n# Load ggplot2\nlibrary(ggplot2)\n\n# Load Titanic dataset\ndata(Titanic)\n\n# Convert Titanic dataset to data frame\ntitanic_df &lt;- as.data.frame(Titanic)\n\n# Count class occurrences\nclass_count &lt;- as.data.frame(table(titanic_df$Class))\n\n# Create pie chart\nggplot(class_count, aes(x = \"\", y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  theme_void() +\n  ggtitle(\"Distribution of Classes in the Titanic Dataset\") +\n  labs(fill = \"Class\")\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 9: Pie Chart of Supplement Distribution in the ToothGrowth Dataset\nTask: Create a pie chart to show the distribution of the different supplements (VC and OJ) in the ToothGrowth dataset.\nSteps: \n\nLoad the ggplot2 library and the ToothGrowth dataset.\nCreate a data frame that summarizes the count of each supplement type.\nUse ggplot2 to create a pie chart that shows the proportion of each supplement.\n\nHint: Use the fill aesthetic to map the supp column to the pie chart slices.\nSolution:\n\n\nCode\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Summarize count of each supplement type\nsupp_count &lt;- as.data.frame(table(ToothGrowth$supp))\n\n# Create pie chart\nggplot(supp_count, aes(x = \"\", y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Supplement Distribution in ToothGrowth Dataset\", fill = \"Supplement\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 10: Pie Chart of Education Levels in the infert Dataset\nTask: Create a pie chart to show the distribution of different education levels in the infert dataset.\nSteps: \n\nLoad the ggplot2 library and the infert dataset.\nCreate a data frame that summarizes the count of each education level.\nUse ggplot2 to create a pie chart that shows the proportion of each education level.\n\nHint: Use the fill aesthetic to map the education column to the pie chart slices.\nSolution:\n\n\nCode\n# Load ggplot2 and infert dataset\nlibrary(ggplot2)\ndata(infert)\n\n# Summarize count of each education level\neducation_count &lt;- as.data.frame(table(infert$education))\n\n# Create pie chart\nggplot(education_count, aes(x = \"\", y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") + \n  geom_text(aes(label = Freq),\n        position = position_stack(vjust = 0.5),\n        color = \"white\", size = 5) +\n  labs(title = \"Education Levels in Infert Dataset\", fill = \"Education Level\") +\n  theme_void()",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Line Graphs and Pie Charts"
    ]
  },
  {
    "objectID": "Line_Graphs_and_Pie_Charts.html#conclusion",
    "href": "Line_Graphs_and_Pie_Charts.html#conclusion",
    "title": "Line Graphs and Pie Charts",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we learned how to create line graphs and pie charts in R using the ggplot2 package. Line graphs are useful for showing trends over time and comparing multiple data series, while pie charts are useful for showing the relative proportions of different categories. By using ggplot2, we can create high-quality visualizations that help us understand and communicate data more effectively. I hope you found this tutorial helpful and that you can now create your own line graphs and pie charts in R. Thank you for reading!",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Line Graphs and Pie Charts"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization.html",
    "href": "Beginning_Data_Visualization.html",
    "title": "Beginning Data Visualization",
    "section": "",
    "text": "A Graphing Template\nLet’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings.\nggplot(data = &lt;DATA&gt;) +  \\(\\hspace{1in}\\) &lt;GEOM_FUNCTION&gt;( mapping = aes( &lt;MAPPINGS&gt; ) )\nThe rest of this assignment will show you how to complete and extend this template to make different types of graphs. We will begin with the &lt;MAPPINGS&gt; component.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Beginning Data Visualization"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization.html#aesthetic-mappings",
    "href": "Beginning_Data_Visualization.html#aesthetic-mappings",
    "title": "Beginning Data Visualization",
    "section": "Aesthetic Mappings",
    "text": "Aesthetic Mappings\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\n\nIn the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?  \n\n\n\n Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular). \nYou can add a third variable, such as class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue:  \n\n\n\n You can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. \n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\n\n(If you prefer British English, you can use colour instead of color.)\n\nTo map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\n\nThe colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines.\n\nAs an important note, you need to be careful about using quotation marks in the aes() function. If you put a variable name inside aes(), ggplot2 will look for that variable in the data argument. If you put a string inside aes(), ggplot2 will treat it as a constant value. \nFor example, that happens to the last command if we put color = \"class\" instead of color = class?\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = \"class\"))\n\n\n\n\n\n\n\n\nWe can see in this case that all the points are the same color, because ggplot2 is treating “class” as a constant value, not as a variable in the data frame. \nWe can also map different characteristics to the points, and not just color. In the above example, we mapped class to the color aesthetic, but we could have mapped class to the size aesthetic in the same way. In this case, the exact size of each point would reveal its class affiliation. We get a warning here, because mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea.\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, size=class))\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\nOr we could have mapped class to the alpha aesthetic, which controls the transparency of the points, or to the shape aesthetic, which controls the shape of the points.\n\n\nggplot(data=mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class))\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n What happened to the SUVs? ggplot2 will only use six shapes at a time. By default, additional groups will go unplotted when you use the shape aesthetic.\n\nFor each aesthetic, you use aes() to associate the name of the aesthetic with a variable to display. The aes() function gathers together each of the aesthetic mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves aesthetics, visual properties that you can map to variables to display information about the data. \nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values. \nYou can also set the aesthetic properties of your geom manually. For example, we can make all of the points in our plot blue:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\n\n Here, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an aesthetic manually, set the aesthetic by name as an argument of your geom function; i.e. it goes outside of aes(). You’ll need to pick a level that makes sense for that aesthetic:\n\nname of a color as a character string.\nThe size of a point in mm.\nThe shape of a point as a number, as shown in Figure 3.1.\n\n\n\n\n\n  As you start to run R code, you’re likely to run into problems. Don’t worry — it happens to everyone.\n Start by carefully comparing the code that you’re running to the code in the assignment. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every ” is paired with another “. Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\n One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start. In other words, make sure you haven’t accidentally written code like this: \n\n# ggplot(data=mpg)\n# + geom_point( mapping = aes(x = displ, y = hwy))\n\n If you’re still stuck, try the help. You can get help about any R function by running ?function_name in the console, or selecting the function name and pressing F1 in RStudio. Don’t worry if the help doesn’t seem that helpful - instead skip down to the examples and look for code that matches what you’re trying to do.\n If that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to R, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Beginning Data Visualization"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization.html#facets",
    "href": "Beginning_Data_Visualization.html#facets",
    "title": "Beginning Data Visualization",
    "section": "Facets",
    "text": "Facets\nReference : https://rpubs.com/uky994/583752 \nOne way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\n To facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() should be a formula, which you create with ~ followed by a variable name (here “formula” is the name of a data structure in R, not a synonym for “equation”). The variable that you pass to facet_wrap() should be discrete.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\n To facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~. \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\n If you prefer to not facet in the rows or columns dimension, use a \\(.\\) (period) instead of a variable name, e.g. \n\\(\\hspace{1in}\\) + facet_grid( . ~ cyl). \n  Additional help : https://www.youtube.com/watch?v=HPJn1CMvtmI",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Beginning Data Visualization"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization.html#exercises",
    "href": "Beginning_Data_Visualization.html#exercises",
    "title": "Beginning Data Visualization",
    "section": "Exercises",
    "text": "Exercises\n\nProblem 1\nRun ggplot(data = mpg). What do you see? Why do you think this is? \n\nggplot(data = mpg)\n\n\n\n\n\n\n\n\n\n\nProblem 2\nHow many rows are in mpg? How many columns? \n\nnrow(mpg)\n\n[1] 234\n\nncol(mpg)\n\n[1] 11\n\n\n\n\nProblem 3\nWhat does the drv variable describe? Use the str( ) function with ggplot2::mpg to see the different types of variables in the data set.\n\nstr(ggplot2::mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n# drv = the type of drive train, where \n#       f = front-wheel drive, \n#       r = rear wheel drive, \n#       4 = 4wd\n\n\n\nProblem 4\nMake a scatterplot of hwy vs cyl. \n\n# hwy = highway miles per gallon\n\n# cyl = number of cylinders in the car engine \n\n# Note that the higher the number of cylinders should cause more gas usage,\n# causing the mileage to go down.\n\nggplot(data = mpg, aes(x=cyl, y=hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n# or\n\nggplot(data=mpg) +\n  geom_point(aes(x=cyl, y=hwy))\n\n\n\n\n\n\n\n\n\n\nProblem 5\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful? \n\n# class = \"type\" of car\n\n# drv = the type of drive train, where \n#       f = front-wheel drive, \n#       r = rear wheel drive, \n#       4 = 4wd\n\nggplot(data=mpg, aes(x=class, y=drv)) +\n  geom_point()\n\n\n\n\n\n\n\n# This isn't particularly useful as most cars have multiple drive trains,\n# depending on what is offered for the vehicle.\n\n\n\nProblem 6\nConsider the following plot. Whay are the points not blue?\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = \"blue\")) + \n  geom_point( )\n\n\n\n\n\n\n\n# The problem is that the color is not mapped to a variable. In the mapping for\n# aesthtics, we are defining what the variables are. In this case, we are saying\n# x = displ, y = hwy, and color = \"blue\". This is saying that we have a variable\n# called \"x\", \"y\", and \"color\" and we are defining what they are. We are making\n# a new variable (column) and just filling it in with the text \"blue\". \n\n# We can change the color of the points in the geom_point function.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nProblem 7\nWhich variables in mpg are categorical? Which variables are continuous? Discrete?\n\nstr(ggplot2::mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n# From the output : \n\n# Categorical : manufacturer, model, trans, drv, fl, class\n\n# Continuous : displ, cty, hwy\n\n# Discrete : year, cyl\n\n\n\nProblem 8\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\n\n# Note that some quantitative variables are discrete and some are continuous.\n\n# Discrete : year, cyl\n\n# Continuous : displ, cty, hwy\n\n# We can use the previous ggplot and make some changes :\n\n# Here is the previous plot : \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\n# This already has a color to the points. We could change ALL the points\n# to a different color :\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"green\")\n\n\n\n\n\n\n\n# Note : UGH! Bad color!\n\n# What if we wanted to change the color based on the variable \"displ\"? \n\nggplot(data = mpg) + \n  geom_point(aes(x=displ, y=hwy, color=displ))\n\n\n\n\n\n\n\n# Notice the colors. You get different grades of the blue color. The larger\n# values of \"displ\" have a lighter color of blue and the smaller values \n# have a darker color. \n\n# The reason we don't have major changes in the color (blue, red, green, etc)\n# is because this is continuous (quantitative). Some values are \"slightly\" \n# different than others, so the color is \"slightly\" different to represent \n# that change.\n\n# Let's throw in a categorical variable. Let's compare hwy to drv\n\nggplot(data=mpg, aes(x=drv, y=hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n# This shows us the front wheel drive cars tend to get higher gas mileage\n# on the highway. \n\n# We can make this more interesting by coloring the dots by the manufacturer\n# of the cars to see if a certain brand typcially gets higher gas mileage.\n\nggplot(data=mpg, aes(x=drv, y=hwy, color = manufacturer)) +\n  geom_point()\n\n\n\n\n\n\n\n# or\n\nggplot(data=mpg) +\n  geom_point(aes(x=drv, y=hwy, color = manufacturer))\n\n\n\n\n\n\n\n# Notice that from the picture, \"audi\" seems to have the highest gas\n# mielage for their cars.\n\n# We could then have the dots get larger as the mileage goes up.So change \n# the size of the points based on the \"hwy\" variable.\n\nggplot(data=mpg) +\n  geom_point(aes(x=drv, y=hwy, color = manufacturer, size=hwy))\n\n\n\n\n\n\n\n\n\n\nProblem 9\nWhat happens if you map the same variable to multiple aesthetics?\n\n# Note that we just did this with the last example! \n\n# What this did kind of depends on your point of view. \n\n# In some ways it was redundant. We knew that the higher points represented\n# the higher mileage. By adding the aesthetic that the size of the points\n# goes up with the mileage, we don't get any new information.\n\n# On the other hand, this also reinforces the mileage data and makes it easier \n# to see that the bigger points means bigger mileage. \n\n\n\nProblem 10\nWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\n\n?geom_point\n\n# According to the help file, \"the stroke aesthetic to modify the width of the\n# border\"\n\n# Here is an example of a plot comparing \"wt\" to \"mpg\".\n\n# The points have shape 21 which is a circle with a border. The border of the\n# points is black and the interior is white : \n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(shape = 21, colour = \"black\", fill = \"white\", size = 5)\n\n\n\n\n\n\n\n# The \"stroke\" command perhaps refers to the size of the stroke of the pen \n# being used? This means a bigger stroke has a bigger outline?\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(shape = 21, colour = \"black\", fill = \"white\", size = 5, stroke = 5)\n\n\n\n\n\n\n\n\n\n\nProblem 11\nWhat happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? Note, you’ll also need to specify x and y.\n\n# Here is a basic plot comparing \"displ\" to \"hwy\"\n\nggplot(data=mpg) +\n  geom_point(aes(x=displ, y=hwy))\n\n\n\n\n\n\n\n# Let's add in the condition \n\nggplot(data=mpg) +\n  geom_point(aes(x=displ, y=hwy, color = displ&lt;5))\n\n\n\n\n\n\n\n# It appears to show us that we can differentiate two parts of the graph by \n# color. In other words, points below a threshold get one color and points\n# above the threshold get a different color.\n\n\n\nProblem 12\nWhat happens if you facet on a continuous variable?\n\n# Let's look at an example. Create a graph comparing displ (engine displacement, \n# in litres) to hwy (highway miles per gallon), but faceted with cty (city miles \n# per gallon) : \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ cty)\n\n\n\n\n\n\n\n# The continuous variable is converted to a categorical variable, and \n# the plot contains a facet for each distinct value.\n\n\n\nProblem 13\nWhat do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl)) +\n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n# When looking at this graph, there are several blank facets\n\n# How do they relate to this plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\n\n\n\n\n\n\n\n# Each blank facet corresponds to the locations where there is no point.\n\n\n\nProblem 14\nWhat plots does the following code make? What does . do?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n# The symbol . ignores that dimension when faceting. For example, \n# drv ~ . facet by values of drv on the y-axis\n\n# While, . ~ cyl will facet by values of cyl on the x-axis.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n\n\n\n\n\n\n\nProblem 15\nConsider the first faceted plot in this section. What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n# What are the advantages to using faceting instead of the colour aesthetic? \n\n# There are sometimes so many different colors / variables that the points can start to run\n# together and it is difficult to differentiate the variables. A facet can break up the main\n# data set by the variables themselves and make it easier to compare one variable to another.\n\n# What are the disadvantages? \n\n\n# How might the balance change if you had a larger dataset?\n\n\n\nProblem 16\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\n\n# Answer\n\n\n\nProblem 17\nWhen using facet_grid() you should usually put the variable with more unique levels in the columns. Why?\n\n# Answer",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Beginning Data Visualization"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html",
    "href": "Measures_of_Central_Tendencies.html",
    "title": "Measures Of Central Tendencies",
    "section": "",
    "text": "The Mean\nWhen we talk about the typical, or “average”, value of some variable measured on a continuous scale, we are usually talking about the mean value of that variable. To be even more specific, we are usually talking about the arithmetic mean value. This value has some favorable characteristics that make it a good description of central tendency.\nHowever, there are a couple of potentially problematic characteristics of the mean as well:\nLet’s assume we have a data set with \\(n\\) points and we label them as \\(a_1\\), \\(a_2\\), . . . \\(a_n\\). Here is the fancy, schmancy formula for finding the Average / Mean :\nThe capital sigma there is a mathematical symbol that tells us to sum up what ever follows the sigma and we will then divide by \\(n\\) which is the number of points we have in the data set. It can be said a little easier as :\nCaution : The mean is a nice way to get a value that describes the centrality of a data set, but it can sometimes be problematic if there are values that drastically higher or lower than most of the values in a data set.\nConsider this data set :\nWe can quickly calculate the average to be \\[(1 + 2 + 3 + \\dots +9\\,)\\,/\\,9 = \\frac{45}{9}=5\\] which is right smack dab in the middle of the data set. 50% of the scores are above and below the value for the average, os it is a very good measure of centrality for this set.\nHowever, adding a single value can sometimes make the average a poor choice for measure of centrality. Consider adding the value \\(100\\) to the previous data set :\nCalculating the average for this data set yields as average of : \\[(1 + 2 + 3 + \\dots +9+100\\,)\\,/\\,9 = \\frac{145}{10} = 14.5\\] In this example, the mean is actually larger than 90% of all the scores in the data set. That means it is a poor choice as a representative value for the data set.\nNote that scores that are far away from most of the values in a data set are called outliers and as you will see, even a single outlier can vastly affect the mean. This means that the mean is not resistant to outliers!\nThis is one of the reasons why it is important that there is more than one option as a measure of centrality.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#the-mean",
    "href": "Measures_of_Central_Tendencies.html#the-mean",
    "title": "Measures Of Central Tendencies",
    "section": "",
    "text": "For starters it’s simple. Most people are familiar with the mean, and at the very least, have some intuitive sense of what it means (no pun intended).\nIn addition, there can be only one mean value for any set of values.\n\n\n\nIt’s susceptible to extreme values in your data. In other words, a couple of people with very atypical values for the characteristic you are interested in can drastically alter the value of the mean, and your estimate for the typical person in your population of interest along with it.\nAdditionally, it’s highly likely to calculate a mean value that is not actually observed anywhere in your data. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(1 \\,\\,\\,2 \\,\\,\\,3\\,\\,\\,4\\,\\,\\,5\\,\\,\\,6\\,\\,\\,7\\,\\,\\,8\\,\\,\\,9\\)\n\n\n\n\n\n\\(1 \\,\\,\\,2 \\,\\,\\,3\\,\\,\\,4\\,\\,\\,5\\,\\,\\,6\\,\\,\\,7\\,\\,\\,8\\,\\,\\,9\\,\\,\\,100\\)",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#the-median",
    "href": "Measures_of_Central_Tendencies.html#the-median",
    "title": "Measures Of Central Tendencies",
    "section": "The Median",
    "text": "The Median\n\nThe median is probably the second most commonly used measure of central tendency.\n\nLike the mean, it’s computationally simple and relatively straightforward to understand.\nThere can be one, and only one, median.\nAnd, its value may also be unobserved in the data. \n\nThe idea of finding the Median is fairly simple : \n\nLine up the values from smallest to largest\nThe value that occurs in the middle will be the median\nThis means the median scores has half of the values below it and half the values above it, when you exclude the spot representing the median. \n\nThere is a small caveat depending on if there is an even or odd amount of values in the data set. Let’s assume there are n values in the data set. \nLet’s assume n is odd. Consider this example where we have a data set with 7 values, where the data has already been arranged from smallest to largest: \n\n\n\n\nWhen n is odd, the median is the value located in spot (n+1)/2. So in this case the median is located in spot \\((7+1)/2 = 4\\)th spot : \n\n\n\n\nNotice that when we exclude the median, we have just as many values below the median (3) as we have above the median (3). \nImportant : The median is a spot-based measure. We are looking for the value that occurs in the middle spot. In this example, the median was in spot 4 with the median taking on the value of 18. \nIt is entirely possible that the median takes on a value below or above this spot if there are repeated values. Consider a similar data set : \n\n\\(8\\,\\,\\, 11\\,\\,\\,  15\\,\\,\\,  15\\,\\,\\,  24\\,\\,\\,  30\\,\\,\\,  31\\)\n\n\nIn this example, there are still 7 values in the data set so the median is still located in spot 4 : \n\n\\(8\\,\\,\\, 11\\,\\,\\,  15\\,\\,\\,  \\underline{\\,\\,15\\,\\,}\\,\\,\\,  24\\,\\,\\,  30\\,\\,\\,  31\\)\n\n\nIn this case, the median takes on the value in the fourth spot, namely 15. We still have three values below the median : \\(8\\,\\,\\, 11 \\,\\,\\, 15\\,\\) and three values above the median : \\(24 \\,\\,\\, 30\\,\\,\\, 31\\,\\). So even though we have a value for the median that is also in a different spot of the data set, we still have 50% of the scores above the median and 50% of the scores below the median. This means that the important aspect of finding the median is finding the spot that is in the middle and that will inform us on the value of the median. \nWhat happens when n is even? This complicates the process a bit because there is not a single value that falls in the middle. Consider this example of an already ordered set of six values : \n\n\n\n\nAs you can see above, we don’t have a single spot that is right in the middle that we can call the median. However, we do have two values in the middle. In this case, those values are 18 and 24. To get the median, we will simply find the average of these two values to create the median.\nFinding the two middle spots is simple. The first value is located in spot \\((n\\,/\\,2)\\) and the second spot in lcoated immediatley following, \\((n\\,/\\,2) + 1\\). In the following example we have a data set with \\(6\\) values. Since this is an even amount of values, we will work with the values in spots \\((6\\,/\\,2\\,) = 3\\) and \\((6\\,/\\,2\\,) + 1 = 4\\) \n\n\n\n\nAs you can see spot 3 has the value \\(18\\) and spot 4 has the value \\(24\\). The median is the average of these two values. Therefore the median is \\((18 + 24)/2 = (42) / 2 = 21\\). \nNote that in this case we still have the same amount of scores below the median as we have above the median. This shows us that 50% of the data set is below this value and 50% above this value which is the definition of median. \nAn interesting result is that in this case the median is not a value from the data set! If you look at the orginal six values in the data set, 21 is not one of them! If you have a data set with an even amount of values, this will almost always be the case. THe only time where the median will actually be a value on in the data set is if the two values in the middle happen to be the exact same value. Otherwise, the median will be a value not in the original data set. \nLet’s revisit an issue we discovered when discussing averages, namely outliers. How do outliers affect the median?\nLet calculate the median for the following two data sets : \n\n\\(1 \\,\\,\\,2 \\,\\,\\,3\\,\\,\\,4\\,\\,\\,5\\,\\,\\,6\\,\\,\\,7\\,\\,\\,8\\,\\,\\,9\\)\n\n In this example, there are \\(9\\) scores, which tells us the median is located in spot \\(\\frac{9+1}{2} = \\frac{10}{2}= 5\\). Spot 5 happens to contain the value \\(5\\). Notice that this stil has 50% of the scores below and 50% of the scores above the median which tells us it is a good representative for the measure of centrality. \nWhat does adding an outlier do? How does it affect the median? Let’s explore and find out. \n\n\\(1 \\,\\,\\,2 \\,\\,\\,3\\,\\,\\,4\\,\\,\\,5\\,\\,\\,6\\,\\,\\,7\\,\\,\\,8\\,\\,\\,9\\,\\,\\,100\\)\n\n\nWe now have 10 scores which means we want to take the average of the tow middle scores in spots \\(\\frac{10}{2} = 5\\) and \\(\\frac{10}{2} + 1 = 6\\).\n\n\\(1 \\,\\,\\,2 \\,\\,\\,3\\,\\,\\,4\\,\\,\\underline{\\,5\\,}\\,\\,\\,\\underline{\\,6\\,}\\,\\,7\\,\\,\\,8\\,\\,\\,9\\,\\,\\,100\\)\n\n The average of these two values gives us \\(\\frac{5 + 6}{2} = 5.5\\). Again, think about the definition of a measure of centrality. If you consider the median to be 5.5, then notice there are 5 values below the median and 5 valuse above the median, showing us that this is a good representative for the measure of centrality.\nThis tells us that outliers do not affect the median very much at all. The median moved from \\(5\\) up to \\(5.5\\). What this shows us is that medians are resistant to outiers! \n\n\n\n\n\n\nImportant\n\n\n\nThis means that if we have a data set that contains outliers, we need to consider using the median as the measure of centrality and not the mean. This raises the question of when does a data set have outliers, and this will be discussed when we talk about Measures of Spread.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#the-mode",
    "href": "Measures_of_Central_Tendencies.html#the-mode",
    "title": "Measures Of Central Tendencies",
    "section": "The Mode",
    "text": "The Mode\nAnd finally, we have the mode, or the value that is most often observed in the data. It doesn’t get much simpler than that. But, unlike the mean and the median, there can be more than one mode for a given set of values. In fact, there can even be no mode if all the values are observed the exact same number of times. However, if there is a mode, by definition it’s observed in the data. \nConsider these examples : \nMode Example 1 \n\n\\(8\\,\\,\\, 11\\,\\,\\,  15\\,\\,\\,  15\\,\\,\\,  24\\,\\,\\,  30\\,\\,\\,  31\\)\n\n\nIn this data set, then value \\(15\\) appears twice and everything else appears once. Therefore the mode of this data set is \\(15\\). \nMode Example 2 \n\n\\(8\\,\\,\\, 11\\,\\,\\,  15\\,\\,\\,  15\\,\\,\\,  24\\,\\,\\,  30\\,\\,\\,  30\\,\\,\\ 31\\)\n\n\nIn this data set, then value \\(15\\) and \\(30\\) appear twice and everything else appears once. Therefore the mode of this data set is \\(15\\) and \\(30\\). \nMode Example 3 \n\n\\(8\\,\\,\\, 11\\,\\,\\,  12\\,\\,\\,  15\\,\\,\\,  24\\,\\,\\,  30\\,\\,\\,  33\\,\\,\\ 37\\)\n\n\nIn this data set, no value appears more than any other value, so this data set does not have a mode!",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#summary",
    "href": "Measures_of_Central_Tendencies.html#summary",
    "title": "Measures Of Central Tendencies",
    "section": "Summary",
    "text": "Summary\nHere is a graphic that reviews what we have discussed so far :",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#calculating-the-mean-and-median",
    "href": "Measures_of_Central_Tendencies.html#calculating-the-mean-and-median",
    "title": "Measures Of Central Tendencies",
    "section": "Calculating the Mean and Median",
    "text": "Calculating the Mean and Median\n\nNow that we are all on the same page with respect to the fundamentals of central tendency, let’s remind ourselves how to calculate these measures using R. \nCalculating the mean is really straightforward. We can just use base R’s built-in mean( ) function.\nLoad the dplyr package. We will need several dplyr functions and can install and load the package with the code below.\n\n# Here is the command of we need to install the package. \n\n# install.packages(\"dplyr\")\n\n# We can load the dplyr pack :\n\nlibrary(dplyr)\n\n\nWe can simulate some data by typing (cut and paste?) the following into your code:\n\n# Load up tibble, if needed\n\nlibrary(tibble)\n\n# We can now create the tibble\n\nheight_and_weight_20 &lt;- tribble(\n  ~id,   ~sex,     ~ht_in, ~wt_lbs,\n  \"001\", \"Male\",   71,     190,\n  \"002\", \"Male\",   69,     177,\n  \"003\", \"Female\", 64,     130,\n  \"004\", \"Female\", 65,     153,\n  \"005\", NA,       73,     173,\n  \"006\", \"Male\",   69,     182,\n  \"007\", \"Female\", 68,     186,\n  \"008\", NA,       73,     185,\n  \"009\", \"Female\", 71,     157,\n  \"010\", \"Male\",   66,     155,\n  \"011\", \"Male\",   71,     213,\n  \"012\", \"Female\", 69,     151,\n  \"013\", \"Female\", 66,     147,\n  \"014\", \"Female\", 68,     196,\n  \"015\", \"Male\",   75,     212,\n  \"016\", \"Female\", 69,     19000,\n  \"017\", \"Female\", 66,     194,\n  \"018\", \"Female\", 65,     176,\n  \"019\", \"Female\", 65,     176,\n  \"020\", \"Female\", 65,     102\n)\n\n\nIn the code above, here are what the steps were :\n\nWe loaded the tibble package so that we could use its tribble() function.\nWe used the tribble( ) function to simulate some data – heights and weights for 20 hypothetical students.\n\nThe tribble( ) function creates something called a tibble. A tibble is the tidyverse version of a data frame. In fact, it is a data frame, but with some additional functionality. You can use the link to read more about it if you’d like.\n\nMore information on tribble( ) vs tibble( ) vs data.frame  \n\nWe used the tribble( ) function instead of the data.frame( ) function to create our data frame above because we can use the tribble( ) function to create our data frames in rows (like you see above) instead of columns with the c( ) function.\nUsing the tribble( ) function to create a data frame isn’t any better or worse than using the data.frame( ) function. I just wanted you to be aware that it exists and is sometimes useful.\n\n\n\nMEAN Example : Find the mean of the heights in the data frame.\n\nThe name of the tibble is height_and_weight_20.\nThe name of the variable we want to use is the column ht_in\nThe way we can access a variable (column) of a data fram or tibble uses syntax such as data_frame_name$variable_name\nTherefore we want to find the mean of height_and_weight_20$ht_in\nWe will use the command mean( ) as follows :\n\n\nmean(height_and_weight_20$ht_in)\n\n[1] 68.4\n\n\n\nMEDIAN Example : Find the median of the heights in the data frame. \nIf we were going to do this by hand, we would first need to sort the data. We can accompish this using the sort( ) command. It will sort the list and the default listing is smallest to largest:\n\nsort(height_and_weight_20$ht_in)\n\n [1] 64 65 65 65 65 66 66 66 68 68 69 69 69 69 71 71 71 73 73 75\n\n\n\nBecause we have an even amount of terms, we would then locate the two middle values and find their average. We have 20 values in this data set, so we are looking at spots 10 and 11.\n\n\\(64\\,\\,\\, 65\\,\\,\\, 65\\,\\,\\, 65\\,\\,\\, 65\\,\\,\\, 66\\,\\,\\, 66\\,\\,\\, 66\\,\\,\\, 68\\,\\,\\underline{\\, 68\\,}\\,\n\\underline{\\, 69\\,}\\,\\,\n69\\,\\,\\, 69\\,\\,\\, 69\\,\\,\\, 71\\,\\,\\, 71\\,\\,\\, 71\\,\\,\\, 73\\,\\,\\, 73\\,\\,\\, 75\\)\n\n\nThe median is the average of these two values which says the median is \\(\\frac{68+69}{2} = \\frac{137}{2} = 68.5\\)\nWe clearly don’t want to do this if we have a large amount of values in the data set. We will use the median( ) function to help us out.\n\nmedian(height_and_weight_20$ht_in)\n\n[1] 68.5",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#what-about-the-mode",
    "href": "Measures_of_Central_Tendencies.html#what-about-the-mode",
    "title": "Measures Of Central Tendencies",
    "section": "What About The Mode?",
    "text": "What About The Mode?\n\nBase R does not have a built-in mode( ) function. Well, it actually does have a mode( ) function, but for some reason that function does not return the mode value(s) of a set of numbers. Instead, the mode( ) function gets or sets the type or storage mode of an object. For example:\n\nmode(height_and_weight_20$ht_in)\n\n[1] \"numeric\"\n\n\n This is clearly not what we are looking for. So, how do we find the mode value(s)? Go back to the slides from the lesson to see how we found this.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#quickly-compare-mean-and-median",
    "href": "Measures_of_Central_Tendencies.html#quickly-compare-mean-and-median",
    "title": "Measures Of Central Tendencies",
    "section": "Quickly Compare Mean And Median",
    "text": "Quickly Compare Mean And Median\n\nNow that you know how to calculate the mean and the median, let’s compare these three measures of central tendency. This is a good opportunity to demonstrate some of the different characteristics of each that we spoke about earlier. Try the following code :\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_weight    = min(ht_in),\n    mean_weight   = mean(ht_in),\n    median_weight = median(ht_in),\n    max_weight    = max(ht_in)\n  )\n\n# A tibble: 1 × 4\n  min_weight mean_weight median_weight max_weight\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1         64        68.4          68.5         75\n\n\n\nThis example shows you how you can find the min( ), mean( ), median( ), and max( ) values in a data set. The summarise( ) command just prints them out nicely for us as a \\(1 \\times 4\\) tibble. \nThis also allows us to quickly compare the values of the mean and the median. This can sometimes give us hints as to if we have any extreme values (outliers) in our data. If you remember, outiers can really affect the mean. Therefore, if we have a high outlier, it would pull the mean up much more than the median. If we have a low outlier, then the mean would be pulled down much more than the median. \nSo in the previous example, where the mean is 68.4 and the median is 68.5 then we have one of two options for us:\n\nThere are no outliers in the data set\nThere are the same amount of high and low outliers balancing each other out, or perhaps a bi-modal distribution. \n\nThis tells us that if the mean and median are not claose to each other, then there are some values that are affecting the mean. These could be outliers or they could be something as mundane as a data entry error. By running these comparisons, we can sometimes notice there is a problem with the data set. \nFor example, let’s do a quick summary for the weights instead of the heights and examine the output.\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_weight    = min(wt_lbs),\n    mean_weight   = mean(wt_lbs),\n    median_weight = median(wt_lbs),\n    max_weight    = max(wt_lbs)\n  )\n\n# A tibble: 1 × 4\n  min_weight mean_weight median_weight max_weight\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1        102       1113.          176.      19000\n\n\n\nDo you see any red flags as you scan the results? Do you really think a mean weight of 1,113 pounds sounds reasonable? This should definitely be a red flag for you. Now move your gaze three columns to the right and notice that the maximum value of weight is 19,000 lbs – an impossible value for a study in human populations. In this case the real weight was supposed to be 190 pounds, but the person entering the data accidently got a little trigger-happy with the zero key. \nThis is an example of how we can use descriptive analysis to uncover errors in our data. Oftentimes, for various reasons, some observations for a given variable take on values that don’t make sense. Starting by calculating some basic descriptive statistics for each variable is one approach you can use to try to figure out if you have values in your data that don’t make sense. \nIn this case we can just go back and fix our data, but what if we didn’t know this value was an error? What if it were a value that was technically possible, but very unlikely? Well, we can’t just change values in our data. It’s unethical, and in some cases illegal. Below, we discuss how the properties of the median and mode can come in handy in situations such as this.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#properties-of-mean-median-and-mode",
    "href": "Measures_of_Central_Tendencies.html#properties-of-mean-median-and-mode",
    "title": "Measures Of Central Tendencies",
    "section": "Properties of mean, median, and mode",
    "text": "Properties of mean, median, and mode\n\nDespite the fact that this impossibly extreme value is in our data, the median and mode estimates are reasonable estimates of the typical person’s weight in this sample. This is what is meant when it is said that the median and mode were more “resistant to extreme values” than the mean. \nYou may also notice that no person in our sample had an actual weight of 1,113 (the mean) or even 176 (the median). This is what I meant above when I said that the mean and median values are “not necessarily observed in the data.” \nIn this case, the mode value (176) is also a more reasonable estimate of the average person’s weight than the mean. And unlike the mean and the median, participants 18 and 19 actually weigh 176 pounds. I’m not saying that the mode is always the best measure of central tendency to use. However, I am saying that you can often learn useful information from your data by calculating and comparing these relatively simple descriptive statistics on each of your numeric variables.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#missing-data",
    "href": "Measures_of_Central_Tendencies.html#missing-data",
    "title": "Measures Of Central Tendencies",
    "section": "Missing Data",
    "text": "Missing Data\nWe can use the dplyr::filter( ) function to remove all the rows from our data frame that contained a missing value for any of our variables of interest. This is called a complete case analysis. This method should pretty much always work, but in this section I’m going to show you an alternative method for dropping missing values from your analysis that you are likely to come across often when reading R documentation – the na.rm argument. \nMany R functions that perform calculations on numerical variables include an na.rm – short for “Remove NA” – argument. By default, this argument is typically set to FALSE. By passing the value TRUE to this argument, we can perform a complete case analysis. Let’s quickly take a look at how it works. \nWe already saw that we can calculate the mean value of a numeric vector using the mean( ) function. For instance, if I wanted to find the average of the three numbers 34, 87, 23, I could create a vector of these three numbers and then put the vector into the mean( ) function.\n\nmean(c(34, 87, 23))\n\n[1] 48\n\n\n But what happens when our vector has a missing value?\n\nmean(c(34, 87, NA))\n\n[1] NA\n\n\n As you can see, the mean( ) function returns NA by default when we pass it a numeric vector that contains a missing value. It took me a little while to wrap my head around why this is the case when I was a student. Perhaps some of you are confused as well. The logic goes something like this. \nIn R, an NA doesn’t represent the absence of a value – a value that doesn’t exist at all; rather, it represents a value that does exist, but is unknown to us. So, if I ask you to tell me the mean of a set of numbers that contains 34, 87, and some unknown number what would your answer be? Well, you can’t just give me the mean of 34 and 87. That would imply that the unknown number doesn’t exist. Further, you can’t really give me any numeric answer because that answer will depend on the value of the missing number. So, the only logical answer to give me is something like “I don’t know” or “it depends.” That is essentially what R is telling us when it returns an NA. \nWhile this answer is technically correct, it usually isn’t very satisfying to us. Instead, we often want R to calculate the mean of the numbers that remain after all missing values are removed from the original set. The implicit assumption is that the mean of that reduced set of numbers will be “close enough” to the mean of the original set of numbers for our purposes. We can ask R to do this by changing the value of the na.rm argument from FALSE – the default – to TRUE. \n\nmean(c(34, 87, NA), na.rm = TRUE)\n\n[1] 60.5\n\n\n\nFinally, let’s work with mutate( ) and `na.rm = TRUEin  a **dplyr** pipeline. We will first use thereplace( )`` function to add some missing values to our height_and_weight_20 data. (Remember to make sure the dplyr package is loaded up so we can use the mutate( ) and %&gt;% commands.) \nNote : Here is more information on the mutate( ) command. It basically either (a) creates a new variable, which means we are adding a column to the data set or (b) manipulates a current variable, as we see below : \n\nheight_and_weight_20 &lt;- height_and_weight_20 %&gt;% \n  mutate(ht_in = replace(ht_in, c(1, 2), NA)) %&gt;% \n  print()\n\n# A tibble: 20 × 4\n   id    sex    ht_in wt_lbs\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 001   Male      NA    190\n 2 002   Male      NA    177\n 3 003   Female    64    130\n 4 004   Female    65    153\n 5 005   &lt;NA&gt;      73    173\n 6 006   Male      69    182\n 7 007   Female    68    186\n 8 008   &lt;NA&gt;      73    185\n 9 009   Female    71    157\n10 010   Male      66    155\n11 011   Male      71    213\n12 012   Female    69    151\n13 013   Female    66    147\n14 014   Female    68    196\n15 015   Male      75    212\n16 016   Female    69  19000\n17 017   Female    66    194\n18 018   Female    65    176\n19 019   Female    65    176\n20 020   Female    65    102\n\n\n Here’s what we did in this command :\n\n“height_and_weight_20 %&gt;%”\n\nThis says to take the data set and “pipe it” into the next command, which is the mutate( ) command.\n\n“mutate(ht_in = replace(ht_in, c(1,2), NA)) %&gt;%”\n\nSince the variable ht_in already exists in this data frame, we are not going to create a new column (variable)\nWe are then going to look at the variable ht_in, look at spots 1 (71) and 2 (69), and then replace them with an NA.\n\nWe then piped this result to the print( ) command which printed out the modified data set so we could verify the results. We didn’t absolutely need to do this as we could also have examined the variable in the Environment pane on Posit Cloud, but it does look nice to see it.\n“height_and_weight_20 &lt;-”\n\nThis command tells us to store the result back in the variable “height_and_weight-20”\n\n\n\nThe height variable now has a couple of NA values. What happens if we try to do a quick summary of the variable? \n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_height    = min(ht_in),\n    mean_height   = mean(ht_in),\n    median_height = median(ht_in),\n    max_height    = max(ht_in)\n  )\n\n# A tibble: 1 × 4\n  min_height mean_height median_height max_height\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1         NA          NA            NA         NA\n\n\n\nThose NA’s really messed up our analysis. We can try again, only let’s not include the NA values by changing the na.rm argument to TRUE.\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_height    = min(ht_in, na.rm = TRUE),\n    mean_height   = mean(ht_in, na.rm = TRUE),\n    median_height = median(ht_in, na.rm = TRUE),\n    max_height    = max(ht_in, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 4\n  min_height mean_height median_height max_height\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1         64        68.2            68         75\n\n\n\nThese are just a few of hte ways we can handle data set swith extreme or missing values. We will delve more into this when we dive deeper into data cleaning.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Central_Tendencies.html#exercises",
    "href": "Measures_of_Central_Tendencies.html#exercises",
    "title": "Measures Of Central Tendencies",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will practice reading in data, calculating descriptive statistics (mean and median), handling missing values (NA), and using the mutate function to create new data columns. You will use built-in datasets from R for this assignment.\n\nProblem 1: Mean and Median of Sepal Length in the iris Dataset\nTask: Calculate the mean and median of the Sepal.Length column in the iris dataset. \nSteps:\n\nLoad the iris dataset.\nCalculate the mean and median of the Sepal.Length column.\n\nCode Example:\n\n\nCode\n# Load iris dataset\ndata(iris)\n\n# Calculate mean and median of Sepal.Length\nmean_sepal_length &lt;- mean(iris$Sepal.Length)\nmedian_sepal_length &lt;- median(iris$Sepal.Length)\n\nmean_sepal_length\n\n\n[1] 5.843333\n\n\nCode\nmedian_sepal_length\n\n\n[1] 5.8\n\n\n\n\nProblem 2: Mean and Median of Ozone Levels in the airquality Dataset\nTask: Calculate the mean and median of the Ozone column in the airquality dataset after removing NA values. \nSteps:\n\nLoad the airquality dataset.\nRemove NA values from the Ozone column.\nCalculate the mean and median of the Ozone column.\n\nCode Example: \n\n\nCode\n# Load airquality dataset\ndata(airquality)\n\n# Remove NA values from Ozone column\nozone_no_na &lt;- na.omit(airquality$Ozone)\n\n# Calculate mean and median of Ozone\nmean_ozone &lt;- mean(ozone_no_na)\nmedian_ozone &lt;- median(ozone_no_na)\n\nmean_ozone\n\n\n[1] 42.12931\n\n\nCode\nmedian_ozone\n\n\n[1] 31.5\n\n\n\n\nProblem 3: Mean and Median of Annual Lynx Trappings in the lynx Dataset\nTask: Calculate the mean and median of the annual number of lynx trapped in the lynx dataset. \nSteps:\n\nLoad the lynx dataset.\nCalculate the mean and median of the lynx trappings.\n\nCode Example:\n\n\nCode\n# Load lynx dataset\ndata(lynx)\n\n# Calculate mean and median of lynx trappings\nmean_lynx &lt;- mean(lynx)\nmedian_lynx &lt;- median(lynx)\n\nmean_lynx\n\n\n[1] 1538.018\n\n\nCode\nmedian_lynx\n\n\n[1] 771\n\n\n\n\nProblem 4: Mean and Median of Tooth Length in the ToothGrowth Dataset\nTask: Calculate the mean and median of the len column in the ToothGrowth dataset. \nSteps:\n\nLoad the ToothGrowth dataset.\nCalculate the mean and median of the len column.\n\nCode Example:\n\n\nCode\n# Load ToothGrowth dataset\ndata(ToothGrowth)\n\n# Calculate mean and median of len\nmean_len &lt;- mean(ToothGrowth$len)\nmedian_len &lt;- median(ToothGrowth$len)\n\nmean_len\n\n\n[1] 18.81333\n\n\nCode\nmedian_len\n\n\n[1] 19.25\n\n\n\n\nProblem 5: Mean and Median of Age in the infert Dataset\nTask: Calculate the mean and median of the age column in the infert dataset. \nSteps:\n\nLoad the infert dataset.\nCalculate the mean and median of the age column.\n\nCode Example:\n\n\nCode\n# Load infert dataset\ndata(infert)\n\n# Calculate mean and median of age\nmean_age &lt;- mean(infert$age)\nmedian_age &lt;- median(infert$age)\n\nmean_age\n\n\n[1] 31.50403\n\n\nCode\nmedian_age\n\n\n[1] 31\n\n\n\n\nProblem 6: Mean and Median of Wind Speed in the airquality Dataset\nTask: Calculate the mean and median of the Wind column in the airquality dataset after removing NA values. \nSteps:\n\nLoad the airquality dataset.\nRemove NA values from the Wind column.\nCalculate the mean and median of the Wind column.\n\nCode Example:\n\n\nCode\n# Load airquality dataset\ndata(airquality)\n\n# Remove NA values from Wind column\nwind_no_na &lt;- na.omit(airquality$Wind)\n\n# Calculate mean and median of Wind\nmean_wind &lt;- mean(wind_no_na)\nmedian_wind &lt;- median(wind_no_na)\n\nmean_wind\n\n\n[1] 9.957516\n\n\nCode\nmedian_wind\n\n\n[1] 9.7\n\n\n\n\nProblem 7: Create a New Column and Calculate Mean and Median in the iris Dataset\nTask: Create a new column Sepal.Area as the product of Sepal.Length and Sepal.Width in the iris dataset. Calculate the mean and median of the Sepal.Area column. \nSteps:\n\nLoad the iris dataset.\nCreate a new column Sepal.Area.\nCalculate the mean and median of the Sepal.Area column.\n\nCode Example:\n\n\nCode\n# Load iris dataset\nlibrary(dplyr)\ndata(iris)\n\n# Create new column Sepal.Area\niris &lt;- mutate(iris, Sepal.Area = Sepal.Length * Sepal.Width)\n\n# Calculate mean and median of Sepal.Area\nmean_sepal_area &lt;- mean(iris$Sepal.Area)\nmedian_sepal_area &lt;- median(iris$Sepal.Area)\n\nmean_sepal_area\n\n\n[1] 17.82287\n\n\nCode\nmedian_sepal_area\n\n\n[1] 17.66\n\n\n\n\nProblem 8: Mean and Median of Temperature in the airquality Dataset\nTask: Calculate the mean and median of the Temp column in the airquality dataset after removing NA values. \nSteps:\n\nLoad the airquality dataset.\nRemove NA values from the Temp column.\nCalculate the mean and median of the Temp column.\n\nCode Example:\n\n\nCode\n# Load airquality dataset\ndata(airquality)\n\n# Remove NA values from Temp column\ntemp_no_na &lt;- na.omit(airquality$Temp)\n\n# Calculate mean and median of Temp\nmean_temp &lt;- mean(temp_no_na)\nmedian_temp &lt;- median(temp_no_na)\n\nmean_temp\n\n\n[1] 77.88235\n\n\nCode\nmedian_temp\n\n\n[1] 79\n\n\n\n\nProblem 9: Create a New Column and Calculate Mean and Median in the ToothGrowth Dataset\nTask: Create a new column Dose.Milligrams as the product of dose and len in the ToothGrowth dataset. Calculate the mean and median of the Dose.Milligrams column. \nSteps:\n\nLoad the ToothGrowth dataset.\nCreate a new column Dose.Milligrams.\nCalculate the mean and median of the Dose.Milligrams column.\n\nCode Example:\n\n\nCode\n# Load ToothGrowth dataset\nlibrary(dplyr)\ndata(ToothGrowth)\n\n# Create new column Dose.Milligrams\nToothGrowth &lt;- mutate(ToothGrowth, Dose.Milligrams = dose * len)\n\n# Calculate mean and median of Dose.Milligrams\nmean_dose_mg &lt;- mean(ToothGrowth$Dose.Milligrams)\nmedian_dose_mg &lt;- median(ToothGrowth$Dose.Milligrams)\n\nmean_dose_mg\n\n\n[1] 25.74583\n\n\nCode\nmedian_dose_mg\n\n\n[1] 19.25\n\n\n\n\nProblem 10: Mean and Median of Child Count in the infert Dataset\nTask: Calculate the mean and median of the parity column in the infert dataset. \nSteps:\n\nLoad the infert dataset.\nCalculate the mean and median of the parity column.\n\nCode Example:\n\n\nCode\n# Load infert dataset\ndata(infert)\n\n# Calculate mean and median of parity\nmean_parity &lt;- mean(infert$parity)\nmedian_parity &lt;- median(infert$parity)\n\nmean_parity\n\n\n[1] 2.092742\n\n\nCode\nmedian_parity\n\n\n[1] 2",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Central Tendencies"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html",
    "href": "Measures_of_Variability.html",
    "title": "Measures Of Variability",
    "section": "",
    "text": "Range\nThe range is the simplest measure of variability. It is the difference between the largest and smallest values in a data set. It is easy to calculate, but it is also very sensitive to outliers. If you have a data set with a large range relative to its values, then it is likely that you have an outlier in your data set. Here is how you can calculate the range for the two data sets we have been working with.\n# Calculate the range for data1\n\nrange(data1)\n\n[1] 5 5\n\n# Calculate the range for data2\n\nrange(data2)\n\n[1]  0 10\nThe output for the range command gives us the lowest and the highest value in the dataset. The range of data1 is \\(5-5=0\\), which is expected because all of the values in data1 are the same. The output for the range of data2 is \\(10-0=10\\), which is expected because the largest value in data2 is 10 and the smallest value is 0.\nWhile this result is interesting, it still does not give us a good idea of how spread out the data is. Perhaps there is one value that is much larger than the rest of the values, and that is why the range is so large. This is why the range is not a great measure of variability. We need to consider other measures of variability to get a better understanding of the data set.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#variance",
    "href": "Measures_of_Variability.html#variance",
    "title": "Measures Of Variability",
    "section": "Variance",
    "text": "Variance\nWhen we talk about the spread of a data set, we are talking about the spread of a data set around the mean. The variance is a measure of variability that is sensitive to outliers. It is the average of the squared differences between each data point and the mean. Consider this data set :\n\ndata3 \\(= \\{1,2,3,4,5,6,7\\}\\)\n\n\nThe mean of this data set is 4. If we are talking about the spread of the data set around the mean, then we want to see how far each point is from the mean. We want to consider the differences between each data point and the mean. The differences are : \n\n\n\n\ndata\ndifference from mean\n\n\n\n\n1\n-3\n\n\n2\n-2\n\n\n3\n-1\n\n\n4\n0\n\n\n5\n1\n\n\n6\n2\n\n\n7\n3\n\n\n\n\nSince we are talking about spread from the mean, it would be natural to think that we should take the average of these differences (these are often called deviations from the mean). However, if we take the average of these differences, we will get 0. This is because the sum of the differences is always 0. Hopefully this makes sense, because if the mean if a true measure of center, then there should be just as much distance above the mean as there is below the mean, cancelling each other off to add up to 0. \nTo get around this, we can square the differences before we take the average. This will make all of the differences positive, and it will give us a measure of how far each point is from the mean. This is the variance. \nThe variance does have one caveat if you are calculating it by hand, and that is if your data set is a sample. If your data set is a sample, then you will need to divide by \\(n-1\\) instead of \\(n\\) when calculating the variance. This is because the sample variance is an unbiased estimator of the population variance. If you are working with a population, then you can divide by \\(n\\). Since we are not going to be calculating this by hand you will not be required to memorize this, but it is included here for completeness or if you take a statistics course in the future. \nIn terms of the mathematical formula for the variance, it is as follows :\n\n\\(variance = \\displaystyle{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n}}\\)\n\n\nWhere \\(x_i\\) is the \\(i^{th}\\) value in the data set, \\(\\bar{x}\\) is the mean of the data set, and \\(n\\) is the number of values in the data set. \nWe will now turn our attention to how we can calculate the variance in R. The variance is a built-in function in R, so we do not need to write our own function to calculate it. The variance function in R is called var. \nWe know that data1 has no variability so the result should be 0. Data2 has a lot of variability so the result should be much larger than 0. Note that if a data set has all of its values the same, then the variance will always be 0. This is because there is no spread in the data set. As soon as one value is different from the rest, then the variance will be greater than 0.\n\n# Calculate the variance for data1\n\nvar(data1)\n\n      data1\ndata1     0\n\n\nAs expected, the variance for data1 is 0. \nWhat about data2?\n\n# Calculate the variance for data2\n\nvar(data2)\n\n         data2\ndata2 27.77778\n\n\n The variance for data2 is 27.77778. \nFYI, this is the way we can make the measure of spread as large as possible. We can do this by having half of the values at the lowest value and the other half at the highest value. \nThe variance is a good measure of variability, but it is not always easy to interpret. This is because the variance is in squared units. This means the variance is not in the same units as the data, so it is hard to interpret. For example, if you are working with data that is measured in inches, then the variance will be in square inches. This is not always useful. We may want to get a measure of variability that is in the same units as the data. This is where the standard deviation comes into play.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#standard-deviation",
    "href": "Measures_of_Variability.html#standard-deviation",
    "title": "Measures Of Variability",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nBecause of the change in units, this is why we often use the standard deviation instead of the variance. The standard deviation is simply the square root of the variance. It is in the same units as the data, so it is easier to interpret. Here is how you can calculate the standard deviation :\n\n# Calculate the standard deviation for data1. \n\n# It is important to point out the the standard deviation command is\n# designed to work with vectors and not data frames. So let's recreate\n# data1 and data2\n\ndata1 &lt;- c(5,5,5,5,5,5,5,5,5,5)\n\nsd(data1)\n\n[1] 0\n\n\nThis one is easy to see since the variance was 0, we knew the standard deviation would also be 0. \nData2 is a little more interesting because the values of the data set were not all the same. What we do know is that it should be the square root of the variance.\n\n# Calculate the standard deviation for data2\n\ndata2 &lt;- c(0,0,0,0,0,10,10,10,10,10)\n\nsd(data2)\n\n[1] 5.270463\n\n\nThe standard deviation for data2 is 5.291503. This is the square root of the variance we calculated earlier. The standard deviation is a good measure of variability because it is in the same units as the data.However, it is not difficult to go back and forth between the variance and square root as we can simply either square the standard deviation to get the variance or take the square root of the variance to get the standard deviation. \nIs there a time when we don’t want to use either the variance or the standard deviation? Yes, there is. If we have a data set with outliers, then the variance and standard deviation may not be the best measures of variability. This is because the variance and standard deviation are sensitive to outliers. This is because of what we saw earlier when discussing measures of center.\nWe saw that the mean is sensitive to outliers, and the variance and standard deviation are based on the mean. This means that the variance and standard deviation are also sensitive to outliers. Hopefully this makes sense because if an outlier affects a mean, then this skewed mean will affect how we calculate the variance or standard deviation. So if we do have outliers in our data set, you may want to consider using the 5-Number summary.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#number-summary",
    "href": "Measures_of_Variability.html#number-summary",
    "title": "Measures Of Variability",
    "section": "5 Number Summary",
    "text": "5 Number Summary\nThe 5-Number Summary is a measure of variability that is not sensitive to outliers. It is a summary of the data set that includes the minimum value, the first quartile, the median, the third quartile, and the maximum value. The first quartile is the value that is greater than 25% of the data, while the third quartile is the value that is greater than 75% of the data. The median is also known as the second quartile which falls in the middle of the data. There are 50% of the data above and below the median. The 5-Number Summary is a good way to see how the data is spread out across the entire data set. \n\n\n\n\nHere is how you can calculate the 5-Number Summary for the two data sets we have been working with.\n\n# Calculate the 5-Number summary for data1.\n\n# We will use the summary( ) function to calculate the 5-Number Summary. \n# This will also show us the mean of the data.\n\nsummary(data1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      5       5       5       5       5       5 \n\n# Calculate the 5-Number summary for data2\n\nsummary(data2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       5       5      10      10",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#calculating-outliers",
    "href": "Measures_of_Variability.html#calculating-outliers",
    "title": "Measures Of Variability",
    "section": "Calculating Outliers",
    "text": "Calculating Outliers\nCalculating outliers is obviosuly a very important part of data analysis. There are many ways to calculate outliers, but one of the most common ways is to use the IQR method. The IQR method is based on the 5-Number Summary. The IQR is the difference between the third quartile and the first quartile. The IQR is a measure of how spread out the middle 50% of the data is. The IQR is used to determine if a data point is an outlier. A data point is considered an outlier if it is more than 1.5 times the IQR below the first quartile or above the third quartile.\n\n\n\n\nHere is how you can determine if a data set has outliers using R :\n\n# Let's create a data set to check. Let's make a data set with 20 values \n# between 0 and 1000 :\n\n\ndata3 &lt;- c(10, 15, 19, 27, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 150, 512, 1000)\n\n# We could use the summary( ) command to calculate the 5-Number summary to\n# determine Q1 and Q3, but here is another way to find them.\n\n# If you want the first quartile, recall it is the 25th percentile\n# so we can use the quantile( ) function to calculate it.\n\nQ1 &lt;- quantile(data3, 0.25)\n\nQ1\n\n  25% \n33.75 \n\n# If you want the third quartile, recall it is the 75th percentile\n# so we can use the quantile( ) function to calculate it.\n\nQ3 &lt;- quantile(data3, 0.75)\n\nQ3\n\n  75% \n81.25 \n\n# Now we can calculate the IQR. From above we saw that Q1 = 33.75 \n# and Q3 = 81.25. So the IQR is :\n\nIQR &lt;- Q3 - Q1\n\nIQR\n\n 75% \n47.5 \n\n# Now that we know IQR = 47.5, we can determine if there are any \n# outliers in the data set by calculating the low and high boundaries.\n\n# The low boundary (fence) is Q1 - 1.5*IQR\n\nLow_Boundary &lt;- (Q1 - 1.5*IQR)\n\nLow_Boundary\n\n  25% \n-37.5 \n\n# This tells us that the low boundary for outliers is -33.75.\n\n# The high boundary (fence) is Q3 + 1.5*IQR\n\nHigh_Boundary &lt;- (Q3 + 1.5*IQR)\n\nHigh_Boundary\n\n  75% \n152.5 \n\n# This tells us that the high boundary for outliers is 82.25\n\n# Therefore ALL values between -33.75 or above 82.25 are NOT outliers, but \n# anything below 33.75 or above 82.25 are considered outliers.\n\n# Recall our data set : \n\ndata3\n\n [1]   10   15   19   27   30   35   40   45   50   55   60   65   70   75   80\n[16]   85   90  150  512 1000\n\n\nBy inspection we can see that we do not have any values less than -33.75 which means we do not have any low outliers. However, we do have values of 512 and 1000 in the data set and these are greater than 82.25. This means we have TWO high outliers. \nBecause this data set was relatively small, we could easily see the outliers by looking at the data set. However, if you have a large data set, then it may be difficult to see the outliers. This is why it is important to have a method to determine if a data point is an outlier. We could use the following command to determine the outliers in the data set :\n\noutliers &lt;- data3[data3 &lt; Q1 - 1.5*IQR | data3 &gt; Q3 + 1.5*IQR]\n\n# let's break down what is going on with this command :\n\n# We are creating a new variable called outliers.\n\n# We are pulling out values from data3 and storing them into the \n# variable \"outliers\".\n\n# Look at the command inside of data3[ ]\n\n# We want low outliers, and that is what the first part of the command is\n# looking for. We are looking for values that are less than Q1 - 1.5*IQR.\n\n# We want high outliers, and that is what the second part of the command is\n# looking for. We are looking for values that are greater than Q3 + 1.5*IQR.\n\n# We are using the \"|\" symbol to indicate that we want to pull out values that\n# are either (less than Q1 - 1.5*IQR) OR (greater than Q3 + 1.5*IQR).\n\n# To sum up, we are looking at the data set data3 and pulling out values that\n# are either low outliers or high outliers and storing them into the variable\n# \"outliers\".\n\n# The command is basically this : \n\n# data_frame[which( data_frame &lt; Lower_Fence | data_frame &gt; Upper_Fence)]\n\n# We can print off the variable to see if any values were captured.\n\noutliers\n\n[1]  512 1000\n\n\nSince this data set has two high outliers, we would not want to use the mean as a measure of center and we would not use the standard deviation as a measure of variability. We would instead report the median as a measure of center and the 5-Number Summary as a measure of variability.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#boxplot",
    "href": "Measures_of_Variability.html#boxplot",
    "title": "Measures Of Variability",
    "section": "Boxplot",
    "text": "Boxplot\nThe 5-Number Summary is often displayed using a boxplot. A boxplot is a graphical representation of the 5-Number Summary. Here is how you can create a boxplot in R :\n\n# We can use the boxplot( ) function to create a boxplot.\n\n# Recall data3 is currently a vector, so we need to turn it into a data frame\n# in order to use ggplot.\n\ndata3 &lt;- as.data.frame(data3)\n\nggplot(data3, aes(y=data3)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThe box part of the boxplot represents the middle 50% of the data. The line on the left is where Q1 is located, the line in the middle is where the median is located, and the line on the right is where Q3 is located. The whiskers represent the minimum and maximum values in the data set that are not outliers. In other words, the left whisker goes down to the lowest value that is not an outlier and the right whisker goes up to the highest value that is not an outlier. The points outside of the whiskers are the outliers. Notice that the boxplot is displaying the two outliers we found above. \nThe two outliers above are causing the picture to be mashed together. Let’s create one more example of a distribution that does not contain any outliers so you can see what a boxplot looks like without any outliers.\n\n# Let's create a new data set that does not contain any outliers.\n\ndata4 &lt;- c(10, 15, 19, 27, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105)\n\n# We will turn data4 into a data frame so we can use ggplot.\n\ndata4 &lt;- as.data.frame(data4)\n\n# We will now create a boxplot for data4.\n\nggplot(data4, aes(y=data4)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThis is a more traditional picture of a boxplot to where it is much easier to see the 25% breaks in the data.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#exercises",
    "href": "Measures_of_Variability.html#exercises",
    "title": "Measures Of Variability",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will practice calculating descriptive statistics (range, variance, standard deviation, and 5-number summary), creating basic boxplots using ggplot2, and determining if the data has any outliers using the Interquartile Range (IQR). You will use built-in datasets from R for this assignment.\n\nProblem 1: Descriptive Statistics of Sepal Length in the iris Dataset\nTask: Calculate the range, variance, standard deviation, and 5-number summary of the Sepal.Length column in the iris dataset. \nSteps:\n\nLoad the iris dataset.\nCalculate the range, variance, standard deviation, and 5-number summary of the Sepal.Length column.\n\nCode Example:\n\n# Load iris dataset\ndata(iris)\n\n# Calculate descriptive statistics\nrange_sepal_length &lt;- range(iris$Sepal.Length)\nvariance_sepal_length &lt;- var(iris$Sepal.Length)\nsd_sepal_length &lt;- sd(iris$Sepal.Length)\nsummary_sepal_length &lt;- summary(iris$Sepal.Length)\n\nrange_sepal_length\n\n[1] 4.3 7.9\n\nvariance_sepal_length\n\n[1] 0.6856935\n\nsd_sepal_length\n\n[1] 0.8280661\n\nsummary_sepal_length\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\n\n\nProblem 2: Descriptive Statistics of Ozone Levels in the airquality Dataset\nTask: Calculate the range, variance, standard deviation, and 5-number summary of the Ozone column in the airquality dataset after removing NA values. \nSteps:\n\nLoad the airquality dataset.\nRemove NA values from the Ozone column.\nCalculate the range, variance, standard deviation, and 5-number summary of the Ozone column.\n\nCode Example:\n\n# Load airquality dataset\ndata(airquality)\n\n# Remove NA values from Ozone column\nozone_no_na &lt;- na.omit(airquality$Ozone)\n\n# Calculate descriptive statistics\nrange_ozone &lt;- range(ozone_no_na)\nvariance_ozone &lt;- var(ozone_no_na)\nsd_ozone &lt;- sd(ozone_no_na)\nsummary_ozone &lt;- summary(ozone_no_na)\n\nrange_ozone\n\n[1]   1 168\n\nvariance_ozone\n\n[1] 1088.201\n\nsd_ozone\n\n[1] 32.98788\n\nsummary_ozone\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   18.00   31.50   42.13   63.25  168.00 \n\n\n\n\nProblem 3: Boxplot of Annual Lynx Trappings in the lynx Dataset\nTask: Create a basic boxplot of the annual number of lynx trapped in the lynx dataset. \nSteps:\n\nLoad the lynx dataset.\nCreate a boxplot using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and lynx dataset\nlibrary(ggplot2)\ndata(lynx)\n\n# Create boxplot\nggplot(data.frame(lynx), aes(y = lynx)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Annual Lynx Trappings\", y = \"Number of Lynx Trapped\")\n\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous.\n\n\n\n\n\n\n\n\n\n\n\nProblem 4: Descriptive Statistics of Tooth Length in the ToothGrowth Dataset\nTask: Calculate the range, variance, standard deviation, and 5-number summary of the len column in the ToothGrowth dataset. \nSteps:\n\nLoad the ToothGrowth dataset.\nCalculate the range, variance, standard deviation, and 5-number summary of the len column.\n\nCode Example:\n\n# Load ToothGrowth dataset\ndata(ToothGrowth)\n\n# Calculate descriptive statistics\nrange_len &lt;- range(ToothGrowth$len)\nvariance_len &lt;- var(ToothGrowth$len)\nsd_len &lt;- sd(ToothGrowth$len)\nsummary_len &lt;- summary(ToothGrowth$len)\n\nrange_len\n\n[1]  4.2 33.9\n\nvariance_len\n\n[1] 58.51202\n\nsd_len\n\n[1] 7.649315\n\nsummary_len\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   4.20   13.07   19.25   18.81   25.27   33.90 \n\n\n\n\nProblem 5: Boxplot of Age in the infert Dataset\nTask: Create a basic boxplot of the age column in the infert dataset. \nSteps:\n\nLoad the infert dataset.\nCreate a boxplot using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and infert dataset\nlibrary(ggplot2)\ndata(infert)\n\n# Create boxplot\nggplot(infert, aes(y = age)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Age in Infert Dataset\", y = \"Age\")\n\n\n\n\n\n\n\n\n\n\nProblem 6: Descriptive Statistics of Wind Speed in the airquality Dataset\nTask: Calculate the range, variance, standard deviation, and 5-number summary of the Wind column in the airquality dataset after removing NA values. \nSteps:\n\nLoad the airquality dataset.\nRemove NA values from the Wind column.\nCalculate the range, variance, standard deviation, and 5-number summary of the Wind column.\n\nCode Example:\n\n# Load airquality dataset\ndata(airquality)\n\n# Remove NA values from Wind column\nwind_no_na &lt;- na.omit(airquality$Wind)\n\n# Calculate descriptive statistics\nrange_wind &lt;- range(wind_no_na)\nvariance_wind &lt;- var(wind_no_na)\nsd_wind &lt;- sd(wind_no_na)\nsummary_wind &lt;- summary(wind_no_na)\n\nrange_wind\n\n[1]  1.7 20.7\n\nvariance_wind\n\n[1] 12.41154\n\nsd_wind\n\n[1] 3.523001\n\nsummary_wind\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.700   7.400   9.700   9.958  11.500  20.700 \n\n\n\n\nProblem 7: Determine Outliers in Sepal Length of the iris Dataset\nTask: Determine if the Sepal.Length column in the iris dataset has any outliers using the Interquartile Range (IQR). \nSteps:\n\nLoad the iris dataset.\nCalculate the IQR of the Sepal.Length column.\nIdentify any outliers in the Sepal.Length column.\n\nCode Example:\n\n# Load iris dataset\ndata(iris)\n\n# Calculate IQR\niqr_sepal_length &lt;- IQR(iris$Sepal.Length)\n\n# Identify outliers\nq1 &lt;- quantile(iris$Sepal.Length, 0.25)\nq3 &lt;- quantile(iris$Sepal.Length, 0.75)\nlower_bound &lt;- q1 - 1.5 * iqr_sepal_length\nupper_bound &lt;- q3 + 1.5 * iqr_sepal_length\noutliers &lt;- iris$Sepal.Length[iris$Sepal.Length &lt; lower_bound | iris$Sepal.Length &gt; upper_bound]\n\niqr_sepal_length\n\n[1] 1.3\n\noutliers\n\nnumeric(0)\n\n\n\n\nProblem 8: Descriptive Statistics of Child Count in the infert Dataset\nTask: Calculate the range, variance, standard deviation, and 5-number summary of the parity column in the infert dataset. \nSteps:\n\nLoad the infert dataset.\nCalculate the range, variance, standard deviation, and 5-number summary of the parity column.\n\nCode Example:\n\n# Load infert dataset\ndata(infert)\n\n# Calculate descriptive statistics\nrange_parity &lt;- range(infert$parity)\nvariance_parity &lt;- var(infert$parity)\nsd_parity &lt;- sd(infert$parity)\nsummary_parity &lt;- summary(infert$parity)\n\nrange_parity\n\n[1] 1 6\n\nvariance_parity\n\n[1] 1.566263\n\nsd_parity\n\n[1] 1.251504\n\nsummary_parity\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   2.093   3.000   6.000 \n\n\n\n\nProblem 9: Boxplot of Sepal Width in the iris Dataset\nTask: Create a basic boxplot of the Sepal.Width column in the iris dataset. \nSteps:\n\nLoad the iris dataset.\nCreate a boxplot using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Create boxplot\nggplot(iris, aes(y = Sepal.Width)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sepal Width in Iris Dataset\", y = \"Sepal Width\")\n\n\n\n\n\n\n\n\n\n\nProblem 10: Determine Outliers in Tooth Length of the ToothGrowth Dataset\nTask: Determine if the len column in the ToothGrowth dataset has any outliers using the Interquartile Range (IQR). \nSteps:\n\nLoad the ToothGrowth dataset.\nCalculate the IQR of the len column.\nIdentify any outliers in the len column.\n\nCode Example:\n\n# Load ToothGrowth dataset\ndata(ToothGrowth)\n\n# Calculate IQR\niqr_len &lt;- IQR(ToothGrowth$len)\n\n# Identify outliers\nq1 &lt;- quantile(ToothGrowth$len, 0.25)\nq3 &lt;- quantile(ToothGrowth$len, 0.75)\nlower_bound &lt;- q1 - 1.5 * iqr_len\nupper_bound &lt;- q3 + 1.5 * iqr_len\noutliers &lt;- ToothGrowth$len[ToothGrowth$len &lt; lower_bound | ToothGrowth$len &gt; upper_bound]\n\niqr_len\n\n[1] 12.2\n\noutliers\n\nnumeric(0)",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#conclusion",
    "href": "Measures_of_Variability.html#conclusion",
    "title": "Measures Of Variability",
    "section": "Conclusion",
    "text": "Conclusion\nMeasures of variability are important because they help us understand how spread out the data is. They help us understand the distribution of the data. There are several ways to measure variability, but we focused on the range, interquartile range, variance, and standard deviation. Each of these measures has its own strengths and weaknesses, and it is important to consider which measure is most appropriate for your data set.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Measures_of_Variability.html#references",
    "href": "Measures_of_Variability.html#references",
    "title": "Measures Of Variability",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Measures Of Variability"
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html",
    "href": "Advanced_Boxplot_Techniques.html",
    "title": "Advanced Boxplot Techniques.",
    "section": "",
    "text": "Boxplots\nA boxplot is a graphical representation of the distribution of a variable. The “box” in the boxplot represents the interquartile range (IQR), which is the range of values between the lower and upper quartiles. The line in the middle of the box represents the median, which is the middle value of the data. The “whiskers” on the boxplot stretch to the minimum and maximum non-outlier values of the data. If a data set does have outliers, then they are displayed as individual points on the boxplot. If you recall, any values outside the lower and upper boundaries of the whiskers are considered outliers. The formulas for the whiskers are as follows:",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#boxplots",
    "href": "Advanced_Boxplot_Techniques.html#boxplots",
    "title": "Advanced Boxplot Techniques.",
    "section": "",
    "text": "Lower whisker: Q1 - 1.5 * IQR\nUpper whisker: Q3 + 1.5 * IQR",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#creating-a-boxplot-with-ggplot",
    "href": "Advanced_Boxplot_Techniques.html#creating-a-boxplot-with-ggplot",
    "title": "Advanced Boxplot Techniques.",
    "section": "Creating a boxplot with ggplot",
    "text": "Creating a boxplot with ggplot\nHere is an example of how to create a boxplot using ggplot:\n\n# Load up the ggplot2 package\n\nlibrary(ggplot2)\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n\nIn this command we :\n\nCreate a new varaible df and stored the data fram there\nWe created three groups (A, B, C) with 100 values each\nA will have values from a normal distribution with a mean of 0\nB will have values from a normal distribution with a mean of 1\nC will have values from a normal distribution with a mean of 2\n\nLet’s first look at how we could create a boxplot for just the variable “A” in the data frame.\n\n# Filter data for Group A and store the result in the new variable df_A\n\ndf_A &lt;- subset(df, group == \"A\")\n\n# Create the box plot\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe can see from this boxplot that we have a low outlier but no high outliers. \nWe could also display this boxplot horizontally by adding coord_flip() to the command:\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nWe are now ready to move on to more advanced ideas for a boxplot.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#boxplot-with-a-jitter-strip",
    "href": "Advanced_Boxplot_Techniques.html#boxplot-with-a-jitter-strip",
    "title": "Advanced Boxplot Techniques.",
    "section": "Boxplot with a Jitter Strip",
    "text": "Boxplot with a Jitter Strip\nOne way to enhance a boxplot is to add a jitter strip to the visualization. This shows the points from the data set and can help to see the density of the data points more clearly. Here is an example of a boxplot with jitter using ggplot:\n\n# Create a boxplot with jitter\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot() +\n  coord_flip() +\n  geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\nRemember we are dealing with a single variable, so technically all of these points should be on a straight line. However, the jitter strip helps us to see the density of the data points more clearly by spacing (“jittering”) them out.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#colors-and-alpha-level",
    "href": "Advanced_Boxplot_Techniques.html#colors-and-alpha-level",
    "title": "Advanced Boxplot Techniques.",
    "section": "Colors and Alpha Level",
    "text": "Colors and Alpha Level\nYou can also change the colors of the boxplot. Here is an example of how to change the color of the interior of the boxplot to light blue, as well as how to make an outlier stand out by changing its color.\n\n# Create a boxplot with a different fill color\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot(fill = \"lightblue\", outlier.colour = \"red\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nFrom here we could add a jitter strip with a different color. Here is an example of how to change the color of the jitter strip to red:\n\n# Create a boxplot with a different fill color and a jitter strip with a different color\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  geom_jitter(width = 0.2, color = \"red\")\n\n\n\n\n\n\n\n\nIf the points have a lot of overlap, they can start to bunch together into a blob and you can’t tell how many points are there. To help combat this, you can change what is called the “alpha level” of the points. This basically changes how transparent the will be. This level can be between 0 and 1 where 0 is basically invisible and 1 is completely solid. Here is an example with the alpha levels at 0.5 :\n\n# Create a boxplot with a different fill color and a jitter strip with a different color\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  geom_jitter(width = 0.2, color = \"red\", alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice how the points are much lighter than the previous picture. When points start to overlap each other, that is when you will notice the points are becoming darker. The more points that overlap, the darker the overlap will be.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#labels",
    "href": "Advanced_Boxplot_Techniques.html#labels",
    "title": "Advanced Boxplot Techniques.",
    "section": "Labels",
    "text": "Labels\nYou can also add labels to the boxplot. These will help the reader better understand the data. Here is an example of how to add labels to the boxplot:\n\n# Create a boxplot with labels\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip()+\n  labs(title = \"Boxplot with Jitter Strip\",\n       subtitle = \"Add a subtitle here\",\n       x = \"Group\",\n       y = \"Value\")\n\n\n\n\n\n\n\n\nThis command adds several labels using the labs( ) layer:\n\nAdded the title at the top of the boxplot\nAdded a subtitle beneath the title\nAdded the x-axis label\nAdded the y-axis label\n\nWe will look at an example in a bit from a data set that has actual data and we will see how to add meaningful labels to the boxplot.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#values-on-the-boxplot",
    "href": "Advanced_Boxplot_Techniques.html#values-on-the-boxplot",
    "title": "Advanced Boxplot Techniques.",
    "section": "Values on the Boxplot",
    "text": "Values on the Boxplot\nA boxplot is a very nice visualzation of the data, but the reader can’t always tell the exact values for the important values of the boxplot. It would be nice to have the values for Q1, Q3, etc., listed on the boxplot. \nHere is an example of how to add the values to the boxplot:\n\n# Compute Quartiles\nq1 &lt;- quantile(df_A$value, 0.25)\nq2 &lt;- quantile(df_A$value, 0.50)  # Median\nq3 &lt;- quantile(df_A$value, 0.75)\n\n\n\n# Boxplot with Custom Legend\nggplot(df_A, aes(y = value)) +\n geom_boxplot(fill = \"lightblue\") +\n geom_text(aes(x = 0.5, y = q1, label = paste(\"Q1: \", round(q1, 2))), vjust = 1.5) +\n geom_text(aes(x = 0.5, y = q2, label = paste(\"Q2: \", round(q2, 2))), vjust = -0.5) +\n geom_text(aes(x = 0.5, y = q3, label = paste(\"Q3: \", round(q3, 2))), vjust = 1.5) +\n coord_flip()\n\n\n\n\n\n\n\n\nIn this example, we used the geom_text( ) layer to add the values for Q1, Q2, and Q3 to the boxplot. We used the vjust argument to adjust the vertical position of the text.\n\nx = 0.5 : This tells us to have the text start at the height of the boxplot where x = 0.5\ny = q1 : This tells us to start the text at the value of q1 which was calculated above\nlabel = paste(“Q1:”, round(q1, 2)) : This tells us to label the text as “Q1:” and then the value of q1 rounded to 2 decimal places\nvjust = 1.5 : This tells us to adjust the vertical position of the text so that it is 1.5 units above the boxplot \n\nNote that since these values are so close together, I had to adjust the vertical position of the text so that they would not overlap. This is why I used the vjust argument to adjust the vertical position of the text.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#adding-data-points-to-the-boxplot",
    "href": "Advanced_Boxplot_Techniques.html#adding-data-points-to-the-boxplot",
    "title": "Advanced Boxplot Techniques.",
    "section": "Adding Data Points to the Boxplot",
    "text": "Adding Data Points to the Boxplot\nYou can also add the values of the data points to the boxplot. This can be useful when you want to see the actual values of the data points, BUT this is not always a good idea. If there are many points, then this will probably all just run together. For instance, we COULD do this for our current data set, but it really wouldn’t be helpful. Here is an example of how to add the values to the boxplot:\n\n# Create a boxplot with values\n\nggplot(df_A, aes(x = group, y = value)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  geom_text(aes(label = value), vjust = -0.5)\n\n\n\n\n\n\n\n\nNotice that they all just run together and don’t really help our understanding of the data set at all.\nHere is an example of a small data set where the points are spread out where this might be helpful:\n\n# Create a data frame with 8 random values from 1 to 100\n\nset.seed(98)  # Set seed for reproducibility\n\ndf_small &lt;- data.frame(value = sample(1:100, 8))\n\n# Create a boxplot\nggplot(df_small, aes(x = factor(1), y = value)) +\n  geom_boxplot(fill = \"lightblue\") +  # Boxplot\n  geom_text(aes(label = value), vjust = -0.5, color = \"blue\") +  # Add labels on each point\n  theme_minimal() +\n  coord_flip() +\n  labs(title = \"Boxplot with Labels\", x = \"\", y = \"Value\") +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())  # Hide x-axis\n\n\n\n\n\n\n\n\nAs you can imagine, once you get a data set with 10 or more points, this really isn’t going to be useful. The best options are to include the important values as we did above or perhaps create a table summarizing the values.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#multiple-boxplots-on-one-graph",
    "href": "Advanced_Boxplot_Techniques.html#multiple-boxplots-on-one-graph",
    "title": "Advanced Boxplot Techniques.",
    "section": "Multiple Boxplots On One Graph",
    "text": "Multiple Boxplots On One Graph\nLet’s go back to our original example of the data frame that had three variables and see how we could put these on the same graph.\nHere is how we could create a boxplot for our. variable df.\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe could then start to fancy this up using some of the commands we saw earlier.\n\nggplot(df, aes(x = group, y = value, fill = group)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Values by Group\", x = \"Group\", y = \"Value\") +\n  scale_fill_manual(values = c(\"A\" = \"skyblue\", \"B\" = \"lightgreen\", \"C\" = \"lightcoral\"))  # Custom colors\n\n\n\n\n\n\n\n\nNotice that we can specify the colors for the boxplots by changing our choices for the fill option.\nHere is what they look like with a jitter strip :\n\nggplot(df, aes(x = group, y = value, fill = group)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Values by Group\", x = \"Group\", y = \"Value\") +\n  geom_jitter() +\n  scale_fill_manual(values = c(\"A\" = \"skyblue\", \"B\" = \"lightgreen\", \"C\" = \"lightcoral\"))  # Custom colors",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#meaningful-example",
    "href": "Advanced_Boxplot_Techniques.html#meaningful-example",
    "title": "Advanced Boxplot Techniques.",
    "section": "Meaningful Example",
    "text": "Meaningful Example\nLet us turn our attention to an example that has more meaning than a data set full of random values. Let’s do a little EDA as we look at the poverty levels of families in the state of Kentucky where they are measured by county. \nThe data is located in the file KY-Poverty-Levels-By-County.csv. Let’s read this file into R.\n\n# Make sure we have the proper library loaded up :\n\nlibrary(readr)\n\n# Read in the csv file into the variable KY_data\n\nKY_data &lt;- read_csv(\"./KY-Poverty-Levels-By-County.csv\")\n\nRows: 120 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (2): Value_Percent, Families_Below_Poverty\nnum (1): Rank_within_US_of_3143 _counties\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf we look at the output from when we first read in the data set, our data set has 120 rows and 4 columns. That means we have 4 variables (listed below) that we are measuring and 120 observations. \nLet’s first take a quick look at the beginning and end of the data set :\n\nhead(KY_data)\n\n# A tibble: 6 × 4\n  County          Value_Percent Families_Below_Poverty Rank_within_US_of_3143 …¹\n  &lt;chr&gt;                   &lt;dbl&gt;                  &lt;dbl&gt;                     &lt;dbl&gt;\n1 Knox County              29.6                   2204                      3126\n2 Wolfe County             29.2                    465                      3124\n3 McCreary County          26.5                    917                      3111\n4 Clay County              26.1                   1342                      3109\n5 Magoffin County          25                      746                      3098\n6 Harlan County            24.9                   1775                      3097\n# ℹ abbreviated name: ¹​`Rank_within_US_of_3143 _counties`\n\ntail(KY_data)\n\n# A tibble: 6 × 4\n  County          Value_Percent Families_Below_Poverty Rank_within_US_of_3143 …¹\n  &lt;chr&gt;                   &lt;dbl&gt;                  &lt;dbl&gt;                     &lt;dbl&gt;\n1 Nelson County             5.9                    750                       611\n2 Woodford County           5.7                    429                       560\n3 Shelby County             5.1                    652                       408\n4 Spencer County            3.8                    214                       178\n5 Boone County              3.1                   1096                        93\n6 Oldham County             3                      547                        84\n# ℹ abbreviated name: ¹​`Rank_within_US_of_3143 _counties`\n\n\nBased on the first (head) and last (tail) six lines of the data set, it looks like we have the following information :\n\nCounty Name\nThe percentage of families living in poverty\nThe number of families in the county living in poverty.\nTheir rank out of the 3143 counties in the US \n\nWe can also take a look at the data using the summary command :\n\n# This is base function for R so we do not have to load any additional packages\n\nsummary(KY_data)\n\n    County          Value_Percent   Families_Below_Poverty\n Length:120         Min.   : 3.00   Min.   :   91.0       \n Class :character   1st Qu.:10.85   1st Qu.:  431.2       \n Mode  :character   Median :13.60   Median :  690.0       \n                    Mean   :14.39   Mean   : 1116.3       \n                    3rd Qu.:17.90   3rd Qu.: 1132.5       \n                    Max.   :29.60   Max.   :19481.0       \n Rank_within_US_of_3143 _counties\n Min.   :  84                    \n 1st Qu.:1976                    \n Median :2484                    \n Mean   :2294                    \n 3rd Qu.:2881                    \n Max.   :3126                    \n\n\nFrom this output we can tell a bit more about the variables. We can see that the first variable is of class character, so there really isn’t any numerical analysis we can do here. However, we do see that the last three variables are all numeric, which is why we get the 5-Number summary and the mean presented to us. \nWhat we could do next is to create a boxplot for the first numeric variable. This variable tells us the percentage of families living in poverty in their county.\n\nggplot(KY_data, aes(y=KY_data$Value_Percent))+\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\nWe really need to make this look better, so let’s add some labels.\n\nggplot(KY_data, aes(y=KY_data$Value_Percent))+\n  geom_boxplot() +\n  coord_flip() + \n  labs(title = \"Poverty Rates for Kentucky Counties\", x = \"\", y = \"Percentage\")\n\n\n\n\n\n\n\n\nWe could also add a little color :\n\nggplot(KY_data, aes(y=KY_data$Value_Percent))+\n  geom_boxplot(fill = \"slategray3\") +\n  coord_flip() + \n  labs(title = \"Poverty Rates for Kentucky Counties\", x = \"\", y = \"Percentage\")\n\n\n\n\n\n\n\n\nWe could also add the values of the quartiles to improve the visualization.\n\n# Compute Quartiles\nq1 &lt;- quantile(KY_data$Value_Percent, 0.25)\nq2 &lt;- quantile(KY_data$Value_Percent, 0.50)  # Median\nq3 &lt;- quantile(KY_data$Value_Percent, 0.75)\n\n# Create the boxplot\n\nggplot(KY_data, aes(y = KY_data$Value_Percent))+\n  geom_boxplot(fill = \"slategray3\") +\n  geom_text(aes(x = 0.5, y = q1, label = paste(\"Q1: \", round(q1, 2))), vjust = 1.5) +\n  geom_text(aes(x = 0.5, y = q2, label = paste(\"Q2: \", round(q2, 2))), vjust = -0.5) +\n  geom_text(aes(x = 0.5, y = q3, label = paste(\"Q3: \", round(q3, 2))), vjust = 1.5) +\n  coord_flip() + \n#  geom_jitter(width = 0.2, color=\"black\") +\n  labs(title = \"Poverty Rates for Kentucky Counties\", y = \"Percentage\")",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#themes",
    "href": "Advanced_Boxplot_Techniques.html#themes",
    "title": "Advanced Boxplot Techniques.",
    "section": "Themes",
    "text": "Themes\nThere are several different built in themes you could use to clean up your ggplot visializations. You can view several of them here :\nhttps://r-charts.com/ggplot2/themes/\nHere are a couple of examples for you. Feel free to look them up and play around a bit. We will go back to the first data set we created and mess around with that one.\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s add a few themes to see what happens.\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_dark()\n\n\n\n\n\n\n\n\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_void()\n\n\n\n\n\n\n\n\nYou can even download other themes. You can find several online at the link given above. Here we will download, install, and use the hrbrthemes package.\n\n# install.packages(\"hrbrthemes\")\nlibrary(hrbrthemes)\n\n# This package loads up 8 themes. We will use the theme \"theme_ipsum()\"\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nFor more information about boxplots, here is a useful webpage : https://www.appsilon.com/post/ggplot2-boxplots",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#exercises",
    "href": "Advanced_Boxplot_Techniques.html#exercises",
    "title": "Advanced Boxplot Techniques.",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will practice creating different types of boxplots using the ggplot2 library in R. You will use built-in datasets from R for this assignment. The problems vary in difficulty from basic boxplots to more advanced plots with additional customization.\n\nProblem 1: Basic Boxplot of Sepal Length in the iris Dataset\nTask: Create a basic boxplot of the Sepal.Length column in the iris dataset. \nSteps:\n\nLoad the iris dataset.\nCreate a boxplot using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Create boxplot\nggplot(iris, aes(y = Sepal.Length)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sepal Length in Iris Dataset\", y = \"Sepal Length\")\n\n\n\n\n\n\n\n\n\n\nProblem 2: Boxplot of Sepal Length by Species in the iris Dataset\nTask: Create a boxplot of the Sepal.Length column grouped by Species in the iris dataset. \nSteps:\n\nLoad the iris dataset.\nCreate a grouped boxplot using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Create boxplot\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sepal Length by Species in Iris Dataset\", x = \"Species\", y = \"Sepal Length\")\n\n\n\n\n\n\n\n\n\n\nProblem 3: Boxplot of Tooth Length in the ToothGrowth Dataset\nTask: Create a boxplot of the len column in the ToothGrowth dataset and add color to the boxes.\nSteps: 1. Load the ToothGrowth dataset. 2. Create a colored boxplot using ggplot2.\nCode Example:\n\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Create boxplot with color\nggplot(ToothGrowth, aes(y = len, fill = supp)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Tooth Length in ToothGrowth Dataset\", y = \"Tooth Length\") +\n  scale_fill_manual(values = c(\"VC\" = \"blue\", \"OJ\" = \"orange\"))\n\n\n\n\n\n\n\n\n\n\nProblem 4: Boxplot with Points of Wind Speed in the airquality Dataset\nTask: Create a boxplot of the Wind column in the airquality dataset and add points to it. \nSteps:\n\nLoad the airquality dataset.\nCreate a boxplot with points using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and airquality dataset\nlibrary(ggplot2)\ndata(airquality)\n\n# Create boxplot with points\nggplot(airquality, aes(x=\"\", y = Wind)) +\n  geom_boxplot() +\n  geom_jitter(position = position_jitter(width = 0.2), alpha = 0.5) +\n  labs(title = \"Boxplot of Wind Speed in Airquality Dataset\", y = \"Wind Speed\")\n\n\n\n\n\n\n\n\n\n\nProblem 5: Boxplot of Annual Lynx Trappings in the lynx Dataset with Changed Alpha Levels\nTask: Create a boxplot of the annual number of lynx trapped in the lynx dataset and change the alpha levels for the points. \nSteps:\n\nLoad the lynx dataset.\nCreate a boxplot with changed alpha levels using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and lynx dataset\nlibrary(ggplot2)\ndata(lynx)\n\n# Create boxplot with changed alpha levels\nggplot(data.frame(lynx), aes( x=\"\", y = lynx)) +\n  geom_boxplot() +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) +\n  labs(title = \"Boxplot of Annual Lynx Trappings\", y = \"Number of Lynx Trapped\")\n\n\n\n\n\n\n\n\n\n\nProblem 6: Boxplot with Jitterstrip of Age in the infert Dataset\nTask: Create a boxplot of the age column in the infert dataset and add a jitterstrip. \nSteps:\n\nLoad the infert dataset.\nCreate a boxplot with jitterstrip using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and infert dataset\nlibrary(ggplot2)\ndata(infert)\n\n# Create boxplot with jitterstrip\nggplot(infert, aes(x=\"\", y = age)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Boxplot of Age in Infert Dataset\", y = \"Age\")\n\n\n\n\n\n\n\n\n\n\nProblem 7: Boxplot with Labels of Sepal Width in the iris Dataset\nTask: Create a boxplot of the Sepal.Width column in the iris dataset and add labels to the plot. \nSteps:\n\nLoad the iris dataset.\nCreate a boxplot with labels using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Create boxplot with labels\nggplot(iris, aes(y = Sepal.Width)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sepal Width in Iris Dataset\", y = \"Sepal Width\", x = \"Species\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nProblem 8: Multiple Boxplots on One Graph for Tooth Length by Supplement and Dose in the ToothGrowth Dataset\nTask: Create multiple boxplots on one graph for the len column by supp and dose in the ToothGrowth dataset. \nSteps:\n\nLoad the ToothGrowth dataset.\nCreate multiple boxplots using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Create multiple boxplots\nggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = supp)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Tooth Length by Supplement and Dose in ToothGrowth Dataset\", x = \"Dose\", y = \"Tooth Length\") +\n  scale_fill_manual(values = c(\"VC\" = \"blue\", \"OJ\" = \"orange\"))\n\n\n\n\n\n\n\n\n\n\nProblem 9: Boxplot with Custom Colors of Wind Speed in the airquality Dataset\nTask: Create a boxplot of the Wind column in the airquality dataset and apply custom colors to the boxplot. \nSteps:\n\nLoad the airquality dataset.\nCreate a boxplot with custom colors using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and airquality dataset\nlibrary(ggplot2)\ndata(airquality)\n\n# Create boxplot with custom colors\nggplot(airquality, aes(y = Wind, fill = factor(Month))) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Wind Speed in Airquality Dataset\", y = \"Wind Speed\", fill = \"Month\") +\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\n\n\n\n\n\n\nProblem 10: Boxplot with Facets of Sepal Length by Species in the iris Dataset\nTask: Create a boxplot of the Sepal.Length column by Species in the iris dataset and use facets to separate the plots by species. \nSteps:\n\nLoad the iris dataset.\nCreate a boxplot with facets using ggplot2.\n\nCode Example:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Create boxplot with facets\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() +\n  facet_wrap(~ Species) +\n  labs(title = \"Boxplot of Sepal Length by Species in Iris Dataset\", x = \"Species\", y = \"Sepal Length\")",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html",
    "href": "Independent_and_Dependent_Variables.html",
    "title": "Independent and Dependent Variables",
    "section": "",
    "text": "Causation\nThe causation model is the one that researchers are always hoping for. This is the model to where there is a direct cause and effect relationship between two variables. In other words, the change in the dependent / response variable Y is only because of the independent / explanatory variable X. Consider the following graphic :\nThe way you can interpret this graphic is as follows. A researcher wonders if variable X causes a change in variable Y The researcher then takes in some data and from the data it looks like there is a relationship between the two variables. This is denoted by the dashed arrow. This dashed arrow is telling us that their appear to be some type of association between the variables. As it turns out, variable X is the only variable causing a change in variable Y This is represented by the solid arrow in the graph.\nThis tells us that if there is a direct cause and effect relationship between two variables with no other variables being the cause for change, then this is the causation model.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#common-response",
    "href": "Independent_and_Dependent_Variables.html#common-response",
    "title": "Independent and Dependent Variables",
    "section": "Common Response",
    "text": "Common Response\nExamine this graphic for the common response model.\n\n\n\n\nThis model starts off similarly as the previous one. Our researcher questioned an association, took in some data, and based on the data it looks like there could be an association between the two variables. That is what the dashed arrow represents. However, this is when the story starts to change. \nWhen it looks like there is an association this means that as variable X (the explanatory variable) changes, it appears to be causing a change in variable Y (the response variable). What is really happening is that a new player has come into the picture, the variable Z. If you look at the graphic, variable Z is causing a change in variable X and variable Y at the same time. So while it looks like X is changing Y, it is the case that X is not affecting Y at all and Z causing the change between both of them. Therefore the association that we are seeing is because of the common response both X and Y are having to Z. \nIn a situation such as this, variable z is called a lurking variable. This is a variable that is affecting our study that we are not accounting for when examining the relationship between X and Y.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#confounding",
    "href": "Independent_and_Dependent_Variables.html#confounding",
    "title": "Independent and Dependent Variables",
    "section": "Confounding",
    "text": "Confounding\nOur last model looks like this :\n\n\n\n\nOur story starts the same as the first two models. We investigate a relationship and based on the data, it appears there is an association between variable X and variable Y (dashed arrow). As you can see, we have a lurking variable, Z, that is complicating the analysis. From the second dashed arrow, there appears to be some type of association between X and Z. What we can see from the solid arrows is that X and Z are both changing Y at the same time! The question we can’t tell from this model is how much of the change in Y we can attribute to X and how much we can attribute to Z. \nSince we can’t determine how much of the change in Y is attributable to X, we don’t know if the association is strong or not. It could be X driving the change in Y or it could be Z driving the change. Since we can’t determine these levels, this model is called confounding.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#association-is-not-causation",
    "href": "Independent_and_Dependent_Variables.html#association-is-not-causation",
    "title": "Independent and Dependent Variables",
    "section": "Association is not Causation",
    "text": "Association is not Causation\nAs the models have shown us, just because it looks like there is an association between two variables, this is not evidence that there is a cause and effect relationship between the two variables. Based on the models above, there could be anything from a 100% relationship (causation) to a 0% relationship (common response), with the truth likely being someplace in between (confounding). \nThe term “association” is often times replaced with the word “correlation”. This leads us to and defines a phrase you have probably heard many times when it comes to using statistics :\n\n\n\n\n\n\nImportant\n\n\n\nCorrelation does NOT imply Causation",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#showing-causation",
    "href": "Independent_and_Dependent_Variables.html#showing-causation",
    "title": "Independent and Dependent Variables",
    "section": "Showing Causation",
    "text": "Showing Causation\nShowing that there is a direct cause and effect relationship between two variables is not easy to prove. Let’s assume that we have made a hypothesis about two variables, collected data, done some analysis, and it appears that there could be an association between the two variables. What is need next are steps that can show the association is valid. To prove causation in statistics, there are three concepts that we must verify : temporal precedence, covariance, and the absence of confounding variables. \n\nTemporal Precedence\nThis is a fancy way to say that the cause must come before the effect. \nIf a researcher wants to see if a change in Variable X causes a change in variable Y, then the researcher will force a change in X and then observe what happens to variable Y. If there really is a relationship, then the researcher will see a change in Y only after there has been a change in X. If the variable Y changes before we do anything to variable X, then that is evidence that a lurking variable has come in to the experiment.\n\n\nCovariance\nCovariance is a statistical measure that one can use to measure how two variables change in relationship to one another. While we won’t be using this in this course, it is worth understanding what it means. \nWhen a researcher calculates the covariance between two variables, the resulting value describes the relationship between the them. If the covariance is a positive value, then the two variables tend to increase together or decrease together. In other words, as the independent variable increases, so does the dependent variable and vice versa. If the covariance returns a negative value, then the variables have an inverse relationship. This tells us that if the independent variable increases, then the dependent variable decreases and vice versa. \nAs the value of the covariance moves further and further away from zero implies that the relationship between the two variables is getting stronger. If the covariance has a value close to zero then this is evidence that there is not much of an association between the two variables.\n\n\nAbsense of Confounding Variables\nThe last concept to consider is finding a way to remove the effect of confounding variables. As a researcher you want to see if a single variable X is causing the change to a variable Y. If there are confounding variables that could be changing both, then there is not a way for us to determine how much of the change is Y we can attribute to X. The only way we could measure this is if we can rule out any other possibility explaining the change in Y. In other words, if we remove all of confounding variables from the equation, the only variable that could be causing the change in the response variable is the explanatory variable.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#exercises",
    "href": "Independent_and_Dependent_Variables.html#exercises",
    "title": "Independent and Dependent Variables",
    "section": "Exercises",
    "text": "Exercises\nFor each question, identify the independent and dependent variables, and determine if there are any causal relationships, confounding variables, or common responses. Provide explanations for your answers.\n\nQuestion 1\nA researcher is studying the effect of fertilizer on plant growth. They apply different amounts of fertilizer to different plants and measure their growth after a month.\n\nIdentify the independent variable.\nIdentify the dependent variable.\n\nAnswer:\n\nIndependent Variable: Amount of fertilizer applied.\n\nExplanation: The researcher manipulates the amount of fertilizer to observe its effect on plant growth.\n\nDependent Variable: Plant growth (measured after a month).\n\nExplanation: The plant growth is measured to see how it changes in response to different amounts of fertilizer.\n\n\n\n\nQuestion 2\nScenario: A study is conducted to examine the relationship between the number of hours studied and the scores on a math test.\n\nIdentify the independent variable.\nIdentify the dependent variable.\n\nAnswer:\n\nIndependent Variable: Number of hours studied.\n\nExplanation: The number of hours studied is varied to observe its impact on test scores.\n\nDependent Variable: Scores on the math test.\n\nExplanation: The test scores are measured to see how they change with different study durations.\n\n\n\n\nQuestion 3\nScenario: A scientist is investigating the effect of temperature on the rate of a chemical reaction. They conduct the experiment at different temperatures and record the reaction rate.\n\nIdentify the independent variable.\nIdentify the dependent variable.\n\nAnswer:\n\nIndependent Variable: Temperature.\n\nExplanation: The scientist changes the temperature to see its effect on the reaction rate.\n\nDependent Variable: Reaction rate.\n\nExplanation: The reaction rate is measured to determine how it varies with temperature.\n\n\n\n\nQuestion 4\nScenario: A researcher is examining the relationship between physical activity and weight loss in a group of people. They track the amount of physical activity and the weight loss of each individual over six months.\n\nIdentify the independent variable.\nIdentify the dependent variable.\n\nAnswer:\n\nIndependent Variable: Amount of physical activity.\n\nExplanation: The amount of physical activity is observed to see its effect on weight loss.\n\nDependent Variable: Weight loss.\n\nExplanation: The weight loss is measured to see how it changes with different levels of physical activity.\n\n\n\n\nQuestion 5\nScenario: A study is conducted to determine if there is a relationship between smoking and lung cancer. The researchers collect data on smoking habits and lung cancer diagnoses.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential causal relationship, and if confounding variables might be present.\n\nAnswer:\n\nIndependent Variable: Smoking habits.\n\nExplanation: The smoking habits are observed to see if they are related to lung cancer.\n\nDependent Variable: Lung cancer diagnoses.\n\nExplanation: The diagnoses of lung cancer are measured to see if they are related to smoking habits.\n\nCausal Relationship: There is a potential causal relationship between smoking and lung cancer.\n\nExplanation: Smoking is known to cause lung cancer, but other factors (confounding variables) such as genetics and environmental exposure might also influence the relationship.\n\n\n\n\nQuestion 6\nScenario: A researcher is studying the effect of a new drug on blood pressure. They administer the drug to one group and a placebo to another group, then measure the blood pressure of both groups.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential causal relationship and if any confounding variables need to be controlled.\n\nAnswer:\n\nIndependent Variable: Administration of the new drug or placebo.\n\nExplanation: The researcher controls who receives the drug and who receives the placebo.\n\nDependent Variable: Blood pressure.\n\nExplanation: The blood pressure is measured to see how it changes in response to the drug or placebo.\n\nCausal Relationship: There is a potential causal relationship between the drug and blood pressure.\n\nExplanation: The drug might cause changes in blood pressure, but other factors such as diet and stress levels (confounding variables) should be controlled to ensure accurate results.\n\n\n\n\nQuestion 7\nScenario: A study is conducted to examine the relationship between income level and educational attainment. Researchers collect data on individuals’ income and their highest level of education completed.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential common response variable that might influence both income and educational attainment.\n\nAnswer:\n\nIndependent Variable: Income level.\n\nExplanation: The income level is observed to see if it is related to educational attainment.\n\nDependent Variable: Educational attainment.\n\nExplanation: The educational attainment is measured to see if it is related to income level.\n\nCommon Response: Socioeconomic status could be a common response variable.\n\nExplanation: Socioeconomic status might influence both income level and educational attainment, creating a relationship between the two.\n\n\n\n\nQuestion 8\nScenario: A researcher is investigating the effect of diet on cholesterol levels. They put participants on different diets and measure their cholesterol levels after three months.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential causal relationship and if any confounding variables need to be controlled.\n\nAnswer:\n\nIndependent Variable: Type of diet.\n\nExplanation: The researcher controls the type of diet that participants follow.\n\nDependent Variable: Cholesterol levels.\n\nExplanation: The cholesterol levels are measured to see how they change with different diets.\n\nCausal Relationship: There is a potential causal relationship between diet and cholesterol levels.\n\nExplanation: Diet might cause changes in cholesterol levels, but factors such as age and physical activity (confounding variables) should be controlled to ensure accurate results.\n\n\n\n\nQuestion 9\nScenario: A study is conducted to determine if there is a relationship between exercise frequency and mental health. Researchers collect data on how often participants exercise and their mental health status.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential causal relationship and if any confounding variables might be present.\n\nAnswer:\n\nIndependent Variable: Exercise frequency.\n\nExplanation: The exercise frequency is observed to see if it is related to mental health.\n\nDependent Variable: Mental health status.\n\nExplanation: The mental health status is measured to see if it is related to exercise frequency.\n\nCausal Relationship: There is a potential causal relationship between exercise frequency and mental health.\n\nExplanation: Exercise might improve mental health, but factors such as stress levels and social support (confounding variables) should be considered.\n\n\n\n\nQuestion 10\nScenario: A researcher is studying the effect of class size on student performance. They collect data on the number of students in a class and the average test scores of the students.\n\nIdentify the independent variable.\nIdentify the dependent variable.\nDetermine if there is a potential causal relationship and if any confounding variables might be present.\n\nAnswer:\n\nIndependent Variable: Class size.\n\nExplanation: The number of students in a class is observed to see its effect on student performance.\n\nDependent Variable: Average test scores.\n\nExplanation: The average test scores are measured to see how they vary with class size.\n\nCausal Relationship: There is a potential causal relationship between class size and student performance.\n\nExplanation: Smaller class sizes might lead to better student performance, but factors such as teaching quality and student socio-economic status (confounding variables) should be considered.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Independent_and_Dependent_Variables.html#conclusion",
    "href": "Independent_and_Dependent_Variables.html#conclusion",
    "title": "Independent and Dependent Variables",
    "section": "Conclusion",
    "text": "Conclusion\nNow that we have established the idea of independent / explanatory variables and dependent / response variables, how can we get started in trying to determine if there is a relationship between the variables?\nWhile there are several steps that would need to be done in order to prove this relationship, we will start down this road the same way we started down the road for single variables. We will ask questions, we will collect data, we will create visualizations for the data, and we will do some EDA to see if there could be a relationship. We will not go all the way to the end and firmly establish if there is a relationship. We will so some early analysis to see if we can easily rule out a relationship and the build up more evidence as we go.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Independent and Dependent Variables"
    ]
  },
  {
    "objectID": "Scatterplots_and_Correlation.html",
    "href": "Scatterplots_and_Correlation.html",
    "title": "Scatterplots and Correlation",
    "section": "",
    "text": "Describing a Scatterplot\nWhen we describe a scatterplot there are three components we want to discuss : form, direction, and strength.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Scatterplots and Correlation"
    ]
  },
  {
    "objectID": "Scatterplots_and_Correlation.html#describing-a-scatterplot",
    "href": "Scatterplots_and_Correlation.html#describing-a-scatterplot",
    "title": "Scatterplots and Correlation",
    "section": "",
    "text": "Form\nWhen we talk about the form of a scatterlpot, we are talking about the shape of the scatterplot. If the graph has a definitive pattern, then it can be modelled. What that means is that we can come up with a mathematical formula that follows the pattern of the data. As we will see later, having a model for our data set allows us to take deeper steps in understanding what the data has told us and what the data may still tell us. \nFor this course, we will focus our efforts on data sets that follow a linear pattern. These pattens are a nice introduction to creating a model for a data set. Linear models (lines) are the easiest to create. This does not mean that a linear model is the best model for every data set, far from it. As you advance in your studies you will find data that follows quadratic, logarithmic, and many other forms. You will even find data sets that will not follow any pattern whatsoever! \nTherefore, at least for this class, we will describe the form of a data set as either linear or non-linear.\n\n\nDirection\nSince we are only going to be creating linear models, the direction of the scatterplot really talks about the direction the model is going. Are the points of the data set going in an upward direction or a downward direction? If the points are going up, then the linear model would be a line that is going up. If the points are going down, then the linear model would be a line that is going down.\nIf you recall, when a line is going up, it has a positive slope, and if it is going down, then it has a negative slope. When a linear model is going up (positive slope), then the smaller explanatory values are matched with lower response variables while larger explanatory variables get matched with larger response variables. Similarly, for linear models that are going in a downward direction (negative slope), then smaller explantory variables get matched with larger response variables and larger explanatory variables get matched with smaller response variables.\nWe will use this same varnacular when discussing the direction of a scatterplot. If the points of the scatterplot are following a linear pattern that is going up, then we will say that this is a positive association. If the scatter plot is following a decreasing linear pattern then we will say it has a negative association.\n\n\nStrength\nThe strength of the scatterplot describes how closely the points follow a pattern. If the points follow a pattern very closely then the data is said to be strong. If it does not follow a pattern then it is said to be weak. \nWhere you have to be careful is when you are only considering one kind of pattern. For example, we are only considering linear patterns for now. So if we are given a data set that clearly follows a quadratic pattern, it would still have a model that is very good for that particular data set. But for our purposes, we would say that the strength is weak because it does not follow a linear pattern. What you need to remember is that when we are talking about the strength of a data set, we are talking exclusively about the linear strength of a data set.\nConsider the following pictures and briefly describe them using the three descriptors form, direction, and strength. \nThe first example is the data set we used above. A line has been added to help think about how to describe the data.\n\n\n\n\nForm : The data loosely follows a linear pattern.\nDirection : The data follows a positve association.\nStrength : While the data is not a perfect line, it does somewhat follow a linear pattern, so we will say this has moderate strength. \n\nConsider this scatterplot for our second example :\n\n\n\n\nForm : The data follows closely a linear pattern.\nDirection : The data follows a negative association.\nStrength : This data set is close to a perfect line, so we will say this association is strong. \n\nThe third example gives us a different shape.\n\n\n\n\nForm : The data clearly follows a pattern, but it is not a linear pattern. This means we will describe this form as non-linear.\nDirection : This data set goes down and up, so we should say it has a non-linear direction.\nStrength : The data closely follows a pattern, but it does not follow a linear pattern, Therefore we will say this has a weak linear strength. \n\nOur fourth example also gives us a different shape :\n\n\n\n\nForm : The data follows a logarithmic pattern so we would say the form is non-linear.\nDirection : This data set goes up but doesn’t follow a linear pattern, so we should say it has a non-linear direction.\nStrength : The data closely follows a pattern, but it does not follow a linear pattern, Therefore we will say this has a weak linear strength. \n\n\n\n\n\nForm : The data is just about a perfect line. Clearly we would say the form is linear.\nDirection : This data set goes up in a linear fashion, so we would say this has a positive association.\nStrength : The data follows a linear pattern about as well as possible. We would say this has a perfect strength. \n\nOur last example shows us a picture with no real pattern at all:\n\n\n\n\nForm : The data appears to be random points. We would say this has no form.\nDirection : This data doesn’t appear to go in any kind of direction so we would say no direction.\nStrength : The data follows no linear pattern at all, so we would say this scatterplot has no strength. \n\nSo if someone walks up to you on the street, hands you a scatterplot and asks you to describe the scatterplot, this is what you want to do. You want to describe the form, direction, and strength of the scatterplot. \nHopefully you have had at least one concern pop in your head when trying to determine some of these attributes from a picture. The form and direction should not be much of an issue from the picture, but what about the strength? This can sometimes be difficult to determine from a picture. \nA picture can be misleading depending on how it is presented. If a picture is drawn with a poor scale, the points themselves might appear to be much further apart from each other than they should, causing one to thing the strength is weaker than it really is. On the other hand, the scale could cause the points to be much closer than they should causing the reader to thing that there is more strength than there should be. What needs to be done to addressing the question of strength is to come up with a quantifiable measure (a formula) that can help us assess the linear strength of the data. \nLuckily for us, there is such a measure and it is called the correlation of the data set.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Scatterplots and Correlation"
    ]
  },
  {
    "objectID": "Scatterplots_and_Correlation.html#correlation",
    "href": "Scatterplots_and_Correlation.html#correlation",
    "title": "Scatterplots and Correlation",
    "section": "Correlation",
    "text": "Correlation\nCorrelation is a device that measures the linear strength between two variables. So before calcuating the correlation of a data set, it is often a good idea to create a scatterplot to see if the data follows a linear pattern. If it does, then calculating the correlation is a good next step. If the data does not follow a linear pattern, then it does not make sense to use a correlation. It is entirely possible that you are given a data set that follows a pattern very closely that is not a linear pattern. The data would have a good strength but it would not have a good linear strength. Since that is what correlation measures, make sure the data somewhat follows a linear pattern before moving forward. \nThat leads us to the obvious question of how we calculate the correlation. For this course, you will not be asked to prove the formula for correlation, nor will you be asked to calculate the correlation by hand. However, for completion sake, I will present to formula below : \n\n\n\n\nwhere\n\n\\(n\\) is the size of the sample\n\\(\\overline{x}\\) is the average of the explanatory variable\n\\(\\overline{y}\\) is the average of the response variable\n\\(s_x\\) is the standard deviation of the explanatory variable\n\\(s_y\\) is the standard deviation of the response variable\n\\(r\\) is the calcualted value called the correlation.\n\nThere are some properties for \\(r\\) we need to know in order to understand the result of the calculation.\n\n\\(r\\) should always be a value between -1 and 1.\nIf the scatterplot has a positive association, then \\(r\\) should have a positve value.\nIf the scatterplot has a negative association, then \\(r\\) should have a negative value.\nIf the strength is strong and positive, then \\(r\\) will be closer to 1.\nIf the strength is strong and negative, then \\(r\\) will be closer to -1.\nIf there is no association and the strength is weak then \\(r\\) will be close to \\(0\\)\n\nLuckily for us, this is very easy to calculate with R. We can use the built in cor( ) command. All you need to tell it are the two variables you want to find the correlation between. The form is pretty simple: \n\ncor( Explanatory Variable, Response Variable)\n\n\nNote : For this particular command the order of explanatory and response is not important, but there are other commands where the order is important. Make sure you read the descriptions to understand the order in which to write out the variables. \nLet’s go back to the grades example we look at above. If you recall, the scatterplot had a positive linear association that was mostly following a linear pattern. The pattern wasn’t very close to perfect, so \\(r \\neq 1\\), but it is not a weak pattern either so \\(r &gt; 0\\). In these cases, a guess of \\(r\\) being between 0.5 and 0.7 is probably pretty good. We can check below:\n\n# Calculate the coorelation between the Quiz Average and grade on Final\n\nr &lt;- cor(grades$Quiz_Average, grades$Final)\n\nr\n\n[1] 0.6086302\n\n\n\nHere are some examples of what other correlation values might look like :",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Scatterplots and Correlation"
    ]
  },
  {
    "objectID": "Scatterplots_and_Correlation.html#conclusion",
    "href": "Scatterplots_and_Correlation.html#conclusion",
    "title": "Scatterplots and Correlation",
    "section": "Conclusion",
    "text": "Conclusion\nLet’s not lose sight of what we are doing and why we are doing it. We have started down the road of trying to determine if there is a relationship between two variables and if there is a way we can create a mathematical model for that relationship. The model we have chosen as our first choice is a linear model. \nOur first step was to look at a visualization of the relationship. Therefore we created a scatterplot for the explanatory and response variable. After we had this completed, we look at the description (form, direction, and strength) of the scatterplot to determine if there was a linear pattern in the data. If there was, we were then curious about the linear strength of the data. To determine this, we calculated the correlation of the data. \nThis brings us to the next question. What’s next? \nIf we have decided that there is a linear relationship between the variables, then we want to come up with a model for this relationship. In our case we will be creating a linear model, which is a line. We will see how we can construct this model and how we can use it in the next section.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Scatterplots and Correlation"
    ]
  },
  {
    "objectID": "Scatterplots_and_Correlation.html#exercises",
    "href": "Scatterplots_and_Correlation.html#exercises",
    "title": "Scatterplots and Correlation",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will practice determining if there is a linear relationship between two quantitative variables in various built-in datasets in R. You will calculate the correlation and create basic scatterplots using ggplot2 to visualize the relationships. Each problem will involve different pairs of variables from different datasets. Provide solutions a nd explanations for each problem. \nFor each of the following problems, answer the following:\n\nCalculate the correlation between the variables given.\nCreate a scatterplot using ggplot2.\nDescribe the scatterplot by discussing the form, direction, and strength of the relationship.\nDetermine if this could be a good candidate for linear regression.\n\n\nProblem 1: Iris Dataset - Sepal Length vs Petal Length\nTask: Determine if there is a linear relationship between Sepal.Length and Petal.Length in the iris dataset.\nSolution and Explanation:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Calculate correlation\ncor_sepal_petal &lt;- cor(iris$Sepal.Length, iris$Petal.Length)\ncor_sepal_petal\n\n[1] 0.8717538\n\n# Create scatterplot\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Sepal Length vs Petal Length\", x = \"Sepal Length\", y = \"Petal Length\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Sepal Length and Petal Length.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 2: Airquality Dataset - Ozone vs Wind\nTask: Determine if there is a linear relationship between Ozone and Wind in the airquality dataset.\nSolution and Explanation:\n\n# Load ggplot2 and airquality dataset\nlibrary(ggplot2)\ndata(airquality)\n\n# Remove NA values\nairquality_clean &lt;- na.omit(airquality)\n\n# Calculate correlation\ncor_ozone_wind &lt;- cor(airquality_clean$Ozone, airquality_clean$Wind)\ncor_ozone_wind\n\n[1] -0.6124966\n\n# Create scatterplot\nggplot(airquality_clean, aes(x = Ozone, y = Wind)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Ozone vs Wind\", x = \"Ozone\", y = \"Wind\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Ozone and Wind.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 3: ToothGrowth Dataset - Dose vs Length\nTask: Determine if there is a linear relationship between dose and len in the ToothGrowth dataset.\nSolution and Explanation:\n\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Calculate correlation\ncor_dose_len &lt;- cor(ToothGrowth$dose, ToothGrowth$len)\ncor_dose_len\n\n[1] 0.8026913\n\n# Create scatterplot\nggplot(ToothGrowth, aes(x = dose, y = len)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Dose vs Length\", x = \"Dose\", y = \"Length\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Dose and Length.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 4: Cars Dataset - Speed vs Stopping Distance\nTask: Determine if there is a linear relationship between speed and dist in the cars dataset.\nSolution and Explanation:\n\n# Load ggplot2 and cars dataset\nlibrary(ggplot2)\ndata(cars)\n\n# Calculate correlation\ncor_speed_dist &lt;- cor(cars$speed, cars$dist)\ncor_speed_dist\n\n[1] 0.8068949\n\n# Create scatterplot\nggplot(cars, aes(x = speed, y = dist)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Speed vs Stopping Distance\", x = \"Speed\", y = \"Stopping Distance\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Speed and Stopping Distance.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 5: Faithful Dataset - Eruptions vs Waiting Time\nTask: Determine if there is a linear relationship between eruptions and waiting in the faithful dataset.\nSolution and Explanation:\n\n# Load ggplot2 and faithful dataset\nlibrary(ggplot2)\ndata(faithful)\n\n# Calculate correlation\ncor_eruptions_waiting &lt;- cor(faithful$eruptions, faithful$waiting)\ncor_eruptions_waiting\n\n[1] 0.9008112\n\n# Create scatterplot\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Eruptions vs Waiting Time\", x = \"Eruptions\", y = \"Waiting Time\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Eruptions and Waiting Time.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 6: ChickWeight Dataset - Time vs Weight\nTask: Determine if there is a linear relationship between Time and weight in the ChickWeight dataset.\nSolution and Explanation:\n\n# Load ggplot2 and ChickWeight dataset\nlibrary(ggplot2)\ndata(ChickWeight)\n\n# Calculate correlation\ncor_time_weight &lt;- cor(ChickWeight$Time, ChickWeight$weight)\ncor_time_weight\n\n[1] 0.8371017\n\n# Create scatterplot\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Time vs Weight\", x = \"Time\", y = \"Weight\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Time and Weight.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 7: Pressure Dataset - Temperature vs Pressure\nTask: Determine if there is a linear relationship between temperature and pressure in the pressure dataset.\nSolution and Explanation:\n\n# Load ggplot2 and pressure dataset\nlibrary(ggplot2)\ndata(pressure)\n\n# Calculate correlation\ncor_temperature_pressure &lt;- cor(pressure$temperature, pressure$pressure)\ncor_temperature_pressure\n\n[1] 0.7577923\n\n# Create scatterplot\nggplot(pressure, aes(x = temperature, y = pressure)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Temperature vs Pressure\", x = \"Temperature\", y = \"Pressure\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Temperature and Pressure.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 8: Trees Dataset - Girth vs Volume\nTask: Determine if there is a linear relationship between Girth and Volume in the trees dataset.\nSolution and Explanation:\n\n# Load ggplot2 and trees dataset\nlibrary(ggplot2)\ndata(trees)\n\n# Calculate correlation\ncor_girth_volume &lt;- cor(trees$Girth, trees$Volume)\ncor_girth_volume\n\n[1] 0.9671194\n\n# Create scatterplot\nggplot(trees, aes(x = Girth, y = Volume)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Girth vs Volume\", x = \"Girth\", y = \"Volume\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Girth and Volume.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 9: USArrests Dataset - Urban Population vs Assaults\nTask: Determine if there is a linear relationship between UrbanPop and Assault in the USArrests dataset.\nSolution and Explanation:\n\n# Load ggplot2 and USArrests dataset\nlibrary(ggplot2)\ndata(USArrests)\n\n# Calculate correlation\ncor_urbanpop_assault &lt;- cor(USArrests$UrbanPop, USArrests$Assault)\ncor_urbanpop_assault\n\n[1] 0.2588717\n\n# Create scatterplot\nggplot(USArrests, aes(x = UrbanPop, y = Assault)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Urban Population vs Assaults\", x = \"Urban Population\", y = \"Assaults\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Urban Population and Assaults.\n# The scatterplot visualizes this relationship.\n\n\n\nProblem 10: Swiss Dataset - Education vs Fertility\nTask: Determine if there is a linear relationship between Education and Fertility in the swiss dataset.\nSolution and Explanation:\n\n# Load ggplot2 and swiss dataset\nlibrary(ggplot2)\ndata(swiss)\n\n# Calculate correlation\ncor_education_fertility &lt;- cor(swiss$Education, swiss$Fertility)\ncor_education_fertility\n\n[1] -0.6637889\n\n# Create scatterplot\nggplot(swiss, aes(x = Education, y = Fertility)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Education vs Fertility\", x = \"Education\", y = \"Fertility\")\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Education and Fertility.\n# The scatterplot visualizes this relationship.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Scatterplots and Correlation"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html",
    "href": "Linear_Modeling_and_Regression.html",
    "title": "Linear Modeling and Regression",
    "section": "",
    "text": "Linear Model\nThe reason we want to use a linear model as our first model is because it is the easiest model to calculate. Here is the basic model :\nTo complete the model all we need to do is to calculate the slope ( \\(\\beta_1\\) ) and the intercept ( \\(\\beta_0\\) )! As we will see, this is quite easy to do in R. Let’s start to work through an example to help us understand the process.\nWe will consider the data set SVA-Weather-2013.csv. This is a dataset that collected data such as temperature, dew point, visibility, precipitation, and more from a local airport. We are wondering if the temperature affects the dew point. In other words, does a change in the temperature necessarily cause a change in the dew point. This tells us that the temperature is the explanatory variable and the dew point is the response variable. Our first steps would be to read in the dataset and to create a scatterplot of the data to see what type of pattern (association) we can see.\n# Load up readr package. It contains the \"read_csv( )\" command\n\nlibrary(readr)\n# Read in the data\n\nSVA &lt;- read_csv(\"SVA-Weather-2013.csv\")\n\nRows: 365 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): YEARMODA, TEMP, DEWP, STP, VISIB, WDSP, MXSPD, MAX, MIN, PRCP\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Examine the first few rows to make sure the data looks OK.\n\nhead(SVA)\n\n# A tibble: 6 × 10\n  YEARMODA  TEMP  DEWP   STP VISIB  WDSP MXSPD   MAX   MIN  PRCP\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 20130101  43.3  25.8  971.  10     5.4   8.9  47.8  37.8     0\n2 20130102  38.5  29.4  973.  10     1.5   5.1  42.3  32       0\n3 20130103  32    19.2  978   10     1.4   7    42.6  21.9     0\n4 20130104  34.6  17.4  978.  10     3.6   8.9  45.9  26.2     0\n5 20130105  36    17.9  981   10     3.4   8    48.2  25.9     0\n6 20130106  40.6  26.9  977.   9.9   3.8   9.9  47.1  31.1    NA\nWe can now create a scatterplot for the data :\n# Remember to make sure tidyverse is loaded up to use ggplot! \n\nlibrary(tidyverse)\n# We are now ready to create the scatterplot\n\nggplot(SVA, aes(x=TEMP, y=DEWP)) +\n  geom_point()\nRecall that we are going to use three characteristics to describe a scatterplot: the form, direction and strength. Based on this scatterplot we could deduce the following :\nThis is showing us that there appears to be a positive, linear association between the two variables and since the pattern has a clear linear pattern we can determine that the correlation is strong. It is not perfect, but it is still good. We will see below that R is close to 0.95 for this data set. All of this tells us that a linear model would be a good fit for this particular dataset.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#linear-model",
    "href": "Linear_Modeling_and_Regression.html#linear-model",
    "title": "Linear Modeling and Regression",
    "section": "",
    "text": "Form : Linear\nDirection : Positive Association\nStrength : Strong correlation",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#finding-the-slope",
    "href": "Linear_Modeling_and_Regression.html#finding-the-slope",
    "title": "Linear Modeling and Regression",
    "section": "Finding the Slope",
    "text": "Finding the Slope\nOur next step then would be to consider how to calculate the slope of the model. While in practice we will not calculate these by hand, it is worth seeing these formulas so we can better understand them and interpret them. The slope of the regression line \\(b_1\\) can be calculated as follows : \n\n\n\n where\n\n\\(s_y\\) is the standard deviation of the response variable\n\\(s_x\\) is the standard deviation of the explanatory variable\n\\(R\\) is the correlation\n\nWe can calculate these easily using R:\n\n# Find the standard deviation of the response variable, Dew Point \n# using the built-in \"sd( )\" command :\n\nDEWP_sd &lt;- sd(SVA$DEWP)\n\n# Print it out to see the result : \n\nDEWP_sd\n\n[1] 18.12688\n\n\n\nNow do the same for the explanatory variable, Temperature\n\nTEMP_sd &lt;- sd(SVA$TEMP)\n\nTEMP_sd\n\n[1] 15.56908\n\n\n\nNext we can calculate the correlation.\n\ncor(SVA$TEMP, SVA$DEWP)\n\n[1] 0.9511601\n\n\n\nWe can now calculate the slope : \n\n\\(\\displaystyle{b_1\\,\\,\\, = \\,\\,\\,\\frac{s_y}{s_x}\\, R\\,\\,\\, =\n\\,\\,\\,\\frac{(18.12668)}{(15.56908)}\\,(0.9511601)\\,\\,\\, =\\,\\,\\, 1.107 }\\)\n\n\n\n\n\n\n\n\nInterpreting the Slope\n\n\n\nHere is how we can interpret the slope. For this example, the slope tells that as the explanatory increases by 1 unit, the reponse variable will increase by 1.107 units. \nIf we had calculated \\(r = -.45\\), then we would say that as the explanatory variable increases by 1 unit, the response variable decreases by 0.45 units",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#finding-the-intercept",
    "href": "Linear_Modeling_and_Regression.html#finding-the-intercept",
    "title": "Linear Modeling and Regression",
    "section": "Finding the Intercept",
    "text": "Finding the Intercept\nNow that we have the slope, we are ready to calculate the intercept, \\(b_0\\). If you recall, the intercept is where the model would intersect the \\(y\\)- axis. The calculation of the intercept uses the fact the regression line passes through the point \\(( \\overline{x}, \\overline{y})\\) where \\(\\overline{x}\\) is the average of the explanatory variable and \\(\\overline{y}\\) is the average of the response variable. \nWe can solve for the intercept \\(b_0\\) by plugging these values back into our model to get \n\n\n\n\nAs you can see from this formula, we will always need to calculate the slope first since we use it here. We also need to calculate these averages: \n\n# Calculate the average of the Temperatures\n\nTEMP_avg &lt;- mean(SVA$TEMP)\n\nTEMP_avg\n\n[1] 53.98219\n\n# Calculate the average of the Dew Points\n\nDEWP_avg &lt;- mean(SVA$DEWP)\n\nDEWP_avg\n\n[1] 41.72849\n\n\nThis shows us \\(\\overline{x} = 53398219\\) and \\(\\overline{y} = 41.72849\\) We can now calculate the intercept as : \n\n\\(\\displaystyle{b_0\\,\\,\\, = \\,\\,\\, \\overline{y} \\,\\,\\, -\\,\\,\\,b_1 \\,\n\\overline{x}\\,\\,\\, =\\,\\,\\, 41.72849\\,\\,\\, - \\,\\,\\, (1.107)\\,(53.98219)\\,\\,\\, =\n\\,\\,\\,-18.03}\\)\n\n\n\n\n\n\n\n\nInterpreting the Intercept\n\n\n\nHere is how we can interpret the intercept. This is the response variable if the explanatory variable we set to zero. \nDo not forget to keep the context of the data set. Sometimes having the explanatory going to zero makes sense, and other times it does not.\n\n\nWe now have the slope and intercept for our model and can construct it as: \n\n\\(\\displaystyle{\\hat{y} \\,\\,\\, = (slope)\\,*\\, x \\,\\,\\, + \\,\\,\\, (intercept)}\\) \n\\(\\displaystyle{\\hat{y} \\,\\,\\, = 1.107\\,*\\, x \\,\\,\\, + \\,\\,\\, (-18.03)}\\) \n\\(\\displaystyle{\\hat{y} \\,\\,\\, = 1.107\\,*\\, x \\,\\,\\, - \\,\\,\\, 18.03}\\)",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#drawing-the-model",
    "href": "Linear_Modeling_and_Regression.html#drawing-the-model",
    "title": "Linear Modeling and Regression",
    "section": "Drawing the Model",
    "text": "Drawing the Model\nNow that we have the equation for the model (or regression line), how do we put the model on the scatterplot? Remember that in order to graph a line, all you need are two points. So let’s create two points, connect the dots and we will have our line on the scatterplot!\nWhen picking two points to plot, make sure you pick two values that are relevant to the data. In this case, the explanatory variables run from about 20 to 80, so let’s pick the values of \\(x_1 = 40\\) and \\(x_2 = 60\\) to create the points for the model. All we do is put these values into the model to see what response values pop back out : \n\n\\(\\displaystyle{y_1 \\,\\,\\, = \\,\\,\\,(1.107)(40)\\,\\,\\, -\\,\\,\\, 18.03\\,\\,\\,\n= \\,\\,\\, 44.28 \\,\\,\\,-\\,\\,\\,18.03\\,\\,\\,=\\,\\,\\,26.25}\\)\n\n\nThis tells me one point on the model is (40, 26.25). We can similary find the second point:\n\n\\(\\displaystyle{y_2 \\,\\,\\, = \\,\\,\\,(1.107)(60)\\,\\,\\, -\\,\\,\\, 18.03\\,\\,\\,\n= \\,\\,\\, 66.42 \\,\\,\\,-\\,\\,\\,18.03\\,\\,\\,=\\,\\,\\,48.39}\\)\n\n\nThe second point we will use is (60, 48.39). We can now draw these points on the scatterplot : \n\n\n\n\nand then connect our dots to draw the model : \n\n\n\n\nThis was a quick example to show you the math behind finding the linear model. We will be using R to do most of the heavy lifting for us in creating the model.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#adding-the-model-to-the-scatterplot-with-ggplot",
    "href": "Linear_Modeling_and_Regression.html#adding-the-model-to-the-scatterplot-with-ggplot",
    "title": "Linear Modeling and Regression",
    "section": "Adding the Model to the Scatterplot with ggplot",
    "text": "Adding the Model to the Scatterplot with ggplot\nAs we have been developing our skills with ggplot we have seen that it builds its images one layer at a time. We will take advantage of this by simply adding one more layer to the scatterplot from earlier when we want to draw the model on the data. \nWe will be using the lm( ) command to draw the linear model for us. This command is located in the stats package, so make sure it is loaded up before using this command. The syntax of the command is fairly easy, but you do have to be a little careful. If you remcall, when we calculated the correlation, the order in which we listed the variables did not matter. However, it is very important to list the variables in the proper order for the lm( ) command. Here is the syntax: \n\n\n\n\nThere are a couple of ways we can denote the explanatory and response variables. We could give the lm( ) command the name of the data set and the short names for the vairables, or we could just give the fullname for the variables. In this example, we could use either one of the following two methods : \n\n\n\n\nor \n\n\n\n\nAgain, it is important to note the order in which the variables are listed : Response followed by Explanatory. \n\n\n\n\n\n\nNote\n\n\n\nNote that it is not necessary to add the “formula =” part of the command. We could do something like this:\n\nlm(DEWP ~ TEMP, data=SVA)\n\n\n\nWhen we run this command, we will see that we are given the slope and the intercept for the linear model. \n\n# Load the stats package\n\nlibrary(stats)\n\n\n# Style 1 : \n\nlm(SVA$DEWP ~ SVA$TEMP)\n\n\nCall:\nlm(formula = SVA$DEWP ~ SVA$TEMP)\n\nCoefficients:\n(Intercept)     SVA$TEMP  \n    -18.053        1.107  \n\n\n\n\n# Style 2 : \n\nlm(DEWP ~ TEMP, data=SVA)\n\n\nCall:\nlm(formula = DEWP ~ TEMP, data = SVA)\n\nCoefficients:\n(Intercept)         TEMP  \n    -18.053        1.107  \n\n\n\nYou will notice a slight discrepancy in our work above and what we are shown here. That is because when we did this by hand we rounded after a few decimal places. R will not do that until the end, causing the slight change. \nWe can now add this layer to the scatterplot using the following command : \n\n\n\n\nYou can see that the first two lines are how we created the initial scatterplot. The third layer we added was the geom_smooth( ) command. This is the command that will put a model on the data for us. Creating a model is a process called smoothing the data, hence the name of the command. There are several methods we could use, so we will tell it to use the lm( ) method for linear model. The last parameter asks if we want to put in the standard error. This is a buffer where we acknowledge that the sampling procedure produces some variability that could affect the model. For this example, let’s leave it out for now so we can see that the scatterplot looks like with just the linear model on top: \n\nggplot(SVA, aes(x=TEMP, y=DEWP)) +\n  geom_point() +\n  geom_smooth(method='lm', se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nJust so you can see it, here is what the plot would look like with the standard error included : \n\nggplot(SVA, aes(x=TEMP, y=DEWP)) +\n  geom_point() +\n  geom_smooth(method='lm', se=TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nYou have to look closely on this one. The gray area around the line represents the standard error. Since the correlation was so strong, there is not much room for error. To show you a better exmaple for standard error, let’s create a scatterplot for Temperature affecting Wind Speed:\n\ncor(SVA$TEMP, SVA$WDSP)\n\n[1] -0.1391327\n\n\n\nIn this case, the correlation is much weaker so the standard error should be larger. We can take a quick look here:\n\nggplot(SVA, aes(x=TEMP, y=WDSP)) +\n  geom_point() +\n  geom_smooth(method='lm', se=TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#conclusion",
    "href": "Linear_Modeling_and_Regression.html#conclusion",
    "title": "Linear Modeling and Regression",
    "section": "Conclusion",
    "text": "Conclusion\nWe have now seen how we can quickly compute and add a linear model to help us understand our dataset. One idea we will come back to in the next section is what we can do with this model. We want to talk a bit more on why this is the “model of best fit”, as well as look at the first application for these regression lines (linear models).",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Linear_Modeling_and_Regression.html#exercises",
    "href": "Linear_Modeling_and_Regression.html#exercises",
    "title": "Linear Modeling and Regression",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will practice determining if there is a linear relationship between two quantitative variables in various built-in datasets in R. You will calculate the correlation, create basic scatterplots using ggplot2, and add a linear model to the scatterplots using geom_smooth with the lm method to visualize the relationships. Each problem will involve different pairs of variables from different datasets. Provide solutions and explanations for each problem.\nFor each problem, answer the following:\n\nCalculate the correlation between the given variables.\nCreate a scatterplot using ggplot2.\nCalculate the slope and intercept for the linear model. Write out the equation for the linear model.\nAdd a linear model to the scatterplot using geom_smooth with the lm method.\nDetermine if the linear model is appropriate for the data.\n\n\nProblem 1: Iris Dataset - Sepal Length vs Petal Length\nTask: Determine if there is a linear relationship between Sepal.Length and Petal.Length in the iris dataset.\nSolution and Explanation:\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Calculate correlation\ncor_sepal_petal &lt;- cor(iris$Sepal.Length, iris$Petal.Length)\ncor_sepal_petal\n\n[1] 0.8717538\n\n# Create scatterplot with linear model\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Sepal Length vs Petal Length with Linear Model\", x = \"Sepal Length\", y = \"Petal Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Sepal Length and Petal Length.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 2: Airquality Dataset - Ozone vs Wind\nTask: Determine if there is a linear relationship between Ozone and Wind in the airquality dataset.\nSolution and Explanation:\n\n# Load ggplot2 and airquality dataset\nlibrary(ggplot2)\ndata(airquality)\n\n# Remove NA values\nairquality_clean &lt;- na.omit(airquality)\n\n# Calculate correlation\ncor_ozone_wind &lt;- cor(airquality_clean$Ozone, airquality_clean$Wind)\ncor_ozone_wind\n\n[1] -0.6124966\n\n# Create scatterplot with linear model\nggplot(airquality_clean, aes(x = Ozone, y = Wind)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Ozone vs Wind with Linear Model\", x = \"Ozone\", y = \"Wind\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Ozone and Wind.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 3: ToothGrowth Dataset - Dose vs Length\nSolution and Explanation:\n\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Calculate correlation\ncor_dose_len &lt;- cor(ToothGrowth$dose, ToothGrowth$len)\ncor_dose_len\n\n[1] 0.8026913\n\n# Create scatterplot with linear model\nggplot(ToothGrowth, aes(x = dose, y = len)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Dose vs Length with Linear Model\", x = \"Dose\", y = \"Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Dose and Length.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 4: Cars Dataset - Speed vs Stopping Distance\nTask: Determine if there is a linear relationship between speed and dist in the cars dataset.\nSolution and Explanation:\n\n# Load ggplot2 and cars dataset\nlibrary(ggplot2)\ndata(cars)\n\n# Calculate correlation\ncor_speed_dist &lt;- cor(cars$speed, cars$dist)\ncor_speed_dist\n\n[1] 0.8068949\n\n# Create scatterplot with linear model\nggplot(cars, aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Speed vs Stopping Distance with Linear Model\", x = \"Speed\", y = \"Stopping Distance\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Speed and Stopping Distance.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 5: Faithful Dataset - Eruptions vs Waiting Time\nTask: Determine if there is a linear relationship between eruptions and waiting in the faithful dataset.\nSolution and Explanation:\n\n# Load ggplot2 and faithful dataset\nlibrary(ggplot2)\ndata(faithful)\n\n# Calculate correlation\ncor_eruptions_waiting &lt;- cor(faithful$eruptions, faithful$waiting)\ncor_eruptions_waiting\n\n[1] 0.9008112\n\n# Create scatterplot with linear model\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Eruptions vs Waiting Time with Linear Model\", x = \"Eruptions\", y = \"Waiting Time\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Eruptions and Waiting Time.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 6: ChickWeight Dataset - Time vs Weight\nTask: Determine if there is a linear relationship between Time and weight in the ChickWeight dataset.\nSolution and Explanation:\n\n# Load ggplot2 and ChickWeight dataset\nlibrary(ggplot2)\ndata(ChickWeight)\n\n# Calculate correlation\ncor_time_weight &lt;- cor(ChickWeight$Time, ChickWeight$weight)\ncor_time_weight\n\n[1] 0.8371017\n\n# Create scatterplot with linear model\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Time vs Weight with Linear Model\", x = \"Time\", y = \"Weight\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Time and Weight.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 7: Pressure Dataset - Temperature vs Pressure\nTask: Determine if there is a linear relationship between temperature and pressure in the pressure dataset.\nSolution and Explanation:\n\n# Load ggplot2 and pressure dataset\nlibrary(ggplot2)\ndata(pressure)\n\n# Calculate correlation\ncor_temperature_pressure &lt;- cor(pressure$temperature, pressure$pressure)\ncor_temperature_pressure\n\n[1] 0.7577923\n\n# Create scatterplot with linear model\nggplot(pressure, aes(x = temperature, y = pressure)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Temperature vs Pressure with Linear Model\", x = \"Temperature\", y = \"Pressure\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Temperature and Pressure.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 8: Trees Dataset - Girth vs Volume\nTask: Determine if there is a linear relationship between Girth and Volume in the trees dataset.\nSolution and Explanation:\n\n# Load ggplot2 and trees dataset\nlibrary(ggplot2)\ndata(trees)\n\n# Calculate correlation\ncor_girth_volume &lt;- cor(trees$Girth, trees$Volume)\ncor_girth_volume\n\n[1] 0.9671194\n\n# Create scatterplot with linear model\nggplot(trees, aes(x = Girth, y = Volume)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Girth vs Volume with Linear Model\", x = \"Girth\", y = \"Volume\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Girth and Volume.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 9: USArrests Dataset - Urban Population vs Assaults\nTask: Determine if there is a linear relationship between UrbanPop and Assault in the USArrests dataset.\nSolution and Explanation:\n\n# Load ggplot2 and USArrests dataset\nlibrary(ggplot2)\ndata(USArrests)\n\n# Calculate correlation\ncor_urbanpop_assault &lt;- cor(USArrests$UrbanPop, USArrests$Assault)\ncor_urbanpop_assault\n\n[1] 0.2588717\n\n# Create scatterplot with linear model\nggplot(USArrests, aes(x = UrbanPop, y = Assault)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Urban Population vs Assaults with Linear Model\", x = \"Urban Population\", y = \"Assaults\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Urban Population and Assaults.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.\n\n\n\nProblem 10: Swiss Dataset - Education vs Fertility\nTask: Determine if there is a linear relationship between Education and Fertility in the swiss dataset.\nSolution and Explanation:\n\n# Load ggplot2 and swiss dataset\nlibrary(ggplot2)\ndata(swiss)\n\n# Calculate correlation\ncor_education_fertility &lt;- cor(swiss$Education, swiss$Fertility)\ncor_education_fertility\n\n[1] -0.6637889\n\n# Create scatterplot with linear model\nggplot(swiss, aes(x = Education, y = Fertility)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Education vs Fertility with Linear Model\", x = \"Education\", y = \"Fertility\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Explanation:\n# The correlation value indicates the strength and direction of the linear relationship between Education and Fertility.\n# The scatterplot visualizes this relationship, and the linear model (line) shows the best fit line.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Linear Modeling and Regression"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html",
    "href": "Residuals_Outliers_Predictions.html",
    "title": "Residuals, Outliers and Predictions",
    "section": "",
    "text": "Finding Residuals\nWe found that the slope of the model (or regression line) was \\(b_1 \\,= \\,1.107\\) and the intercept was found to be \\(b_0\\, = \\,-18.03\\). This gave us a regression line of\nWhen constructed in this manner, this model is known as the line of best fit. It was mentioned earlier that what is meant by best fit is where the vertical distances between the points and the model is as small as possible. We will jump into a deeper example in a bit, but the first application of this regression line is to make predictions. What this means is that every point that is on the regression line is the predicted response value based on the explanatory value given. Consider this small example :\nHere is a table that has the actual points from the data set and the predicted values taken from the linear model:\nX\nY\nPredicted_Y\n\n\n\n\n1\n2\n3.5\n\n\n2\n8\n4.0\n\n\n4\n3\n5.0\n\n\n6\n7\n6.0\n\n\n8\n8\n7.0\nThis image shows is two different type of points, one that is above the regression line and one that is below. Consider the point that is 4 units above the line. It appears the actual point is located at \\((2, 8)\\) while the point on the line is at \\((2, 4)\\). We can interpret this example from the regression line as:\nWhen the explanatory value of \\(x = 2\\) is used in the model, the predicted response value is \\(\\hat{y} = 4.0\\).\nAs you can see from the graph, the predicted value does not match up with the actual value (green arrow). We have some distance between the actual and predicted \\(y\\) values. The distance between the actual point and the line is called a residual. You can see the point above the line has a residual of \\(4\\) and the point below the line has a residual of \\(-2\\) (red arrow).\nCalculating the residual is easy enough :\nAs you can see above, the point \\((\\,2\\, , \\,8\\,)\\) has a predicted value of \\(4\\) when \\(x=2\\), which leads to a residual of \\(8-4 =4\\).\nThe point \\((\\,4\\, , \\, 3 \\,)\\) has a predicted value of 5 when \\(x=4\\), which returns a residual of \\(3 - 5 = -2\\).\nThis shows us that when a residual turns out positive, then the actual point is above the regression line and when the actual point is below the line the residual will wind up being negative. This brings us back to the idea of “line of best fit”.\nIf we had wanted, we could have put any line on this graph to use as our linear model. But how could we determine when one model was better than another? What we can do is use the residuals to help us decide. It is tempting to say that the model where the sum of the residuals is the smallest would be the best fit. While this is a good idea, there are some problems that might arise because we are adding positive and negative values together.\nOur solution will then be to take all of the residuals and square them. When we do this, we will turn the negative residuals into positive values when they are squared and the positive residuals will still return positive values when they are squared. We can now add these squared residuals together and use the model that has the smallest total.\nThe model that we created earlier has been shown as the model that returns the smallest value. This is why the linear model (or regression line) is also sometimes called the least squares regression line.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#finding-residuals",
    "href": "Residuals_Outliers_Predictions.html#finding-residuals",
    "title": "Residuals, Outliers and Predictions",
    "section": "",
    "text": "\\(\\displaystyle{\\hat{y} \\,\\,\\, = 1.107\\,*\\, x \\,\\,\\, - \\,\\,\\, 18.03}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(residual \\,\\, = \\,\\, (actual \\, response\\, value\\,) \\, - \\, (predicted\n\\, response\\, value \\,)\\)",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#finding-outliers",
    "href": "Residuals_Outliers_Predictions.html#finding-outliers",
    "title": "Residuals, Outliers and Predictions",
    "section": "Finding Outliers",
    "text": "Finding Outliers\nWhen we determing how to find some measures of center and spread, there were certain situations when one measure was better than another. For example, when we were calculating the average of a data set, we had to be cautious if the data set had outliers. As we saw, even a few outliers could severly affect the reliability of the mean. Since the standard deviation is calculated using the mean, it is also affected by outliers. So what does that say about the correlation between two variables, the slope of the regression line ot the intercept of the regression line? After all, they also use averages and standard deviations when we calculated these values? \nThis tells us that outliers can affect all three of them, so they can also affect our model. That leads us to the question of how we can determine if a data set has outliers. We saw how we can use the IQR method if we are looking at a single variable, but how does having two variables change identifying outliers? It all comes back to the scatterplot. \nAn outlier is a point that does not follow the pattern of the data. It will be a point that stands out away from the others. For example, look at the following scatterplot: \n\n\n\n\nIn this graph we can definitely see one point that does not follow the pattern at all. This is a point that we would investigate as a possible outlier. Do not rule it out of hand because there are several reasons why this point appears to be an outlier.\n\nMeasurement Error: This can occur when collecting the data and the measurement tool that is being used is faulty or inaccurate.\nData Entry Error: Errors caused by humans, due to invalid data collection, data entry, or measurement can lead to outliers in the data.\nExperimental Error: If the data collection involves experimentation, oftentimes there can be errors while planning and executing the experiment.\nData Processing Error: After the data is collected, the data is often processed. This process includes data modeling and manipulation; which can l ead to the creation of outliers if not performed correctly.\nTrue Outlier: Outliers that are not created due to human error are natural Outliers. These data points are true and can have many reasons behind their existence.\n\n\nOne of the ways to test if this is an outlier is to examine the scatterplot with and without the possible outliers. If removing the point causes a drastic change in the regression line, then this is good evidence that that point is an outlier and needs to be investigated further. \nHere is a side by side picture of the scatterplot with and without the suspected outlier: \n\n\n\n\nAs you can see, we do have a dramatic shift in the regression line. The data set that contains the outlier has a regression line that has a negative association and when the suspected outlier is removed, then we can see that the new model has changed direction and now has a positive association. If the removed point were not an outlier, then the regression line would have changed slightly.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#influential-observations",
    "href": "Residuals_Outliers_Predictions.html#influential-observations",
    "title": "Residuals, Outliers and Predictions",
    "section": "Influential Observations",
    "text": "Influential Observations\nThere is one other type of point to be cautious of because it may look like an outlier, but it is really what is called an influential observation. These are points that are far from most of the data, yet it still follows the pattern of the regession line. In the example above, the suspected outlier was far from the data and did not follow the pattern of the model. Here is an example: \n\n\n\n\nAs you can see from this graph, we have a point that looks like it could be an outlier. However, it still follows the model for the regression line fairly closely. Because of this, we will not call this point an outlier, but we will label it as an influential observation.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#making-predictions",
    "href": "Residuals_Outliers_Predictions.html#making-predictions",
    "title": "Residuals, Outliers and Predictions",
    "section": "Making Predictions",
    "text": "Making Predictions\nLet’s now take the ideas and turn our attention back to the weather example we have been using. \nWe have been examining if there is any kind of association between Temperature and Dew Point. Based upon earlier work, we found the data to be appropriate for a linear model. We then developed the model as : \n\n\\(\\hat{y} \\, = \\, 1.107*x - 18.03\\)\n\n\nUsing this model, we could now start to make predictions for the Dew Point based on values from the Temperature. For instance, I could ask what the predicted (expected) Dew Point will be if the Temperature is 60 degrees. All we need to do is to go to our model and and let the explanatory variable, x, be 60: \n\n\\(\\hat{y} \\, = \\, (1.107)*(60) - 18.03 = 66.42 - 18.03 = 48.39\\)\n\n\nThis tells us that when the Temperature is 60 degrees, the expected Dew Point is at 48.39 degrees. Also, this says that the point \\((60, 48.39)\\) is a point on the regression line. \nFor another example, let’s consider the first day of the year. On this day the temperature was listed at 43.3 degrees, what would be the predicted value for the Dew Point? \n\n\\(\\hat{y} \\, = \\, (1.107)*(43.3) - 18.03 = 47.93 - 18.03 = 29.9\\)\n\n\nFinding predictions one at a time like this would be fine, but what if we wanted to make several predictions at once? We could use the predict( ) function in R to help us with this. The syntax for this function is: \n\npredict(regression_line, newdata = data.frame(Explanatory_Values))\n\n\nThis says that we need to calculate the regression line, save it to a variable, and then run the predict( ) command using the name of this regression line. We will also need to create a data frame with the explanatory values that we want to use for our predictions. \nLet’s go back to the example where we wanted to make a prediction for the Dew Point when the Temperature was 60 degrees. We will use the predict( ) function to make this prediction. \n\n# Recall the lm( ) command needs the Explanatory first followed by Response\n\nreg_line &lt;- lm(SVA$DEWP ~ SVA$TEMP)\n\n# We have now saved the regression line to the variable \"reg_line\"\n\n# We will now send all of the temperatures in SVA$TEMP to the \"predict( )\" \n# command and save the results to the variable \"predictions\". If we didn't\n# save the results, then all of the predictions would be printed to the console.\n\npredictions &lt;- predict(reg_line, newdata = data.frame(SVA$TEMP))\n\n# We can now just look at the first few predictions.\n\nhead(predictions, 8)\n\n       1        2        3        4        5        6        7        8 \n29.89879 24.58315 17.38490 20.26420 21.81460 26.90874 26.90874 22.81128 \n\n\n\nIf we wanted to use predict( ) to make a prediction for the Dew Point when the Temperature was 43.3 degrees, we could use the following code: \n\n# Recall the predict command needs a data frame for the values we want to\n# predict. We will use the value 43.3 for the temperature.\n\nreg_line&lt;- lm(DEWP ~ TEMP, data = SVA)\n\n# We need to create a data frame for this point. Let's call the new data\n# frame \"predicted-values\" \n\npredicted_values = data.frame(TEMP = 43.3)\n\npredict(reg_line, newdata = predicted_values)\n\n       1 \n29.89879 \n\n\nThis tells me that when the temperature was 43.3 degrees, the predicted Dew Point was 29.9 degrees. \nIf we wanted to predict 5 values, say Temp = 30, 35, 40, 45, 50, we could use the following code: \n\n# Recall the predict command needs a data frame for the values we want to\n# predict. We will use the values 30, 35, 40, 45, and 50 for the temperature.\n\n# We need to create a data frame for these points. Let's call the new data\n\npredicted_values = data.frame(TEMP = c(30, 35, 40, 45, 50))\n\npredict(reg_line, newdata = predicted_values)\n\n       1        2        3        4        5 \n15.17006 20.70717 26.24429 31.78141 37.31852 \n\n\nThe output is in the same order as the data frame we created. \nThis says that\n\nwhen the temperature was 30 degrees, the predicted Dew Point was 13.3 degrees.\nwhen the temperature was 35 degrees, the predicted Dew Point was 18.4 degrees.\nwhen the temperature was 40 degrees, the predicted Dew Point was 23.5 degrees.\nwhen the temperature was 45 degrees, the predicted Dew Point was 28.6 degrees.\nwhen the temperature was 50 degrees, the predicted Dew Point was 33.7 degrees.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#calculating-residuals-and-residual-plots",
    "href": "Residuals_Outliers_Predictions.html#calculating-residuals-and-residual-plots",
    "title": "Residuals, Outliers and Predictions",
    "section": "Calculating Residuals and Residual Plots",
    "text": "Calculating Residuals and Residual Plots\nHow close is the predicted value from the actual value? Here is the actual data from that day: \n\n\n\n\nWe could then calculate the residual to get : \n\n\\(residual \\, = \\, 25.8 - 29.9\\, = \\, -4.1\\)\n\n\nBecause the residual is negative, we know our point falls below the regression line telling us that the predicted value is larger than the acutal value. So the regression line is above the actual data point. \nLuckily, R has a built-in function that will calculate all of these residuals for us. The syntax is: \n\nresid(regression_line)\n\n\nThis says that we need to calculate the regression line, save it to a variable, and then run the resid( ) command using the name of this regression line. Here is how we could calculate all of the residuals for our current Weather data set. \n\n# Recall the lm( ) command needs the Explanatory first followed by Response\n\nreg_line &lt;- lm(SVA$DEWP ~ SVA$TEMP)\n\n# We have now saved the regression line to the variable \"reg_line\"\n\n# We will now send this to the \"resid( )\" command. Note that there are \n# 365 days in this data set. In order to not print out the entire set\n# of residuals, I will print off the first 12 so you can see the result\n\nhead(resid(reg_line), 12)\n\n           1            2            3            4            5            6 \n-4.098786162  4.816845190  1.815095980 -2.864204336 -3.914596813 -0.008743526 \n           7            8            9           10           11           12 \n-0.808743526  0.988722308  5.252391271 -3.399571118  1.640079040  7.074055210 \n\n\n\nResiduals are another tool we can use to help us determine if the linear model is a good fit. \nA residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate. \n\n# Load required library for ggplot\n\nlibrary(ggplot2)\n\n# Fit a linear model predicting TEMP using DEW\n\nmodel &lt;- lm(TEMP ~ DEWP, data = SVA)\n\n# In order to use ggplot, we need to create a data frame\n# Create a data frame with explanatory values and residuals\n\nresidual_data &lt;- data.frame(\n  Explanatory_Values = SVA$TEMP,\n  Residuals = resid(model)\n)\n\n# Create a residual plot using ggplot2\n\n# Notice the third layer is just a dashed horizontal line through 0. This makes \n# it easier to see which points are above and below the regression line.\n\n# The fourth layer is an arrow to show you the first residual we found.\n\n# The Explanatory value was 43.3 and the residual was -4.1\n\nggplot(residual_data, aes(x = Explanatory_Values, y = Residuals)) +\n  geom_point(color = \"blue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_segment(aes(x = 50, y = -12, xend = 43.3, yend = -4.1),\n    arrow = arrow(length = unit(0.25, \"cm\"), type = \"closed\"),\n    color = \"red\",\n    size = 0.25,\n    linetype = \"solid\"\n  ) +\n  theme_minimal() +\n  labs(title = \"Residual Plot\",\n       x = \"Explanatory Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\n\nRemember that if there is no clear pattern to this picture, then that is more evidence that the linear model is a good fit. In this case the points are fairly random around the line \\(y = 0\\) which leads to confidence the linear pattern is a good choice. \nHere is an example of a residual plot with a non-linear pattern. Once we see this, we should recognize that the linear model is not a good fit. \n\n\n\n\nThere is one final warning for you when it comes to making predictions using a model. When you are making predictions, you want to make sure that you are using explanatory values that are relevant to the data set. For instance, in our SVA data set, look at the largest and smallest temperatures: \n\n# Find the maximum value\n\nmax(SVA$TEMP)\n\n[1] 79.1\n\n# Find the minimum value\nmin(SVA$TEMP)\n\n[1] 16.9\n\n\n\nBased on this quick check, we only want to make predictions based on the values for which the model is built. In other words, for this example we only want to use values that fall between 16.9 and 79.1. It is not bad to expand your values a little bit, but the further you get away from these values, the less reliable your prediction will become.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#extrapolation",
    "href": "Residuals_Outliers_Predictions.html#extrapolation",
    "title": "Residuals, Outliers and Predictions",
    "section": "Extrapolation",
    "text": "Extrapolation\nFor example, could we make a prediction on the dew point if the temperature were 300 degrees? Absolutely. All we are doing is evaluating our model when the temperature is 300: \n\n\\(\\hat{y} \\, = \\, (1.107)*(300) - 18.03 = 332.1 - 18.03 = 314.07\\)\n\n\nAs an exercise, this is certainly as answer. Unfortunately, it is not very reliable or even useful. When someone is trying to make predictions outside of the bounds of the model, this is called extrapolation. When someone does this, feel free to discount the results presented. Could they be correct? Maybe, but highly unlikely. \n\n\n\n\n\n\nImportant\n\n\n\nExtrapolation produces predictions that are unreliable. Avoid this practice at all times.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#putting-it-all-together",
    "href": "Residuals_Outliers_Predictions.html#putting-it-all-together",
    "title": "Residuals, Outliers and Predictions",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nLet’s walk through an example where we will go through these steps one at a time and then put them all together. \n\nCreate a Scatterplot with Linear Model:\n\nUse ggplot2 to create a scatterplot of the two variables.\nAdd a linear model to the scatterplot using geom_smooth with the lm method.\n\nDetermine Possible Outliers or Influential Observations:\n\nIdentify any potential outliers or influential points from the scatterplot.\n\nCalculate Residuals:\n\nFit a linear model using lm().\nExtract the residuals from the model.\n\nMake a Residual Plot:\n\nCreate a residual plot using ggplot2.\n\nMake Predictions Using the Linear Model:\n\nUse the predict() function to make three predictions.\nEnsure that one of the values used for prediction is an example of extrapolation (a value outside the range of the original data).\n\n\nTask: Determine if there is a linear relationship between speed and dist in the cars dataset.\nWe will first load up the data set and take a look at the first few rows:\n\n# Load ggplot2 and cars dataset\nlibrary(ggplot2)\ndata(cars)\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\nmin(cars$speed)\n\n[1] 4\n\nmax(cars$speed)\n\n[1] 25\n\nmin(cars$dist)\n\n[1] 2\n\nmax(cars$dist)\n\n[1] 120\n\n\nFrom the output of head(cars), we can see that the cars dataset contains two variables: speed and dist.\nLet’s make sure we have an outlier in the data set. We will add one using the rbind( ) function to add a row to the data frame. We will want to add a point that is far from the rest of the data. The speed has values that range between 4 - 25 and the stopping distance has values that range between 2 - 120.\nLet’s add a point with a speed of 10 and a stopping distance of 200.\n\nrbind( ) is a function that is used to combine two data frames by row.\ndata.frame( ) is a function that creates a data frame, so data.frame(speed = 10, dist = 200) creates a data frame with one row and two columns (speed and dist).\nWe will combine these and save the result in a new data frame called cars_with_outlier. \n\n\n# Add an outlier to the dataset\ncars_with_outlier &lt;- rbind(cars, data.frame(speed = 10, dist = 200))\n\nNow, we can proceed with the analysis:\n\n# Scatterplot with linear model\nggplot(cars_with_outlier, aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Speed vs Stopping Distance with Linear Model\", x = \"Speed\", y = \"Stopping Distance\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom the scatterplot, there is a point that is a clear candidate to be an outlier. \nAt this point we would need to question if we should continue with the analysis or remove the outlier. For the sake of this assignment, we will continue with the analysis.’\nWe can create the linear model and store it in a variable called model_cars. This will allow us to easily see the slope and intercept for the linear model.\n\n# Fit linear model\nmodel_cars &lt;- lm(dist ~ speed, data = cars_with_outlier)\n\nmodel_cars\n\n\nCall:\nlm(formula = dist ~ speed, data = cars_with_outlier)\n\nCoefficients:\n(Intercept)        speed  \n     -3.764        3.258  \n\n\nNow that we have the model, we are ready to calculate the residuals and make a residual plot. \nWe will calculate the residuals and store them in a variable called residuals_cars.\n\n# Calculate residuals\nresiduals_cars &lt;- resid(model_cars)\n\nresiduals_cars\n\n          1           2           3           4           5           6 \n -7.2664872   0.7335128 -15.0394515   2.9605485  -6.2971063 -15.5547611 \n          7           8           9          10          11          12 \n-10.8124159  -2.8124159   5.1875841 -15.0700707  -4.0700707 -21.3277254 \n         13          14          15          16          17          18 \n-15.3277254 -11.3277254  -7.3277254 -12.5853802  -4.5853802  -4.5853802 \n         19          20          21          22          23          24 \n  7.4146198 -15.8430350  -5.8430350  18.1569650  38.1569650 -25.1006898 \n         25          26          27          28          29          30 \n-19.1006898   8.8993102 -16.3583445  -8.3583445 -19.6159993 -11.6159993 \n         31          32          33          34          35          36 \n -1.6159993 -12.8736541   1.1263459  21.1263459  29.1263459 -22.1313089 \n         37          38          39          40          41          42 \n-12.1313089   9.8686911 -29.3889637 -13.3889637  -9.3889637  -5.3889637 \n         43          44          45          46          47          48 \n  2.6110363  -1.9042732 -17.1619280  -4.4195828  17.5804172  18.5804172 \n         49          50          51 \n 45.5804172   7.3227624 171.1875841 \n\n\nWe can now create the residual plot using ggplot( ) where we will keep the same independent variable speed and use the residuals_cars as the dependent variable. We will add a horizontal dashed line at 0 to help us see which points are above and below the regression line. \n\n# Residual plot\nggplot(cars_with_outlier, aes(x = speed, y = residuals_cars)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color=\"red\") +\n  labs(title = \"Residual Plot of Speed vs Stopping Distance\", x = \"Speed\", y = \"Residuals\")\n\n\n\n\n\n\n\n\nNotice that we have a little more evidence that the point we added is an outlier! \nFinally, we can use the linear model to make predictions. We will use the predict() function to make three predictions: one for a speed of 10, one for a speed of 15, and one for a speed of 50 (which is an example of extrapolation). \nIf you recall, we need to use a data frame to make the predictions. We will create a data frame with the speed values we want to predict. \n\n# Make predictions\npredictions &lt;- predict(model_cars, newdata = data.frame(speed = c(10, 15, 50))) # 50 is extrapolated\npredictions\n\n        1         2         3 \n 28.81242  45.10069 159.11861 \n\n\nWe could interpret the output as follows :\n\nFor a speed of 10, the predicted stopping distance is approximately 4.5.\nFor a speed of 15, the predicted stopping distance is approximately 6.5.\nFor a speed of 50 (extrapolation), the predicted stopping distance is approximately 120.5.\n\nThis concludes the analysis of the linear relationship between speed and dist in the cars dataset.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Residuals_Outliers_Predictions.html#exercises",
    "href": "Residuals_Outliers_Predictions.html#exercises",
    "title": "Residuals, Outliers and Predictions",
    "section": "Exercises",
    "text": "Exercises\nIn this assignment, you will be told which data set and two different quantitative variables to analyse. The data sets are already built into R so you don’t have to download any of the data. You will create scatterplots with linear models using ggplot2, determine any possible outliers or influential observations, calculate the residuals, make a residual plot, and use the linear model to make three predictions, where one of the values is an example of extrapolation.\nSteps\n\nCreate a Scatterplot with Linear Model:\n\nUse ggplot2 to create a scatterplot of the two variables.\nAdd a linear model to the scatterplot using geom_smooth with the lm method.\n\nDetermine Possible Outliers or Influential Observations:\n\nIdentify any potential outliers or influential points from the scatterplot.\n\nCalculate Residuals:\n\nFit a linear model using lm().\nExtract the residuals from the model.\n\nMake a Residual Plot:\n\nCreate a residual plot using ggplot2.\n\nMake Predictions Using the Linear Model:\n\nUse the predict() function to make three predictions.\nEnsure that one of the values used for prediction is an example of extrapolation (a value outside the range of the original data).\n\n\n\nProblem 1: Iris Dataset - Sepal Length vs Petal Length\n\n# Load ggplot2 and iris dataset\nlibrary(ggplot2)\ndata(iris)\n\n# Scatterplot with linear model\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Sepal Length vs Petal Length with Linear Model\", x = \"Sepal Length\", y = \"Petal Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_iris &lt;- lm(Petal.Length ~ Sepal.Length, data = iris)\n\n# Calculate residuals\nresiduals_iris &lt;- resid(model_iris)\n\n# Residual plot\nggplot(iris, aes(x = Sepal.Length, y = residuals_iris)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Sepal Length vs Petal Length\", x = \"Sepal Length\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_iris, newdata = data.frame(Sepal.Length = c(5, 6, 10))) # 10 is extrapolated\npredictions\n\n        1         2         3 \n 2.190722  4.049154 11.482886 \n\n\n\n\nProblem 2: Airquality Dataset - Ozone vs Wind\n\n# Load ggplot2 and airquality dataset\nlibrary(ggplot2)\ndata(airquality)\n\n# Remove NA values\nairquality_clean &lt;- na.omit(airquality)\n\n# Scatterplot with linear model\nggplot(airquality_clean, aes(x = Ozone, y = Wind)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Ozone vs Wind with Linear Model\", x = \"Ozone\", y = \"Wind\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_airquality &lt;- lm(Wind ~ Ozone, data = airquality_clean)\n\n# Calculate residuals\nresiduals_airquality &lt;- resid(model_airquality)\n\n# Residual plot\nggplot(airquality_clean, aes(x = Ozone, y = residuals_airquality)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Ozone vs Wind\", x = \"Ozone\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_airquality, newdata = data.frame(Ozone = c(50, 100, 200))) # 200 is extrapolated\npredictions\n\n         1          2          3 \n 9.4222467  6.1479811 -0.4005501 \n\n\n\n\nProblem 3: ToothGrowth Dataset - Dose vs Length\n\n# Load ggplot2 and ToothGrowth dataset\nlibrary(ggplot2)\ndata(ToothGrowth)\n\n# Scatterplot with linear model\nggplot(ToothGrowth, aes(x = dose, y = len)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Dose vs Length with Linear Model\", x = \"Dose\", y = \"Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_toothgrowth &lt;- lm(len ~ dose, data = ToothGrowth)\n\n# Calculate residuals\nresiduals_toothgrowth &lt;- resid(model_toothgrowth)\n\n# Residual plot\nggplot(ToothGrowth, aes(x = dose, y = residuals_toothgrowth)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Dose vs Length\", x = \"Dose\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_toothgrowth, newdata = data.frame(dose = c(0.5, 1, 3))) # 3 is extrapolated\npredictions\n\n       1        2        3 \n12.30429 17.18607 36.71321 \n\n\n\n\nProblem 4: Cars Dataset - Speed vs Stopping Distance\n\n# Load ggplot2 and cars dataset\nlibrary(ggplot2)\ndata(cars)\n\n# Scatterplot with linear model\nggplot(cars, aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Speed vs Stopping Distance with Linear Model\", x = \"Speed\", y = \"Stopping Distance\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_cars &lt;- lm(dist ~ speed, data = cars)\n\n# Calculate residuals\nresiduals_cars &lt;- resid(model_cars)\n\n# Residual plot\nggplot(cars, aes(x = speed, y = residuals_cars)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Speed vs Stopping Distance\", x = \"Speed\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_cars, newdata = data.frame(speed = c(10, 15, 30))) # 30 is extrapolated\npredictions\n\n        1         2         3 \n 21.74499  41.40704 100.39317 \n\n\n\n\nProblem 5: Faithful Dataset - Eruptions vs Waiting Time\n\n# Load ggplot2 and faithful dataset\nlibrary(ggplot2)\ndata(faithful)\n\n# Scatterplot with linear model\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Eruptions vs Waiting Time with Linear Model\", x = \"Eruptions\", y = \"Waiting Time\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_faithful &lt;- lm(waiting ~ eruptions, data = faithful)\n\n# Calculate residuals\nresiduals_faithful &lt;- resid(model_faithful)\n\n# Residual plot\nggplot(faithful, aes(x = eruptions, y = residuals_faithful)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Eruptions vs Waiting Time\", x = \"Eruptions\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_faithful, newdata = data.frame(eruptions = c(2, 3, 5))) # 5 is extrapolated\npredictions\n\n       1        2        3 \n54.93368 65.66332 87.12260 \n\n\n\n\nProblem 6: ChickWeight Dataset - Time vs Weight\n\n# Load ggplot2 and ChickWeight dataset\nlibrary(ggplot2)\ndata(ChickWeight)\n\n# Scatterplot with linear model\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Time vs Weight with Linear Model\", x = \"Time\", y = \"Weight\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_chickweight &lt;- lm(weight ~ Time, data = ChickWeight)\n\n# Calculate residuals\nresiduals_chickweight &lt;- resid(model_chickweight)\n\n# Residual plot\nggplot(ChickWeight, aes(x = Time, y = residuals_chickweight)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Time vs Weight\", x = \"Time\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_chickweight, newdata = data.frame(Time = c(10, 20, 50))) # 50 is extrapolated\npredictions\n\n       1        2        3 \n115.4978 203.5282 467.6194 \n\n\n\n\nProblem 7: Pressure Dataset - Temperature vs Pressure\n\n# Load ggplot2 and pressure dataset\nlibrary(ggplot2)\ndata(pressure)\n\n# Scatterplot with linear model\nggplot(pressure, aes(x = temperature, y = pressure)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Temperature vs Pressure with Linear Model\", x = \"Temperature\", y = \"Pressure\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_pressure &lt;- lm(pressure ~ temperature, data = pressure)\n\n# Calculate residuals\nresiduals_pressure &lt;- resid(model_pressure)\n\n# Residual plot\nggplot(pressure, aes(x = temperature, y = residuals_pressure)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Temperature vs Pressure\", x = \"Temperature\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_pressure, newdata = data.frame(temperature = c(0, 100, 500))) # 500 is extrapolated\npredictions\n\n          1           2           3 \n-147.898873    3.343115  608.311066 \n\n\n\n\nProblem 8: Trees Dataset - Girth vs Volume\n\n# Load ggplot2 and trees dataset\nlibrary(ggplot2)\ndata(trees)\n\n# Scatterplot with linear model\nggplot(trees, aes(x = Girth, y = Volume)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Girth vs Volume with Linear Model\", x = \"Girth\", y = \"Volume\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_trees &lt;- lm(Volume ~ Girth, data = trees)\n\n# Calculate residuals\nresiduals_trees &lt;- resid(model_trees)\n\n# Residual plot\nggplot(trees, aes(x = Girth, y = residuals_trees)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Girth vs Volume\", x = \"Girth\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_trees, newdata = data.frame(Girth = c(8, 12, 20))) # 20 is extrapolated\npredictions\n\n        1         2         3 \n 3.583392 23.846818 64.373669 \n\n\n\n\nProblem 9: USArrests Dataset - Urban Population vs Assaults\n\n# Load ggplot2 and USArrests dataset\nlibrary(ggplot2)\ndata(USArrests)\n\n# Scatterplot with linear model\nggplot(USArrests, aes(x = UrbanPop, y = Assault)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Urban Population vs Assaults with Linear Model\", x = \"Urban Population\", y = \"Assaults\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_usarrests &lt;- lm(Assault ~ UrbanPop, data = USArrests)\n\n# Calculate residuals\nresiduals_usarrests &lt;- resid(model_usarrests)\n\n# Residual plot\nggplot(USArrests, aes(x = UrbanPop, y = residuals_usarrests)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Urban Population vs Assaults\", x = \"Urban Population\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_usarrests, newdata = data.frame(UrbanPop = c(50, 80, 100))) # 100 is extrapolated\npredictions\n\n       1        2        3 \n147.5986 192.3118 222.1206 \n\n\n\n\nProblem 10: Swiss Dataset - Education vs Fertility\n\n# Load ggplot2 and swiss dataset\nlibrary(ggplot2)\ndata(swiss)\n\n# Scatterplot with linear model\nggplot(swiss, aes(x = Education, y = Fertility)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatterplot of Education vs Fertility with Linear Model\", x = \"Education\", y = \"Fertility\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Fit linear model\nmodel_swiss &lt;- lm(Fertility ~ Education, data = swiss)\n\n# Calculate residuals\nresiduals_swiss &lt;- resid(model_swiss)\n\n# Residual plot\nggplot(swiss, aes(x = Education, y = residuals_swiss)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residual Plot of Education vs Fertility\", x = \"Education\", y = \"Residuals\")\n\n\n\n\n\n\n\n# Make predictions\npredictions &lt;- predict(model_swiss, newdata = data.frame(Education = c(10, 20, 50))) # 50 is extrapolated\npredictions\n\n       1        2        3 \n70.98656 62.36305 36.49254",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Residuals, Outliers and Predictions"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html",
    "href": "Advanced_Scatterplot_Techniques.html",
    "title": "Advanced Scatterplot Techniques",
    "section": "",
    "text": "Aesthetics\nAesthetics basically describe what you are going to see on the graph. The very first aesthetic you will need to use is when you initially call ggplot :\nThe first argument you will give ggplot is the data set you want to use. The aesthetics in this command label which variable will be used on the x-axis and which will be used on the y-axis. Any aesthetics that are listed at this level will be used as the default for the rest of the visualization.\nFor this section we will be creating many different examples using the mtcars built in data set. Here is a quick summary of the data.\n# Take a quick peek at the data set mtcars\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nWe can use the help file to give us a better understanding of the data set :\nAfter reading the description of the variables, suppose we are interested in the relationship between the weight (wt) of the car and the miles per gallon (mpg) for the car. The traditional concept is that if a car is heavier it will need more gas to move the car, so heavier cars will use more gas giving them a lower mpg.\nWe can investigate this with a quick scatterplot. We are saying that the weight is affecting the miles per gallon so we will use wt as the explanatory variable (x-axis) and mpg as the response variable (y-axis). Here is the basic command in R:\nlibrary(tidyverse)\n\nggplot( mtcars, aes(x=wt, y=mpg))\nSo far we have set up the canvas for the plot. For this canvas layer, when we add conditions to the `aes( )`` portion, we are giving instructions for all subsequent layers to use. These aesthetics are used as defaults for each subsequent layer. Usually this layer will give the explanatory and response variable. There are some other options that you can explore but we will just use this for now.\nThe next step is to add some type of geometry to show the data. There are several types of geometries one could add to the visualization.\nThere are several more that you can check out on the cheatsheet or through other sources.\nFor this example, let’s just add some basic points to the graph.\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point()\nThere could be instances where you need to adjust the values of the variables. For instance, what if we wanted to transform these data points to the metric system? We would need to change miles to kilometers by multiplying the miles by 1.60934 and change the weight to kilograms by multiplying the weight by 0.453.\nggplot(mtcars, aes(x=wt*1.60934, y=mpg*.0453)) +\n  geom_point()\nWe obviously need to work on the labels for the previous graph! You can see how to do this in previous sections and examples.\nWe are now ready to add some other aesthetics to the graph to pretty this up!",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#aesthetics",
    "href": "Advanced_Scatterplot_Techniques.html#aesthetics",
    "title": "Advanced Scatterplot Techniques",
    "section": "",
    "text": "ggplot( Dataset, aes(x, y))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeom_point() for scatterplots, dot plots, etc\ngeom_line() for adding a line to the plot\ngeom_histogram() for adding a histogram",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#geom_point",
    "href": "Advanced_Scatterplot_Techniques.html#geom_point",
    "title": "Advanced Scatterplot Techniques",
    "section": "geom_point( )",
    "text": "geom_point( )\nThe first geometry we will discuss is the one we used above. What we want to do is change the presentation of the points on the scatterplot. There are several modifications that we can make. Here are the aesthetics we can use for geom_point( ):\n\nx\ny\ncolour\nshape\nsize\nstroke\nfill\nalpha (transparency)\ngroup \n\n(More information can be found here.) \nThere are too many ways to show you an exhaustive list on the different types and combinations you could make with your visualization. Here you will be presented with many different examples showing a few of the ways these aesthetics could be used. \nWe have already discussed some of the coloring options, so let’s talk about some of the different shapes that could be used. \n\n\n\n\n\n# Example where we change the shape, size, and color of the points\n\n# The default size is 0.5 (diameter)\n\nggplot(mtcars, aes(wt, mpg)) + \n  geom_point(shape=18, size=5, color=\"blue\")\n\n\n\n\n\n\n\n\n\nLook at the chart above showing the different shapes and consider shape 23. This looks like it is a blue diamond, but it represents a diamond filled with a color. If we wished, we could change the color of the diamond. \n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point( shape=23, fill=\"green\", size=6)\n\n\n\n\n\n\n\n\n Notice that the diamond is has a black outline. We could change the color of the outline as well as how thick that line is. This outline is called the stroke. You can think of it is how wide is the stroke of the pen that is making the shapes. \n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point( shape=23, fill=\"green\", color=\"darkred\", stroke=3, size=6)\n\n\n\n\n\n\n\n\nAdmittedly, not the best looking color scheme. \nWe can also see that some of the points are overlapping each other. That can sometimes be confusing. As we saw in a previous sectino we can make the points more transparent. This is called chaning the alpha level of the points. The alpha level needs to be between 0 and 1 where 0 is completely transparent and 1 is no transparency. \n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point( shape=23, fill=\"green\", color=\"darkred\", stroke=3, \n              alpha = 0.4, size=6)\n\n\n\n\n\n\n\n\nThe way you can read this is that the darker portions of the graph have more overlap than the lighter portions. \nWe could also group the points together based on another variable. For instance, the number of cylinders a car reflects the size of the engine. So a car with more cylinders (cyl) usually weights more than a car with less cylinders. We can change the color of the points using the number of cylinders as our grouping. We will make this grouping a default for the entire graph, so that means we will add the new condition to the original ggplot( ) call.\n\n# For this type of example, we want the cylinders to be used as factors to \n# determine the different shapes. They will be automatically given a shape and\n# a key will appear to show you the mapping.\n\nggplot(mtcars, aes(x=wt, y=mpg, group=cyl)) +\n  geom_point(aes(shape=factor(cyl)))\n\n\n\n\n\n\n\n\n\nIn order for this to stand out, we could also change the color of each group: \n\n# Add color as a factor for the cylinders.\n\nggplot(mtcars, aes(x=wt, y=mpg, group=cyl)) +\n  geom_point(aes(shape=factor(cyl), color=factor(cyl)))\n\n\n\n\n\n\n\n\n\nIf we are feeling really sassy, we could change the size based on the number of cylinders that are in a car.\n\nggplot(mtcars, aes(x=wt, y=mpg, group=cyl)) +\n  geom_point(aes(shape=factor(cyl),color=factor(cyl), size=factor(cyl)))\n\n\n\n\n\n\n\n\n\nWe could also change these values manually if we do not like the defaults. We can use the following functions:\n\nscale_shape_manual( ) - this changes the shapes\nscale_color_manual( ) - this changes the colors\nscale_size_manual( ) - this changes the size \n\nIn this example, we are working with the cylinder size. There are three values for this variable. If we want to change these manually, make sure that you have a value for each of these three factor levels. \nWe will create a vector describing which values we want to use. For example, if I want to use the shapes 3, 16, and 17 from the chart above, I would create a vector with this three variables and use it in the scale_shape_manual( ) command: \n\nscale_shape_manual(values=c(3, 16, 17))\n\n\nWe would do something similar for other choices.\n\n\n# We can also change where the legend is positioned in our visualization\n\nggplot(mtcars, aes(x=wt, y=mpg, group=cyl)) +\n  geom_point(aes(shape=factor(cyl),color=factor(cyl), size=2))+\n  scale_shape_manual(values=c(3, 16, 17)) +\n  scale_color_manual(values=c('#999999', '#E69F00', '#56B4E9')) +\n  theme(legend.position=\"top\")",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#faceting",
    "href": "Advanced_Scatterplot_Techniques.html#faceting",
    "title": "Advanced Scatterplot Techniques",
    "section": "Faceting",
    "text": "Faceting\nThere are times when we have a large data set and plotting all of the points together is not a useful visualization. We could create a single graphic one variable at a time, or we could create what is called a facet. A facet is the process that will take a single plot and break it up into smaller subplots. This will create a grid with each of the smaller plots for easy comparison between the variables. \nTo help with this, let’s use the gapminder.csv data set. This data set can be found on our Github repository. Here is a brief description of the data set. \n\n\n\n\n\n# Make sure we have the libary loaded that uses read_csv\n\nlibrary(readr)\n\n# Read the data set into \"gapdata\"\n\ngapdata &lt;- read_csv(\"./gapminder.csv\")\n\n# Look at the first few lines of the data\n\nhead(gapdata)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\n\nLet’s take a quick look at the relationship between x = gpdPercap and y = lifeExp. \n\nggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIf we were to describe this scatterplot we would say\n\nform : nonlinear (logarithmic)\ndirection : positive\nstrength : fairly strong \n\nThere is definitely a pattern here, so we would say the strength is strong, but we could also note that the linear strength would be weak. The pattern is nonlinear and looks to follow an logarithmic pattern and the association has a positive direction. \nIf we wanted to consider the data set by the continent, we could do what we just learned about to color the points by their continent. \n\n  ggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n  geom_point(aes(color=continent))\n\n\n\n\n\n\n\n\n\nWhile the colors look nice, this is just a jumbled mess. What we could do is change how we view this graph. Since this shape is logarithmic, we should be able to use the scale_x_log10( ) option. If the shape is logarithmic, this command transforms the points into a plt that should look more linear.\n\n  ggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n  geom_point(aes(color=continent)) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThis makes the graph a little easier to read, but it is still a jumbled mess. What we need to do is create a facet so we can see the subplots of each continent individually. The command facet_wrap( ) tries to sensibly wrap a series of facets using smaller subplots. We can give it the directions on how to set up the facet. Fos this example, we want a facet by continent so we will add : \n\n`facet_wrap(~continent)\n\n\n\n ggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n  geom_point() +\n  scale_x_log10() +\n  facet_wrap(~continent)\n\n\n\n\n\n\n\n\n\nWe can also clean this up using the optimization techniques we saw earlier. For instance, if we wanted to have one column of facets with colors :\n\nggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n  geom_point(aes(colour=continent)) +\n  scale_x_log10() +\n  facet_wrap(~continent, ncol=1) \n\n\n\n\n\n\n\n\n\nUsing this visualization, it is now much easier to compare the continents. You can see that the population on Oceania has much less variation in their GDP Per Capita and life expectancy when compared to Asia. By taking the plot and creating these facets, it was mich easier to interpret the data and find a better story the data is trying to tell us.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#saving-scatterplots",
    "href": "Advanced_Scatterplot_Techniques.html#saving-scatterplots",
    "title": "Advanced Scatterplot Techniques",
    "section": "Saving Scatterplots",
    "text": "Saving Scatterplots\nAfter you have been working with the data and options to create the perfect scatterplot, you awnt to save it so you can use it anytime you want. You can save the plot to some file format on your computer. You could then upload the graph to a Google Doc, Word file, etc.. You can also save the size, set the width or height, and more. \nYou can read more about `ggsave() here. \nLet’s save the previous plot as a PDF file. \n\n# We need to recreate the plot and save it to a variable. Let's call it \"pfinal\"\n\npfinal &lt;- ggplot(gapdata, aes(x=gdpPercap, y=lifeExp)) +\n          geom_point(aes(colour=continent)) +\n          scale_x_log10() +\n          facet_wrap(~continent, ncol=1) \n\n# We can now save this to our computer. Let's call the file saved on the \n# computer \"myplot.pdf\"\n\nggsave(pfinal, file=\"myplot.pdf\", width=5, height=15)\n\n\nThis is now in my working directory : \n\n\n\n\nAnd here is my new PDF :",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#conclusion",
    "href": "Advanced_Scatterplot_Techniques.html#conclusion",
    "title": "Advanced Scatterplot Techniques",
    "section": "Conclusion",
    "text": "Conclusion\nWhile we have gone over a lot of ways to customize a scatterplot, we really have barely scratched the surface for all we can do. Feel free to play around with other options and other geometries to see what else you can create. \nOne final note is to make sure that the substance of the visualization is more important than the style of the visualization. You don’t want you picture to be so flashy that the story you are trying to tell gets lost. These customization be very beneficial, just don’t over do it!",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Advanced_Scatterplot_Techniques.html#exercises",
    "href": "Advanced_Scatterplot_Techniques.html#exercises",
    "title": "Advanced Scatterplot Techniques",
    "section": "Exercises",
    "text": "Exercises\n\nProblem 1: Custom Colors and Shapes\nUse the iris dataset to create a scatterplot of Sepal.Length vs. Sepal.Width. Change the color of the points based on Species and use different shapes for each species.\n\n# Solution\nlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species, shape = Species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nProblem 2: Size and Alpha Levels\nUse the iris dataset to create a scatterplot of Sepal.Length vs. Petal.Length. Change the size of the points based on Petal.Width and set the alpha level to 0.6.\n\n# Solution\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, size = Petal.Width)) +\n  geom_point(alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\nProblem 3: Fill and Stroke\nUse the iris dataset to create a scatterplot of Petal.Length vs. Petal.Width. Change the fill color of the points based on Species and add a black border (stroke) to the points.\n\n# Solution\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) +\n  geom_point(shape = 21, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nProblem 4: Faceted Scatterplot (Single Variable)\nUse the iris dataset to create a scatterplot of Sepal.Length vs. Sepal.Width. Facet the plot by Species.\n\n# Solution\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  facet_wrap(~ Species)\n\n\n\n\n\n\n\n\n\n\nProblem 5: Faceted Scatterplot (Multiple Variables)\nUse the iris dataset to create a scatterplot of Petal.Length vs. Petal.Width. Facet the plot by Species and include a different color for each species.\n\n# Solution\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~ Species)\n\n\n\n\n\n\n\n\n\n\nProblem 6: Log Scale\nUse the diamonds dataset to create a scatterplot of carat vs. price. Apply a log scale to the y-axis.\n\n# Solution\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nProblem 7: Custom Colors and Alpha\nUse the diamonds dataset to create a scatterplot of carat vs. price. Change the color of the points based on clarity and set the alpha level to 0.5.\n\n# Solution\nggplot(diamonds, aes(x = carat, y = price, color = clarity)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nProblem 8: Size and Shape\nUse the ToothGrowth dataset to create a scatterplot of len vs. dose. Change the size of the points based on supp and use different shapes for each supplement type.\n\n# Solution\nggplot(ToothGrowth, aes(x = dose, y = len, size = supp, shape = supp)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\nProblem 9: Faceted Scatterplot with Custom Colors\nUse the ToothGrowth dataset to create a scatterplot of len vs. dose. Facet the plot by supp and change the color of the points based on dose.\n\n# Solution\nggplot(ToothGrowth, aes(x = dose, y = len, color = factor(dose))) +\n  geom_point() +\n  facet_wrap(~ supp)\n\n\n\n\n\n\n\n\n\n\nProblem 10: Fill, Size, and Alpha\nUse the ChickWeight dataset to create a scatterplot of Time vs. weight. Change the fill color of the points based on Diet, the size of the points based on Chick, and set the alpha level to 0.7.\n\n# Solution\nggplot(ChickWeight, aes(x = Time, y = weight, fill = factor(Diet), size = Chick)) +\n  geom_point(shape = 21, alpha = 0.7)",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Advanced Scatterplot Techniques"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html",
    "href": "Estimating_Parameters_Confidence_Intervals.html",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "",
    "text": "Central Limit Theorem\nThe theory that is driving the calculation of the margin of error is the Central Limit Theorem. The Central Limit Theorem states that the distribution of the sample mean is approximately normally distributed. Here is a way to wrap your head around this idea. Here is a picture of three completely different distributions, all with the same mean, and none of which look anything remotely like the normal distribution.\nThe first distribution is a uniform distribution. The second distribution is a a bimodal distribution. The third distribution is a positive skewed distribution.\nThe sampling distribution just means that we need to take every possible sample and look at the distribution that results from those samples. In the picture above, a regular distribution is just a sampling distribution with samples of size 1.\nLook what happens when we create a sampling distribution for the three examples above where we are taking samples of size 5.\nIn this scenario, the third distribution is starting to look like a normal distribution. This is because it wasn’t too far off from a normal distribution to begin with. The first two distributions are still not looking like a normal distribution. What happens if we increase the sample size to 30?\nWow! All three of the distributions are starting to look like a normal distribution. This is the Central Limit Theorem in action. The Central Limit Theorem states that the distribution of the sample mean is going to be normally distributed as the size of the samples get larger. Generally, once the sample size hits 30, the distribution of the sample mean is going to be normally distributed. This is going to be true no matter what the original distribution of the population is. The Central Limit Theorem also states that the standard deviation of the sample mean is going to be the standard deviation of the sample divided by the square root of the number of observations in the sample.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#creating-a-confidence-interval",
    "href": "Estimating_Parameters_Confidence_Intervals.html#creating-a-confidence-interval",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "Creating a Confidence Interval",
    "text": "Creating a Confidence Interval\nTherefore we are going to use the statistic and the margin of error to come up with an interval that we will claim has a certain probability of containing the parameter. This interval is called a confidence interval. We are going to construct confidence intervals that are reasonable to use. \nFor example, let’s think about the average test score from a typical college course. Here are three different confidence intervals to consider:\n\nA 100% confidence interval for the average test score is 0 to 100.\nA 5% confidence interval for the average test score is 82 to 85.\nA 95% confidence interval for the average test score is 78 to 85.\n\nThe first confidence interval is not very useful because it is too wide. Has an interval been constructed that will definitely catch the parameter? Absolutely! But it is not very useful because it is too wide. \nThe second confidence interval is too narrow. It is too narrow because it is unlikely that the interval will contain the parameter. This is saying that the confidence interval is 5% likely to contain the parameter. This is not a very useful confidence interval, either. \nThe third confidence interval is a good confidence interval. It is saying that based on the methods used, the interval is 95% likely to contain the parameter. This is a good confidence because we have a high level of confidence and a reasonable interval. \nWhen you are creating a confidence interval, these are the trade offs you need to consider when creating the confidence interval. You do not want the interval to be so wide it is useless or too small so that you don’t have much confidence that you have the parameter. You want to have a reasonable interval that you have a high level of confidence that you have the parameter. \nThe idea for creating a confidence interval is not difficult. You are going to take a sample from the population and calculate the statistic from the sample. You are then going to calculate the margin of error The confidence interval can be constructed as follows : \n\n\\(\\bar{x} \\pm MOE\\)\n\n\nAnd this leads us to the question of how to calculate the margin of error. The margin of error is calculated as follows: \n\n\\(\\displaystyle{MOE = (test\\,statistic) \\times \\frac{standard\\,deviation}{\\sqrt{n}}}\\)",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#test-statistic",
    "href": "Estimating_Parameters_Confidence_Intervals.html#test-statistic",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "Test Statistic",
    "text": "Test Statistic\nThe test statistic is the number that you are going to use to help create the confidence interval. The test statistic is going to be based on the level of confidence that you want. The higher level of confidence will result in a higher test statistic. The information we are given in the problem will dictate to us the type of test statistic that we are going to use. There are 2 types of test statistics that we are going to use to create a confidence interval. They are the z-score and the t-score. They are similar to each other, especially if we have a large sample size. So which one do we use? \nIf the sample size is small, or we do not know the standard deviation of the population, then we are going to use the t-score. If we know the population standard deviation and have a large sample size (\\(n \\geq 30\\)), then we are going to use the z-score. Here is a flow chart to help you decide which test statistic to use: \n\n\n\n\nThe normal distribution and the t-distribution are very similar to each other. The t-distribution is going to be wider than the normal distribution. The t-distribution is going to be wider because it has more variability. All this means is that we are not as certain about the t-distribution as we are about the normal distribution. Thus the probabilities in the tails of the t-distribution are going to be larger than the normal distribution. \n\n\n\n As mentioned above, once the samples sizes get large, there is little difference in the sampling distributions. \nThis means the confidence intervals should take on one of the following two forms:\n\n\\(\\displaystyle{\\bar{x} \\pm (Z\\,score) \\times \\frac{\\sigma}{\\sqrt{n}}}\\)\n\nwhere\n\n\\(Z\\) corresponds to the level of confidence that you want\n\\(\\sigma\\) is the standard deviation of the population\n\\(n\\) is the number of observations in the sample\n\nor\n\n\\(\\displaystyle{\\bar{x} \\pm (t \\, score) \\times \\frac{s_x}{\\sqrt{n}}}\\)\n\nwhere\n\n\\(t\\) corresponds to the level of confidence that you want\n\\(s_x\\) is the standard deviation of the sample\n\\(n\\) is the number of observations in the sample \n\nLet’s discuss how to find the values for these two test statistics.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#z-score",
    "href": "Estimating_Parameters_Confidence_Intervals.html#z-score",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "Z-Score",
    "text": "Z-Score\nThe first test statistic that we are going to use is the z-score. The z-score comes from the normal distribution and is the number of standard deviations that you are away from the mean. The z-score is going to be used when we know the standard deviation of the population and when we have a large sample size. \n\nEmpirical Rule\nA z-score is the number of standard deviations that you are away from the mean. This also shows us the percentage of the distribution that is within the z-score. The Empirical Rule gives us a few of the more useful z-scores: \n\n\n\n\nBased on the Empirical Rule, we can see that 95% of the distribution is within almost 2 standard deviations of the mean. This means that if we want to have a 95% confidence interval, we are going to use a z-score of 1.96. If we want to have a 90% confidence interval, we are going to use a z-score of 1.645. If we want to have a 99% confidence interval, we are going to use a z-score of 2.58. \n\n\n\n\nIf we wanted to construct a 95% confidence interval, we would use the following formula to create it :\n\n\\(\\displaystyle{\\bar{x} \\pm (1.96) \\times \\frac{\\sigma}{\\sqrt{n}}}\\)\n\nIf we wanted to construct a 90% confidence interval, we would use the following formula to create it : \n\n\\(\\displaystyle{\\bar{x} \\pm (1.645) \\times \\frac{\\sigma}{\\sqrt{n}}}\\)\n\n\nIf we wanted to construct a 99% confidence interval, we would use the following formula to create it : \n\n\\(\\displaystyle{\\bar{x} \\pm (2.58) \\times \\frac{\\sigma}{\\sqrt{n}}}\\)\n\n\nWhen we look at the formula, you may notice that there is not much that we are in control of throughout this process. We are not in control of the sample mean or the standard deviation of the sample. What we can control is the number of observations in the sample and the level of confidence that we want. \nWhat if we have created a confidence interval and think it is too narrow, then we can increase the level of confidence. If we created a 90% confidence interval then we could increase the level of confidence to 95% to make the confidence interval wider. We do have to be too cautious about making it too wide. \nWhat if we have created a confidence interval and think it is too wide, then we can increase the number of observations in the sample. If we increase the number of observations in the sample then the standard deviation of the sample is going to decrease. This is going to make the margin of error smaller and the confidence interval narrower. \nConsider this image: \n\n\n\n\nThis image shows the relationship between the number of observations in the sample and the margin of error. The first part shows the length of the confidence interval with a small sample size of \\(n=5\\). The second part shows the length of the confidence interval with a slightly larger sample of sinze \\(10\\). The third part shows the length of the confidence interval with a larger sample size of \\(n=150\\). As the number of observations in the sample increases, the margin of error decreases. This is because the larger sample size is giving us more information so the data is not as varied. While this is a good thing, it is not always possible to increase the number of observations in the sample. This could be a process that takes a lot of time and money. \nThe moral of this story is that you may have to make some trade offs when creating a confidence interval. \n\n\nExample Using Z-Score\nExample : Suppose we collect a population of turtles with the following information :\n\nPopulation size : \\(n=35\\)\nPopulation mean weight : \\(\\bar{x} = 300\\)\nPopulation standard deviation : \\(\\sigma = 18.5\\) \n\nWe want to construct a 95% confidence interval for the average weight of the turtles.\n\n# Sample size\n\nn &lt;- 35\n\n# Sample mean weight\n\nx_bar &lt;- 300\n\n# Sample standard deviation\n\nsigma &lt;- 18.5\n\n# Z-score for 95% confidence interval\n\nz &lt;- 1.96\n\n# Margin of error\n\nMOE &lt;- z * sigma / sqrt(n)\n\nMOE\n\n[1] 6.129059\n\n# Confidence interval\n\nCI &lt;- c(x_bar - MOE, x_bar + MOE)\n\nCI\n\n[1] 293.8709 306.1291\n\n\n\nOur interpretation would be that we are 95% confident that the value of the parameter is between 293.87 and 306.13. \nI hope some questions are popping into your mind right now. For instance, if we have the data for the entire population, why do we need to create a confidence interval? This is an excellent question. One of the reasons is that we could just be working through an example where the population is given to us. In practice, we will not have the data for the entire population. Which means we will almost always be using a t-score to create a confidence interval.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#t-score",
    "href": "Estimating_Parameters_Confidence_Intervals.html#t-score",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "T-Score",
    "text": "T-Score\nThe t-score is going to be used when we do not know the standard deviation of the population or when we have a small sample size. We can use R to calculate the value of a t-score that we need. We do need a little bit of information to calculate the t-score. We need to know the level of confidence that we want and the degrees of freedom. The degrees of freedom is the number of observations in the sample minus 1. This represents the number of independent scores in the data set. Let’s revist the previous example, but with a smaller sample size and the stnadard deviation of the population was unknown so we calculated it from the sample. \n\nSample size : \\(n=25\\)\nSample mean weight : \\(\\bar{x} = 300\\)\nSample standard deviation : \\(s_x = 19.3\\) \n\nWe want to construct a 95% confidence interval for the average weight of the turtles.\n\n# Sample size\n\nn &lt;- 25\n\n# Sample mean weight\n\nx_bar &lt;- 300\n\n# Sample standard deviation\n\ns_x &lt;- 19.3\n\n# Degrees of freedom\n\ndf &lt;- n - 1\n\n# T-score for 95% confidence interval. Use the qt command.\n\nt &lt;- qt(0.975, df)\n\n# Margin of error\n\nMOE &lt;- t * s_x / sqrt(n)\n\nMOE\n\n[1] 7.966648\n\n# Confidence interval\n\nCI &lt;- c(x_bar - MOE, x_bar + MOE)\n\nCI\n\n[1] 292.0334 307.9666\n\n\n\nThe interpretation here is that the confidence interval goes from 292.03 to 307.97. We are 95% confident that the parameter is in this interval. \nLet’s go over the command we used to find the t-score. \n\nt &lt;- qt(0.975, df)\n\nThe qt command is used to find the t-score. The first argument is the level of confidence that we want. In this case we want a 95% confidence interval so we are going to use 0.975. Why do we do this? Consider this image: \n\n\n\n\nThe t-score needed is the mark at the end of the green shaded area, which is the middle 95% of the t-distribution. If you look at the mark though, it covers the first 97.5% of the t-distribution. This is why we use 0.975 as the argument in the qt command. The second argument is the degrees of freedom. The degrees of freedom is the number of observations in the sample minus 1. \nWhen people think about the end areas for the confidence interval, this is measured using the variable \\(\\alpha\\). A 95% confidence interval would have \\(\\alpha\\) represent the other 5%, so \\(\\alpha = 0.05\\). \nThis means that the area in each tail is half of \\(\\alpha\\), or 0.025. This is why we use 0.975 as the argument in the qt command, because \\(1 - 0.025 = 0.975\\).\nWe could have also used the following command to find the t-score:\n\nn &lt;- 25\n\ndf &lt;- n - 1\n\nalpha = 0.05\n\nt &lt;- qt(1 - (alpha/2), df)\n\nIf we wanted a 90% confidence interval, we would use the following command to find the t-score:\n\nt &lt;- qt(0.95, df)\n\n\n\n\n\nAs you can see, the purple region ends after the first 95% of the area of the distribution. This is why we use 0.95 as the argument in the qt command.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#conclusion",
    "href": "Estimating_Parameters_Confidence_Intervals.html#conclusion",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\nThis section has been about constructing confidence intervals. We are trying to estimate a parameter from a population. We know that when we take a sample, the value from the sample is almost sure to be different from the population. So instead of using the singular value from the sample, we create an interval that we feel comfortable that captures the parameter. Because of the randomness of drawing a sample we are not going to definitively know if the parameter is in the interval. However, based on the methods we have discussed in this section, we can be highly confident that the parameter is in the interval.",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Estimating_Parameters_Confidence_Intervals.html#exercises",
    "href": "Estimating_Parameters_Confidence_Intervals.html#exercises",
    "title": "Estimating Parameters Using Confidence Intervals",
    "section": "Exercises",
    "text": "Exercises\nFor the following problems you will need to calculate the following :\n\nThe mean of the sample\nThe standard deviation of the sample\nThe number of observations in the sample\nThe alpha level\nThe test statistic (t-score)\nThe margin of error\nThe confidence interval\n\n\nProblem 1: Estimating the Mean Waiting Time for Old Faithful Eruptions\nUse the faithful dataset to estimate the mean waiting time between eruptions using a 99% confidence interval.\n\n# Solution\ndata(\"faithful\")\n\nwaiting_times &lt;- faithful$waiting\n\nmean_waiting &lt;- mean(waiting_times)\nsd_waiting &lt;- sd(waiting_times)\nn &lt;- length(waiting_times)\nalpha_level &lt;- 0.01\nt_score &lt;- qt(1 - (alpha/2), df = n - 1)\n\nmargin_of_error &lt;- t_score * (sd_waiting / sqrt(n))\nlower_bound &lt;- mean_waiting - margin_of_error\nupper_bound &lt;- mean_waiting + margin_of_error\n\nlist(mean = mean_waiting, sd = sd_waiting, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 70.89706\n\n$sd\n[1] 13.59497\n\n$t_score\n[1] 1.968756\n\n$alpha_level\n[1] 0.01\n\n$margin_of_error\n[1] 1.622878\n\n$lower_bound\n[1] 69.27418\n\n$upper_bound\n[1] 72.51994\n\n\nResult: The 99% confidence interval for the mean waiting time between eruptions is approximately [69.2741808, 72.5199369].\n\n\nProblem 2: Estimating the Mean Speed of Cars in the cars Dataset\nUse the cars dataset to estimate the mean speed of cars using a 95% confidence interval.\n\n# Solution\ndata(\"cars\")\ncar_speeds &lt;- cars$speed\n\nmean_speed &lt;- mean(car_speeds)\nsd_speed &lt;- sd(car_speeds)\nn &lt;- length(car_speeds)\nalpha_level &lt;- 0.05\nt_score &lt;- qt(1 - (alpha/2), df = n - 1)\n\nmargin_of_error &lt;- t_score * (sd_speed / sqrt(n))\nlower_bound &lt;- mean_speed - margin_of_error\nupper_bound &lt;- mean_speed + margin_of_error\n\nlist(mean = mean_speed, sd = sd_speed, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 15.4\n\n$sd\n[1] 5.287644\n\n$t_score\n[1] 2.009575\n\n$alpha_level\n[1] 0.05\n\n$margin_of_error\n[1] 1.502732\n\n$lower_bound\n[1] 13.89727\n\n$upper_bound\n[1] 16.90273\n\n\nResult: The 95% confidence interval for the mean speed of cars is approximately [13.8972681, 16.9027319].\n\n\nProblem 3: Estimating the Mean Gas Mileage of Cars in the mtcars Dataset\nUse the mtcars dataset to estimate the mean miles per gallon (mpg) of cars using a 99% confidence interval.\n\n# Solution\ndata(\"mtcars\")\nmpg &lt;- mtcars$mpg\n\nmean_mpg &lt;- mean(mpg)\nsd_mpg &lt;- sd(mpg)\nn &lt;- length(mpg)\nalpha_level &lt;- 0.01\nt_score &lt;- qt(1 - (alpha/2), df = n - 1)\n\nmargin_of_error &lt;- t_score * (sd_mpg / sqrt(n))\nlower_bound &lt;- mean_mpg - margin_of_error\nupper_bound &lt;- mean_mpg + margin_of_error\n\nlist(mean = mean_mpg, sd = sd_mpg, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 20.09062\n\n$sd\n[1] 6.026948\n\n$t_score\n[1] 2.039513\n\n$alpha_level\n[1] 0.01\n\n$margin_of_error\n[1] 2.172946\n\n$lower_bound\n[1] 17.91768\n\n$upper_bound\n[1] 22.26357\n\n\nResult: The 99% confidence interval for the mean miles per gallon of cars is approximately [17.9176785, 22.2635715].\n\n\nProblem 4: Estimating the Mean Height of Trees in the trees Dataset\nUse the trees dataset to estimate the mean height of trees using a 95% confidence interval.\n\n# Solution\ndata(\"trees\")\ntree_heights &lt;- trees$Height\n\nmean_height &lt;- mean(tree_heights)\nsd_height &lt;- sd(tree_heights)\nn &lt;- length(tree_heights)\nalpha_level &lt;- 0.05\nt_score &lt;- qt(0.975, df = n - 1)\n\nmargin_of_error &lt;- t_score * (sd_height / sqrt(n))\nlower_bound &lt;- mean_height - margin_of_error\nupper_bound &lt;- mean_height + margin_of_error\n\nlist(mean = mean_height, sd = sd_height, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 76\n\n$sd\n[1] 6.371813\n\n$t_score\n[1] 2.042272\n\n$alpha_level\n[1] 0.05\n\n$margin_of_error\n[1] 2.3372\n\n$lower_bound\n[1] 73.6628\n\n$upper_bound\n[1] 78.3372\n\n\nResult: The 95% confidence interval for the mean height of trees is approximately [73.6628001, 78.3371999].\n\n\nProblem 5: Estimating the Mean Air Quality in New York\nUse the airquality dataset to estimate the mean ozone level using a 90% confidence interval.\n\n# Solution\ndata(\"airquality\")\nozone_levels &lt;- airquality$Ozone[!is.na(airquality$Ozone)]\n\nmean_ozone &lt;- mean(ozone_levels)\nsd_ozone &lt;- sd(ozone_levels)\nn &lt;- length(ozone_levels)\nt_score &lt;- qt(0.95, df = n - 1)\nalpha_level &lt;- 0.10\nmargin_of_error &lt;- t_score * (sd_ozone / sqrt(n))\nlower_bound &lt;- mean_ozone - margin_of_error\nupper_bound &lt;- mean_ozone + margin_of_error\n\nlist(mean = mean_ozone, sd = sd_ozone, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 42.12931\n\n$sd\n[1] 32.98788\n\n$t_score\n[1] 1.658212\n\n$alpha_level\n[1] 0.1\n\n$margin_of_error\n[1] 5.078851\n\n$lower_bound\n[1] 37.05046\n\n$upper_bound\n[1] 47.20816\n\n\nResult: The 90% confidence interval for the mean ozone level is approximately [37.0504593, 47.2081614].\n\n\nProblem 6: Estimating the Mean Depth of Quakes in Fiji\nUse the quakes dataset to estimate the mean depth of earthquakes using a 95% confidence interval.\n\n# Solution\ndata(\"quakes\")\nquake_depths &lt;- quakes$depth\n\nmean_depth &lt;- mean(quake_depths)\nsd_depth &lt;- sd(quake_depths)\nn &lt;- length(quake_depths)\nt_score &lt;- qt(0.975, df = n - 1)\nalpha_level &lt;- 0.05\nmargin_of_error &lt;- t_score * (sd_depth / sqrt(n))\nlower_bound &lt;- mean_depth - margin_of_error\nupper_bound &lt;- mean_depth + margin_of_error\n\nlist(mean = mean_depth, sd = sd_depth, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 311.371\n\n$sd\n[1] 215.5355\n\n$t_score\n[1] 1.962341\n\n$alpha_level\n[1] 0.05\n\n$margin_of_error\n[1] 13.37499\n\n$lower_bound\n[1] 297.996\n\n$upper_bound\n[1] 324.746\n\n\nResult: The 95% confidence interval for the mean depth of earthquakes is approximately [297.9960124, 324.7459876].\n\n\nProblem 7: Estimating the Mean Monthly Sunspots\nUse the sunspot.month dataset to estimate the mean number of monthly sunspots using a 99% confidence interval.\n\n# Solution\ndata(\"sunspot.month\")\nmonthly_sunspots &lt;- sunspot.month\n\nmean_sunspots &lt;- mean(monthly_sunspots)\nsd_sunspots &lt;- sd(monthly_sunspots)\nn &lt;- length(monthly_sunspots)\nalpha_level &lt;- 0.01\nt_score &lt;- qt(0.995, df = n - 1)\n\nmargin_of_error &lt;- t_score * (sd_sunspots / sqrt(n))\nlower_bound &lt;- mean_sunspots - margin_of_error\nupper_bound &lt;- mean_sunspots + margin_of_error\n\nlist(mean = mean_sunspots, sd = sd_sunspots, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 51.96481\n\n$sd\n[1] 44.12524\n\n$t_score\n[1] 2.577378\n\n$alpha_level\n[1] 0.01\n\n$margin_of_error\n[1] 2.0177\n\n$lower_bound\n[1] 49.94711\n\n$upper_bound\n[1] 53.98251\n\n\nResult: The 99% confidence interval for the mean number of monthly sunspots is approximately [49.9471096, 53.9825096].\n\n\nProblem 8: Estimating the Mean Pressure in the pressure Dataset\nUse the pressure dataset to estimate the mean pressure using a 95% confidence interval.\n\n# Solution\ndata(\"pressure\")\npressure_values &lt;- pressure$pressure\n\nmean_pressure &lt;- mean(pressure_values)\nsd_pressure &lt;- sd(pressure_values)\nn &lt;- length(pressure_values)\nt_score &lt;- qt(0.975, df = n - 1)\nalpha_level &lt;- 0.05\nmargin_of_error &lt;- t_score * (sd_pressure / sqrt(n))\nlower_bound &lt;- mean_pressure - margin_of_error\nupper_bound &lt;- mean_pressure + margin_of_error\n\nlist(mean = mean_pressure, sd = sd_pressure, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 124.3367\n\n$sd\n[1] 224.6225\n\n$t_score\n[1] 2.100922\n\n$alpha_level\n[1] 0.05\n\n$margin_of_error\n[1] 108.2646\n\n$lower_bound\n[1] 16.07211\n\n$upper_bound\n[1] 232.6013\n\n\nResult: The 95% confidence interval for the mean pressure is approximately [16.0721066, 232.6013039].\n\n\nProblem 9: Estimating the Mean Weight of Chickens\nUse the ChickWeight dataset to estimate the mean weight of chickens at Time 20 using a 95% confidence interval.\n(Hint : Look at the set up of ChickWeight using ?Chickweight. There are 4 variables, one of which is “Time”. We want the variable Chickweight$Time when Time = 20. Filter the dataset to get the weights at Time 20)\n\n# Solution\n\n# ?ChickWeight\n\ndata(\"ChickWeight\")\n\n# chick_weights &lt;- filter(ChickWeight, Time == 20)$weight\n\nchick_weights &lt;- ChickWeight$weight[ChickWeight$Time == 20]\n\nmean_weight &lt;- mean(chick_weights)\nsd_weight &lt;- sd(chick_weights)\nn &lt;- length(chick_weights)\nt_score &lt;- qt(0.975, df = n - 1)\nalpha_level &lt;- 0.05\nmargin_of_error &lt;- t_score * (sd_weight / sqrt(n))\nlower_bound &lt;- mean_weight - margin_of_error\nupper_bound &lt;- mean_weight + margin_of_error\n\nlist(mean = mean_weight, sd = sd_weight, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 209.7174\n\n$sd\n[1] 66.51171\n\n$t_score\n[1] 2.014103\n\n$alpha_level\n[1] 0.05\n\n$margin_of_error\n[1] 19.75154\n\n$lower_bound\n[1] 189.9659\n\n$upper_bound\n[1] 229.4689\n\n\nResult: The 95% confidence interval for the mean weight of chickens at Time 20 is approximately [189.9658522, 229.4689304].\n\n\nProblem 10: Estimating the Mean Sepal Length of Versicolor Species\nUse the iris dataset to estimate the mean sepal length of the Versicolor species using a 90% confidence interval.\n(Hint : We want to pull out the Sepal.Length column for the Versicolor species)\n\n# Solution\ndata(\"iris\")\n\n# versicolor_sepal_length &lt;- filter(iris, Species == \"versicolor\")$Sepal.Length\n\nversicolor_sepal_length &lt;- iris$Sepal.Length[iris$Species == \"versicolor\"]\n\nmean_sepal_length &lt;- mean(versicolor_sepal_length)\nsd_sepal_length &lt;- sd(versicolor_sepal_length)\nn &lt;- length(versicolor_sepal_length)\nt_score &lt;- qt(0.95, df = n - 1)\nalpha_level &lt;- 0.10\nmargin_of_error &lt;- t_score * (sd_sepal_length / sqrt(n))\nlower_bound &lt;- mean_sepal_length - margin_of_error\nupper_bound &lt;- mean_sepal_length + margin_of_error\n\nlist(mean = mean_sepal_length, sd = sd_sepal_length, t_score = t_score, alpha_level = alpha_level,\n     margin_of_error = margin_of_error, lower_bound = lower_bound, upper_bound = upper_bound)\n\n$mean\n[1] 5.936\n\n$sd\n[1] 0.5161711\n\n$t_score\n[1] 1.676551\n\n$alpha_level\n[1] 0.1\n\n$margin_of_error\n[1] 0.1223842\n\n$lower_bound\n[1] 5.813616\n\n$upper_bound\n[1] 6.058384\n\n\nResult: The 90% confidence interval for the mean sepal length of the Versicolor species is approximately [5.8136158, 6.0583842].",
    "crumbs": [
      "Step 4 - Experimentation and Prediction",
      "Estimating Parameters Using Confidence Intervals"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "",
    "text": "Problem 1\nGender identity of individuals in a survey.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-1",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-1",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "",
    "text": "Type: Qualitative (Nominal) \nExplanation: Gender identity categories (e.g., male, female, non-binary) do not have a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-2",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-2",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 2",
    "text": "Problem 2\nAnnual income of households in a city. \n\nType: Quantitative (Continuous) \nExplanation: Annual income can take on any value within a range and can be measured with precision.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-3",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-3",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 3",
    "text": "Problem 3\nNumber of books read by students in a year. \n\nType: Quantitative (Discrete) \nExplanation: The number of books is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-4",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-4",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 4",
    "text": "Problem 4\nEducational attainment levels (e.g., high school diploma, bachelor’s degree, master’s degree, etc.). \n\nType: Qualitative (Ordinal) \nExplanation: Educational levels have a meaningful order (e.g., high school &lt; bachelor’s &lt; master’s).",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-5",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-5",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 5",
    "text": "Problem 5\nNumber of reported hate crimes in different states. \n\nType: Quantitative (Discrete) \nExplanation: The number of hate crimes is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-6",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-6",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 6",
    "text": "Problem 6\nTypes of housing (e.g., owned, rented, homeless). \n\nType: Qualitative (Nominal) \nExplanation: Housing types are categories without a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-7",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-7",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 7",
    "text": "Problem 7\nAge of participants in a community health study. \n\nType: Quantitative (Continuous) \nExplanation: Age can take on any value within a range and can be measured with precision.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-8",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-8",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 8",
    "text": "Problem 8\nPolitical party affiliation (e.g., Democrat, Republican, Independent). \n\nType: Qualitative (Nominal) \nExplanation: Political party categories do not have a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-9",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-9",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 9",
    "text": "Problem 9\nNumber of languages spoken by immigrants in a region. \n\nType: Quantitative (Discrete) \nExplanation: The number of languages is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-10",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-10",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 10",
    "text": "Problem 10\nFrequency of attending religious services (e.g., never, occasionally, regularly). \n\nType: Qualitative (Ordinal) \nExplanation: Frequency categories have a meaningful order (e.g., never &lt; occasionally &lt; regularly).",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-11",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-11",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 11",
    "text": "Problem 11\nNumber of protests attended in a year. \n\nType: Quantitative (Discrete) \nExplanation: The number of protests is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-12",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-12",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 12",
    "text": "Problem 12\nTypes of discrimination experienced (e.g., racial, gender, disability). \n\nType: Qualitative (Nominal) \nExplanation: Discrimination types are categories without a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-13",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-13",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 13",
    "text": "Problem 13\nHeight of students in a classroom. \n\nType: Quantitative (Continuous) \nExplanation: Height can take on any value within a range and can be measured with precision.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-14",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-14",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 14",
    "text": "Problem 14\nMarital status (e.g., single, married, divorced, widowed). \n\nType: Qualitative (Nominal) \nExplanation: Marital status categories do not have a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-15",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-15",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 15",
    "text": "Problem 15\nNumber of social media accounts owned by individuals. \n\nType: Quantitative (Discrete) \nExplanation: The number of social media accounts is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-16",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-16",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 16",
    "text": "Problem 16\nSeverity of food insecurity (e.g., none, mild, moderate, severe). \n\nType: Qualitative (Ordinal) \nExplanation: Severity levels have a meaningful order (e.g., none &lt; mild &lt; moderate &lt; severe).",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-17",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-17",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 17",
    "text": "Problem 17\nNumber of children in a family. \n\nType: Quantitative (Discrete) \nExplanation: The number of children is countable and takes on whole number values.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-18",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-18",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 18",
    "text": "Problem 18\nTypes of employment (e.g., full-time, part-time, unemployed). \n\nType: Qualitative (Nominal) \nExplanation: Employment types are categories without a natural order.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-19",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-19",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 19",
    "text": "Problem 19\nMonthly electricity usage in kilowatt-hours. \n\nType: Quantitative (Continuous) \nExplanation: Electricity usage can take on any value within a range and can be measured with precision.",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-20",
    "href": "Qualitative_and_Quantitative_Variables_Assignment_Solutions.html#problem-20",
    "title": "Qualitative and Quantitative Variables Assignment Solutions",
    "section": "Problem 20",
    "text": "Problem 20\nFrequency of using public transportation (e.g., never, sometimes, always).\n\nType: Qualitative (Ordinal) \nExplanation: Frequency categories have a meaningful order (e.g., never &lt; sometimes &lt; always).",
    "crumbs": [
      "Appendices",
      "Qualitative and Quantitative Variables Assignment Solutions"
    ]
  },
  {
    "objectID": "Tidy_Data_Assignment_Solutions.html",
    "href": "Tidy_Data_Assignment_Solutions.html",
    "title": "Tidy Data Assignment Solutions",
    "section": "",
    "text": "Solutions",
    "crumbs": [
      "Appendices",
      "Tidy Data Assignment Solutions"
    ]
  },
  {
    "objectID": "Tidy_Data_Assignment_Solutions.html#solutions",
    "href": "Tidy_Data_Assignment_Solutions.html#solutions",
    "title": "Tidy Data Assignment Solutions",
    "section": "",
    "text": "Problem 1 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_weather_data &lt;- gather(weather_data, key = \"Month\", value = \"Temperature\", Jan_Temp:Mar_Temp) %&gt;%\n  mutate(Month = gsub(\"_Temp\", \"\", Month))\n\nkable(tidy_weather_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 2 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_grades_data &lt;- gather(grades_data, key = \"Subject\", value = \"Grade\", Math:History)\n\nkable(tidy_grades_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 3 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_sales_data &lt;- gather(sales_data, key = \"Month\", value = \"Sales\", Jan_Sales:Mar_Sales) %&gt;%\n  mutate(Month = gsub(\"_Sales\", \"\", Month))\n\nkable(tidy_sales_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 4 Solution\nThe dataset is tidy.\n\n\nProblem 5 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_financial_data &lt;- gather(financial_data, key = \"Quarter\", value = \"Revenue\", Q1_Revenue:Q3_Revenue) %&gt;%\n  mutate(Quarter = gsub(\"_Revenue\", \"\", Quarter))\n\nkable(tidy_financial_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 6 Solution\nThe dataset is tidy.\n\n\nProblem 7 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_ratings_data &lt;- gather(ratings_data, key = \"Critic\", value = \"Rating\", Critic1:Critic3)\n\nkable(tidy_ratings_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 8 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_salary_data &lt;- gather(salary_data, key = \"Department\", value = \"Salary\", Dept1_Salary:Dept3_Salary) %&gt;%\n  mutate(Department = gsub(\"_Salary\", \"\", Department))\n\nkable(tidy_salary_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\nProblem 9 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_reviews_data &lt;- gather(reviews_data, key = \"Review_Number\", value = \"Review\", Review1:Review3) %&gt;%\n  mutate(Review_Number = gsub(\"Review\", \"\", Review_Number))\n\nkable(tidy_reviews_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nProblem 10 Solution\nThe dataset is not tidy. Here is the tidy version:\ntidy_enrollment_data &lt;- gather(enrollment_data, key = \"Semester\", value = \"Enrollment\", Semester1:Semester3) %&gt;%\n  mutate(Semester = gsub(\"Semester\", \"\", Semester))\n\nkable(tidy_enrollment_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 11\nIs the data tidy? No\ntidy_sales2_data &lt;- gather(sales2_data, key = \"Month\", value = \"Sales\", January:March)\n\ntidy_sales2_data$Month &lt;- factor(tidy_sales2_data$Month, levels = c(\"January\", \"February\", \"March\"))\n\nkable(tidy_sales_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 12\nIs the data tidy? No\ntidy_survey_data &lt;- gather(survey_data, key = \"Service\", value = \"Satisfaction\", Service1_Satisfaction:Service3_Satisfaction)\n\ntidy_survey_data$Service &lt;- gsub(\"_Satisfaction\", \"\", tidy_survey_data$Service)\n\nkable(tidy_survey_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 13\nIs the data tidy? No\ntidy_weather2_data &lt;- gather(weather2_data, key = \"Time\", value = \"Temperature\", Morning:Evening)\n\ntidy_weather2_data$Time &lt;- factor(tidy_weather2_data$Time, levels = c(\"Morning\", \"Noon\", \"Evening\"))\n\nkable(tidy_weather2_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 14\nIs the data tidy? No\ntidy_exam_scores &lt;- gather(exam_scores, key = \"Subject\", value = \"Score\", Math:History)\n\ntidy_exam_scores$Subject &lt;- factor(tidy_exam_scores$Subject, levels = c(\"Math\", \"Science\", \"History\"))\n\nkable(tidy_exam_scores, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 15\nIs the data tidy? No\ntidy_hospital_data &lt;- gather(hospital_data, key = \"Month\", value = \"Patients\", January:March)\n\ntidy_hospital_data$Month &lt;- factor(tidy_hospital_data$Month, levels = c(\"January\", \"February\", \"March\"))\n\nkable(tidy_hospital_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 16\nIs the data tidy? No\ntidy_marketing_data &lt;- gather(marketing_data, key = \"Week\", value = \"Leads\", Week1:Week3)\n\ntidy_marketing_data$Week &lt;- factor(tidy_marketing_data$Week, levels = c(\"Week1\", \"Week2\", \"Week3\"))\n\nkable(tidy_marketing_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 17\nIs the data tidy? No\ntidy_fitness_data &lt;- gather(fitness_data, key = \"Day\", value = \"Workout_Time\", Monday:Friday)\n\ntidy_fitness_data$Day &lt;- factor(tidy_fitness_data$Day, levels = c(\"Monday\", \"Wednesday\", \"Friday\"))\n\nkable(tidy_fitness_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n ### Solution to Problem 18\nIs the data tidy? No\ntidy_financial2_data &lt;- gather(financial2_data, key = \"Quarter\", value = \"Profit\", Q1:Q4)\n\ntidy_financial2_data$Quarter &lt;- factor(tidy_financial2_data$Quarter, levels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"))\n\nkable(tidy_financial2_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 19\nIs the data tidy? No\ntidy_attendance_data &lt;- gather(attendance_data, key = \"Day\", value = \"Attendance\", Day1:Day3)\n\ntidy_attendance_data$Day &lt;- factor(tidy_attendance_data$Day, levels = c(\"Day1\", \"Day2\", \"Day3\"))\n\nkable(tidy_attendance_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")\n\n\n\nSolution to Problem 20\nIs the data tidy? No\ntidy_production_data &lt;- gather(production_data, key = \"Shift\", value = \"Output\", Shift1:Shift3)\n\ntidy_production_data$Shift &lt;- factor(tidy_production_data$Shift, levels = c(\"Shift1\", \"Shift2\", \"Shift3\"))\n\nkable(tidy_production_data, format = \"html\", table.attr = \"style='width:4in; margin-left:auto; margin.right:auto;'\")  %&gt;%\n  kable_styling(position=\"center\")",
    "crumbs": [
      "Appendices",
      "Tidy Data Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "",
    "text": "Problem 1\nRun ggplot(data = mpg). What do you see? Why do you think this is?\nggplot(data = mpg)\n\n\n\n\n\n\n\n# This give us an empty canvas to start plotting our data.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-2",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-2",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 2",
    "text": "Problem 2\nHow many rows are in mpg? How many columns? \n\n# We need to find information about the data set \"mpg\". There are several \n# commands that can give us this information. Here we will use \"glimspe\".\n\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n# By looking into the console, we can see that this data set has 234 rows\n# and 11 columns.\n\n# Note that we could also have used the \"str\" command :\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n# The output is : \n\n# tibble [234 x 11] \n\n# So 234 rows (observations), and 11 columns (variables).\n\n# There are other commands such as \"nrow\" and \"ncol\" we could have used :\n\nnrow(mpg)\n\n[1] 234\n\n# output : [1] 234\n\nncol(mpg)\n\n[1] 11\n\n# output : [1] 11\n\n# As with most problems in R, there are multiple ways to find an answer.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-3",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-3",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 3",
    "text": "Problem 3\nWhat does the drv variable describe? Read the help for ?mpg to find out. \n\n?mpg\n\n# According to the help file, drv is a character (text) variable : \n\n# the type of drive train, where f = front-wheel drive, r = rear wheel drive, \n# 4 = 4wd\n\n# This means we will have a categorical variable that is either an \"f\", \"r\",\n# or \"4\"",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-4",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-4",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 4",
    "text": "Problem 4\nMake a scatterplot of hwy vs cyl. \n\n# hwy = highway miles per gallon\n\n# cyl = number of cylinders in the car engine \n\n# Note that the higher the number of cylinders should cause more gas usage,\n# causing the mileage to go down.\n\nggplot(data = mpg, aes(x=cyl, y=hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n# or\n\nggplot(data=mpg) +\n  geom_point(aes(x=cyl, y=hwy))",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-5",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-5",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 5",
    "text": "Problem 5\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful? \n\n# class = \"type\" of car\n\n# drv = the type of drive train, where \n#       f = front-wheel drive, \n#       r = rear wheel drive, \n#       4 = 4wd\n\nggplot(data=mpg, aes(x=class, y=drv)) +\n  geom_point()\n\n\n\n\n\n\n\n# This isn't particularly useful as most cars have multiple drive trains,\n# depending on what is offered for the vehicle.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-6",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-6",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 6",
    "text": "Problem 6\nConsider the following plot. Why are the points not blue?\n\n ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\n\n\n\n\nWhen we change the color in the aes function to “blue”, the points will not be blue because the aes part of the command works on mappings to the variables. In this case, we are not making a color choice because of the variable itself. If we want to affect the color based on the variable value, then the color can be specified inside of the aes function. \nHere we want the points to be one specific color that is not dependent on how the data is mapped. So in this case the color change needs to be outside of the aes function. \nThat means we have a parenthesis in the wrong place! \nClose off the parens after the mapping \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-7",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-7",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 7",
    "text": "Problem 7\nWhich variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\n\n# How can you see this information when you run mpg?\n\n# Note that the glimpse command also gives us the type of variable.\n\n# For categorical variables, look for the \"&lt;chr&gt;\" descriptor\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n# Here are the categorical variables :\n\n# manufacturer, model, trans, drv, fl, class",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-8",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-8",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 8",
    "text": "Problem 8\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\n\n# Note that some quantitative variables are discrete and some are continuous.\n\n# Discrete : year, cyl, cty, hwy\n\n# Continuous : displ\n\n# We can use the previous ggplot and make some changes :\n\n# Here is the previous plot : \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\n# This already has a color to the points. We could change ALL the points\n# to a different color :\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"green\")\n\n\n\n\n\n\n\n# Note : UGH! Bad color!\n\n# What if we wanted to change the color based on the variable \"displ\"? \n\n# Since we are going to be coloring the points based on a value of a variable,\n# we need to put the color inside of the `aes` function.\n\nggplot(data = mpg) + \n  geom_point(aes(x=displ, y=hwy, color=displ))\n\n\n\n\n\n\n\n# Notice the colors. You get different grades of the blue color. The larger\n# values of \"displ\" have a lighter color of blue and the smaller values \n# have a darker color. \n\n# The reason we don't have major changes in the color (blue, red, green, etc)\n# is because this is continuous (quantitative). Some values are \"slightly\" \n# different than others, so the color is \"slightly\" different to represent \n# that change.\n\n# Let's throw in a categorical variable. Let's compare hwy to drv\n\nggplot(data=mpg, aes(x=drv, y=hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n# This shows us the front wheel drive cars tend to get higher gas mileage\n# on the highway. \n\n# We can make this more interesting by coloring the dots by the manufacturer\n# of the cars to see if a certain brand typcially gets higher gas mileage.\n\nggplot(data=mpg, aes(x=drv, y=hwy, color = manufacturer)) +\n  geom_point()\n\n\n\n\n\n\n\n# or\n\nggplot(data=mpg) +\n  geom_point(aes(x=drv, y=hwy, color = manufacturer))\n\n\n\n\n\n\n\n# Notice that from the picture, \"audi\" seems to have the highest gas\n# mielage for their cars.\n\n# We could then have the dots get larger as the mileage goes up.So change \n# the size of the points based on the \"hwy\" variable.\n\nggplot(data=mpg) +\n  geom_point(aes(x=drv, y=hwy, color = manufacturer, size=hwy))",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-9",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-9",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 9",
    "text": "Problem 9\nWhat happens if you map the same variable to multiple aesthetics?\n\n# Note that we just did this with the last example! \n\n# What this did kind of depends on your point of view. \n\n# In some ways it was redundant. We knew that the higher points represented\n# the higher mileage. By adding the aesthetic that the size of the points\n# goes up with the mileage, we don't get any new information.\n\n# On the other hand, this also reinforces the mileage data and makes it easier \n# to see that the bigger points means bigger mileage.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-10",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-10",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 10",
    "text": "Problem 10\nWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\n\n?geom_point\n\n# According to the help file, \"the stroke aesthetic to modify the width of the\n# border\"\n\n# Here is an example of a plot comparing \"wt\" to \"mpg\".\n\n# The points have shape 21 which is a circle with a border. The border of the\n# points is black and the interior is white : \n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(shape = 21, colour = \"black\", fill = \"white\", size = 5)\n\n\n\n\n\n\n\n# The \"stroke\" command perhaps refers to the size of the stroke of the pen \n# being used? This means a bigger stroke has a bigger outline?\n\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point(shape = 21, colour = \"black\", fill = \"white\", size = 5, stroke = 5)",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-11",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-11",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 11",
    "text": "Problem 11\nWhat happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? Note, you’ll also need to specify x and y.\n\n# Here is a basic plot comparing \"displ\" to \"hwy\"\n\nggplot(data=mpg) +\n  geom_point(aes(x=displ, y=hwy))\n\n\n\n\n\n\n\n# Let's add in the condition \n\nggplot(data=mpg) +\n  geom_point(aes(x=displ, y=hwy, color = displ&lt;5))\n\n\n\n\n\n\n\n# It appears to show us that we can differentiate two parts of the graph by \n# color. In other words, points below a threshold get one color and points\n# above the threshold get a different color.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-12",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-12",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 12",
    "text": "Problem 12\nWhat happens if you facet on a continuous variable?\n\n# Let's look at an example. Create a graph comparing displ (engine displacement, \n# in litres) to hwy (highway miles per gallon), but faceted with cty (city miles \n# per gallon) : \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ cty)\n\n\n\n\n\n\n\n# The continuous variable is converted to a categorical variable, and \n# the plot contains a facet for each distinct value.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-13",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-13",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 13",
    "text": "Problem 13\nWhat do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl)) +\n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n# When looking at this graph, there are several blank facets\n\n# How do they relate to this plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\n\n\n\n\n\n\n\n# Each blank facet corresponds to the locations where there is no point.",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-14",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-14",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 14",
    "text": "Problem 14\nWhat plots does the following code make? What does . do?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n# The symbol . ignores that dimension when faceting. For example, \n# drv ~ . facet by values of drv on the y-axis\n\n# While, . ~ cyl will facet by values of cyl on the x-axis.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-15",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-15",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 15",
    "text": "Problem 15\nConsider the first faceted plot in this section. What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n# What are the advantages to using faceting instead of the colour aesthetic? \n\n# There are sometimes so many different colors / variables that the points can start to run\n# together and it is difficult to differentiate the variables. A facet can break up the main\n# data set by the variables themselves and make it easier to compare one variable to another.\n\n# What are the disadvantages? \n\n\n# How might the balance change if you had a larger dataset?",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-16",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-16",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 16",
    "text": "Problem 16\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\n\n# Answer",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-17",
    "href": "Beginning_Data_Visualization_Assignment_Solutions.html#problem-17",
    "title": "Beginning Data Visualization Assignment Solutions",
    "section": "Problem 17",
    "text": "Problem 17\nWhen using facet_grid() you should usually put the variable with more unique levels in the columns. Why?\n\n# Answer",
    "crumbs": [
      "Appendices",
      "Beginning Data Visualization Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html",
    "href": "EDA_Assignment_sheet.html",
    "title": "EDA Assignment Sheet",
    "section": "",
    "text": "Introduction\nNow that you have seen some of the introductory data skills in R, it is time to put them to use. This assignment consists of 10 problems where you will download quantitative datasets from various sources. You will then analyze the data by creating different visualizations such as scatterplots, histograms, and bar plots. You will also calculate means, medians, standard deviations, five-number summaries, and regression lines. Some datasets will be continuous and some will be discrete.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-1-gender-pay-gap-analysis",
    "href": "EDA_Assignment_sheet.html#problem-1-gender-pay-gap-analysis",
    "title": "EDA Assignment Sheet",
    "section": "Problem 1: Gender Pay Gap Analysis",
    "text": "Problem 1: Gender Pay Gap Analysis\nDataset: Gender Pay Gap Data\nDescription: This dataset contains information on the gender pay gap across different industries. The variables include industry, median pay for men, and median pay for women (continuous variables).\nTasks:\n\nCalculate the mean and median pay for men and women.\nCreate a barplot to visualize the median pay for men and women across different industries.\nCalculate the pay gap (difference between men’s and women’s pay) for each industry.\nCreate a histogram of the pay gap.\nCalculate a 95% confidence interval for the average pay gap.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-2-racial-disparities-in-incarceration-rates",
    "href": "EDA_Assignment_sheet.html#problem-2-racial-disparities-in-incarceration-rates",
    "title": "EDA Assignment Sheet",
    "section": "Problem 2: Racial Disparities in Incarceration Rates",
    "text": "Problem 2: Racial Disparities in Incarceration Rates\nDataset: US Incarceration Rates\nDescription: This dataset contains the incarceration rates per 100,000 people for different racial groups (discrete variables).\nTasks:\n\nCalculate the mean and standard deviation of incarceration rates for each racial group.\nCreate a barplot to visualize the incarceration rates for each racial group.\nCalculate a 95% confidence interval for the average incarceration rate for each racial group.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-3-access-to-education-by-region",
    "href": "EDA_Assignment_sheet.html#problem-3-access-to-education-by-region",
    "title": "EDA Assignment Sheet",
    "section": "Problem 3: Access to Education by Region",
    "text": "Problem 3: Access to Education by Region\nDataset: Global Education Data\nDescription: This dataset contains information on the average years of schooling for different regions (continuous variable).\nTasks:\n\nCalculate the mean and median years of schooling for each region.\nCreate a histogram of the years of schooling.\nCalculate a 95% confidence interval for the average years of schooling for each region.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-4-unemployment-rates-by-race-and-gender",
    "href": "EDA_Assignment_sheet.html#problem-4-unemployment-rates-by-race-and-gender",
    "title": "EDA Assignment Sheet",
    "section": "Problem 4: Unemployment Rates by Race and Gender",
    "text": "Problem 4: Unemployment Rates by Race and Gender\nDataset: US Unemployment Data\nDescription: This dataset contains unemployment rates for different racial and gender groups (discrete variables).\nTasks:\n\nCalculate the mean and standard deviation of unemployment rates for each racial and gender group.\nCreate a faceted scatterplot to visualize the unemployment rates for each racial and gender group over time.\nCalculate a 95% confidence interval for the average unemployment rate for each racial and gender group.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-5-income-inequality-by-state",
    "href": "EDA_Assignment_sheet.html#problem-5-income-inequality-by-state",
    "title": "EDA Assignment Sheet",
    "section": "Problem 5: Income Inequality by State",
    "text": "Problem 5: Income Inequality by State\nDataset: US Income Inequality Data\nDescription: This dataset contains the Gini coefficient for income inequality by state (continuous variable).\nTasks:\n\nCalculate the mean and median Gini coefficient for the states.\nCreate a histogram of the Gini coefficients.\nCalculate a 95% confidence interval for the average Gini coefficient.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-6-food-insecurity-rates-by-county",
    "href": "EDA_Assignment_sheet.html#problem-6-food-insecurity-rates-by-county",
    "title": "EDA Assignment Sheet",
    "section": "Problem 6: Food Insecurity Rates by County",
    "text": "Problem 6: Food Insecurity Rates by County\nDataset: USDA Food Insecurity Data\nDescription: This dataset contains the percentage of households experiencing food insecurity by county (discrete variable).\nTasks:\n\nCalculate the mean and standard deviation of food insecurity rates for the counties.\nCreate a barplot to visualize the food insecurity rates by county.\nCalculate a 95% confidence interval for the average food insecurity rate.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-7-environmental-pollution-and-health-outcomes",
    "href": "EDA_Assignment_sheet.html#problem-7-environmental-pollution-and-health-outcomes",
    "title": "EDA Assignment Sheet",
    "section": "Problem 7: Environmental Pollution and Health Outcomes",
    "text": "Problem 7: Environmental Pollution and Health Outcomes\nDataset: EPA Air Quality Data\nDescription: This dataset contains air quality index (AQI) values and asthma rates for different regions (continuous variables).\nTasks:\n\nCalculate the mean and median AQI and asthma rates for each region.\nCreate a scatterplot to visualize the relationship between AQI and asthma rates.\nCalculate the correlation coefficient between AQI and asthma rates.\nFit a linear regression model to predict asthma rates based on AQI.\nCreate a residual plot for the regression model.\nCalculate a 95% confidence interval for the slope of the regression line.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-8-access-to-clean-water",
    "href": "EDA_Assignment_sheet.html#problem-8-access-to-clean-water",
    "title": "EDA Assignment Sheet",
    "section": "Problem 8: Access to Clean Water",
    "text": "Problem 8: Access to Clean Water\nDataset: WHO/UNICEF Joint Monitoring Programme for Water Supply, Sanitation and Hygiene\nDescription: This dataset contains the percentage of the population with access to clean water by country (discrete variable).\nTasks:\n\nCalculate the mean and standard deviation of access to clean water rates for the countries.\nCreate a barplot to visualize the access to clean water rates by country.\nCalculate a 95% confidence interval for the average access to clean water rate.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-9-child-mortality-rates-by-region",
    "href": "EDA_Assignment_sheet.html#problem-9-child-mortality-rates-by-region",
    "title": "EDA Assignment Sheet",
    "section": "Problem 9: Child Mortality Rates by Region",
    "text": "Problem 9: Child Mortality Rates by Region\nDataset: UNICEF Child Mortality Data\nDescription: This dataset contains child mortality rates (deaths per 1,000 live births) for different regions (continuous variable).\nTasks:\n\nCalculate the mean and median child mortality rates for each region.\nCreate a histogram of the child mortality rates.\nCalculate a 95% confidence interval for the average child mortality rate.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_sheet.html#problem-10-literacy-rates-by-gender",
    "href": "EDA_Assignment_sheet.html#problem-10-literacy-rates-by-gender",
    "title": "EDA Assignment Sheet",
    "section": "Problem 10: Literacy Rates by Gender",
    "text": "Problem 10: Literacy Rates by Gender\nDataset: UNESCO Literacy Data\nDescription: This dataset contains literacy rates for males and females in different countries (discrete variables).\nTasks:\n\nCalculate the mean and standard deviation of literacy rates for males and females.\nCreate a faceted scatterplot to visualize the literacy rates for males and females by country.\nCalculate a 95% confidence interval for the average literacy rate for males and females.",
    "crumbs": [
      "Appendices",
      "EDA Assignment Sheet"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html",
    "href": "EDA_Assignment_Sheet_Solutions.html",
    "title": "EDA Assignment Solutions",
    "section": "",
    "text": "Problem 1: Gender Pay Gap Analysis",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-1-gender-pay-gap-analysis",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-1-gender-pay-gap-analysis",
    "title": "EDA Assignment Solutions",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\ngender_pay_gap &lt;- read.csv(\"gender_pay_gap_data.csv\")\n\n# Calculate the mean and median pay for men and women\nmean_men &lt;- mean(gender_pay_gap$men_pay, na.rm = TRUE)\nmedian_men &lt;- median(gender_pay_gap$men_pay, na.rm = TRUE)\nmean_women &lt;- mean(gender_pay_gap$women_pay, na.rm = TRUE)\nmedian_women &lt;- median(gender_pay_gap$women_pay, na.rm = TRUE)\n\n# Create a barplot to visualize the median pay for men and women across different industries\nggplot(gender_pay_gap, aes(x = industry)) +\n  geom_bar(aes(y = women_pay), stat = \"identity\", fill = \"pink\") +\n  geom_bar(aes(y = men_pay), stat = \"identity\", fill = \"blue\", alpha = 0.5) +\n  ylab(\"Median Pay\") + xlab(\"Industry\") + \n  ggtitle(\"Median Pay by Gender and Industry\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Calculate the pay gap for each industry\ngender_pay_gap &lt;- gender_pay_gap %&gt;%\n  mutate(pay_gap = men_pay - women_pay)\n\n# Create a histogram of the pay gap\nggplot(gender_pay_gap, aes(x = pay_gap)) +\n  geom_histogram(binwidth = 5000, fill = \"blue\", alpha = 0.7) +\n  xlab(\"Pay Gap\") + ylab(\"Frequency\") + \n  ggtitle(\"Histogram of Gender Pay Gap\")\n\n# Calculate a 95% confidence interval for the average pay gap\npay_gap_mean &lt;- mean(gender_pay_gap$pay_gap, na.rm = TRUE)\npay_gap_sd &lt;- sd(gender_pay_gap$pay_gap, na.rm = TRUE)\nn &lt;- nrow(gender_pay_gap)\nerror_margin &lt;- qt(0.975, df = n-1) * pay_gap_sd / sqrt(n)\nci_lower &lt;- pay_gap_mean - error_margin\nci_upper &lt;- pay_gap_mean + error_margin\nci_lower\nci_upper",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-2-racial-disparities-in-incarceration-rates",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-2-racial-disparities-in-incarceration-rates",
    "title": "EDA Assignment Solutions",
    "section": "Problem 2: Racial Disparities in Incarceration Rates",
    "text": "Problem 2: Racial Disparities in Incarceration Rates\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nincarceration_rates &lt;- read.csv(\"us_incarceration_rates.csv\")\n\n# Calculate the mean and standard deviation of incarceration rates for each racial group\nincarceration_summary &lt;- incarceration_rates %&gt;%\n  group_by(race) %&gt;%\n  summarize(mean_rate = mean(rate, na.rm = TRUE),\n            sd_rate = sd(rate, na.rm = TRUE))\n\n# Create a barplot to visualize the incarceration rates for each racial group\nggplot(incarceration_rates, aes(x = race, y = rate)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  ylab(\"Incarceration Rate\") + xlab(\"Racial Group\") + \n  ggtitle(\"Incarceration Rates by Racial Group\")\n\n# Calculate a 95% confidence interval for the average incarceration rate for each racial group\nincarceration_summary &lt;- incarceration_summary %&gt;%\n  mutate(error_margin = qt(0.975, df = n()-1) * sd_rate / sqrt(n()),\n         ci_lower = mean_rate - error_margin,\n         ci_upper = mean_rate + error_margin)\nincarceration_summary",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-3-access-to-education-by-region",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-3-access-to-education-by-region",
    "title": "EDA Assignment Solutions",
    "section": "Problem 3: Access to Education by Region",
    "text": "Problem 3: Access to Education by Region\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\neducation_data &lt;- read.csv(\"global_education_data.csv\")\n\n# Calculate the mean and median years of schooling for each region\neducation_summary &lt;- education_data %&gt;%\n  group_by(region) %&gt;%\n  summarize(mean_years = mean(years_of_schooling, na.rm = TRUE),\n            median_years = median(years_of_schooling, na.rm = TRUE))\n\n# Create a histogram of the years of schooling\nggplot(education_data, aes(x = years_of_schooling)) +\n  geom_histogram(binwidth = 1, fill = \"green\", alpha = 0.7) +\n  xlab(\"Years of Schooling\") + ylab(\"Frequency\") + \n  ggtitle(\"Histogram of Years of Schooling\")\n\n# Calculate a 95% confidence interval for the average years of schooling for each region\neducation_summary &lt;- education_summary %&gt;%\n  mutate(sd_years = sd(education_data$years_of_schooling, na.rm = TRUE),\n         n = n(),\n         error_margin = qt(0.975, df = n-1) * sd_years / sqrt(n),\n         ci_lower = mean_years - error_margin,\n         ci_upper = mean_years + error_margin)\neducation_summary",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-4-unemployment-rates-by-race-and-gender",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-4-unemployment-rates-by-race-and-gender",
    "title": "EDA Assignment Solutions",
    "section": "Problem 4: Unemployment Rates by Race and Gender",
    "text": "Problem 4: Unemployment Rates by Race and Gender\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nunemployment_data &lt;- read.csv(\"us_unemployment_data.csv\")\n\n# Calculate the mean and standard deviation of unemployment rates for each racial and gender group\nunemployment_summary &lt;- unemployment_data %&gt;%\n  group_by(race, gender) %&gt;%\n  summarize(mean_rate = mean(unemployment_rate, na.rm = TRUE),\n            sd_rate = sd(unemployment_rate, na.rm = TRUE))\n\n# Create a faceted scatterplot to visualize the unemployment rates for each racial and gender group over time\nggplot(unemployment_data, aes(x = year, y = unemployment_rate, color = gender)) +\n  geom_point() +\n  facet_wrap(~ race) +\n  ylab(\"Unemployment Rate\") + xlab(\"Year\") + \n  ggtitle(\"Unemployment Rates by Race and Gender Over Time\")\n\n# Calculate a 95% confidence interval for the average unemployment rate for each racial and gender group\nunemployment_summary &lt;- unemployment_summary %&gt;%\n  mutate(n = n(),\n         error_margin = qt(0.975, df = n-1) * sd_rate / sqrt(n),\n         ci_lower = mean_rate - error_margin,\n         ci_upper = mean_rate + error_margin)\nunemployment_summary",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-5-income-inequality-by-state",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-5-income-inequality-by-state",
    "title": "EDA Assignment Solutions",
    "section": "Problem 5: Income Inequality by State",
    "text": "Problem 5: Income Inequality by State\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nincome_inequality_data &lt;- read.csv(\"us_income_inequality_data.csv\")\n\n# Calculate the mean and median Gini coefficient for the states\nmean_gini &lt;- mean(income_inequality_data$gini_coefficient, na.rm = TRUE)\nmedian_gini &lt;- median(income_inequality_data$gini_coefficient, na.rm = TRUE)\n\n# Create a histogram of the Gini coefficients\nggplot(income_inequality_data, aes(x = gini_coefficient)) +\n  geom_histogram(binwidth = 0.01, fill = \"purple\", alpha = 0.7) +\n  xlab(\"Gini Coefficient\") + ylab(\"Frequency\") + \n  ggtitle(\"Histogram of Gini Coefficients\")\n\n# Calculate a 95% confidence interval for the average Gini coefficient\ngini_mean &lt;- mean(income_inequality_data$gini_coefficient, na.rm = TRUE)\ngini_sd &lt;- sd(income_inequality_data$gini_coefficient, na.rm = TRUE)\nn &lt;- nrow(income_inequality_data)\nerror_margin &lt;- qt(0.975, df = n-1) * gini_sd / sqrt(n)\nci_lower &lt;- gini_mean - error_margin\nci_upper &lt;- gini_mean + error_margin\nci_lower\nci_upper",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-6-food-insecurity-rates-by-county",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-6-food-insecurity-rates-by-county",
    "title": "EDA Assignment Solutions",
    "section": "Problem 6: Food Insecurity Rates by County",
    "text": "Problem 6: Food Insecurity Rates by County\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nfood_insecurity_data &lt;- read.csv(\"usda_food_insecurity_data.csv\")\n\n# Calculate the mean and standard deviation of food insecurity rates for the counties\nfood_insecurity_summary &lt;- food_insecurity_data %&gt;%\n  summarize(mean_rate = mean(food_insecurity_rate, na.rm = TRUE),\n            sd_rate = sd(food_insecurity_rate, na.rm = TRUE))\n\n# Create a barplot to visualize the food insecurity rates by county\nggplot(food_insecurity_data, aes(x = county, y = food_insecurity_rate)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  ylab(\"Food Insecurity Rate\") + xlab(\"County\") + \n  ggtitle(\"Food Insecurity Rates by County\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Calculate a 95% confidence interval for the average food insecurity rate\nmean_rate &lt;- mean(food_insecurity_data$food_insecurity_rate, na.rm = TRUE)\nsd_rate &lt;- sd(food_insecurity_data$food_insecurity_rate, na.rm = TRUE)\nn &lt;- nrow(food_insecurity_data)\nerror_margin &lt;- qt(0.975, df = n-1) * sd_rate / sqrt(n)\nci_lower &lt;- mean_rate - error_margin\nci_upper &lt;- mean_rate + error_margin\nci_lower\nci_upper",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-7-environmental-pollution-and-health-outcomes",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-7-environmental-pollution-and-health-outcomes",
    "title": "EDA Assignment Solutions",
    "section": "Problem 7: Environmental Pollution and Health Outcomes",
    "text": "Problem 7: Environmental Pollution and Health Outcomes\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nair_quality_data &lt;- read.csv(\"epa_air_quality_data.csv\")\n\n# Calculate the mean and median AQI and asthma rates for each region\nair_quality_summary &lt;- air_quality_data %&gt;%\n  group_by(region) %&gt;%\n  summarize(mean_aqi = mean(AQI, na.rm = TRUE),\n            median_aqi = median(AQI, na.rm = TRUE),\n            mean_asthma_rate = mean(asthma_rate, na.rm = TRUE),\n            median_asthma_rate = median(asthma_rate, na.rm = TRUE))\n\n# Create a scatterplot to visualize the relationship between AQI and asthma rates\nggplot(air_quality_data, aes(x = AQI, y = asthma_rate)) +\n  geom_point(color = \"blue\") +\n  xlab(\"Air Quality Index (AQI)\") + ylab(\"Asthma Rate\") + \n  ggtitle(\"Relationship between AQI and Asthma Rates\")\n\n# Calculate the correlation coefficient between AQI and asthma rates\ncorrelation &lt;- cor(air_quality_data$AQI, air_quality_data$asthma_rate, use = \"complete.obs\")\n\n# Fit a linear regression model to predict asthma rates based on AQI\nmodel &lt;- lm(asthma_rate ~ AQI, data = air_quality_data)\nsummary(model)\n\n# Create a residual plot for the regression model\nresiduals &lt;- resid(model)\nggplot(air_quality_data, aes(x = AQI, y = residuals)) +\n  geom_point(color = \"red\") +\n  xlab(\"Air Quality Index (AQI)\") + ylab(\"Residuals\") + \n  ggtitle(\"Residual Plot\")\n\n# Calculate a 95% confidence interval for the slope of the regression line\nconfint(model, level = 0.95)",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-8-access-to-clean-water",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-8-access-to-clean-water",
    "title": "EDA Assignment Solutions",
    "section": "Problem 8: Access to Clean Water",
    "text": "Problem 8: Access to Clean Water\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nclean_water_data &lt;- read.csv(\"who_unicef_clean_water_data.csv\")\n\n# Calculate the mean and standard deviation of access to clean water rates for the countries\nclean_water_summary &lt;- clean_water_data %&gt;%\n  summarize(mean_rate = mean(access_rate, na.rm = TRUE),\n            sd_rate = sd(access_rate, na.rm = TRUE))\n\n# Create a barplot to visualize the access to clean water rates by country\nggplot(clean_water_data, aes(x = country, y = access_rate)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  ylab(\"Access to Clean Water Rate\") + xlab(\"Country\") + \n  ggtitle(\"Access to Clean Water Rates by Country\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Calculate a 95% confidence interval for the average access to clean water rate\nmean_rate &lt;- mean(clean_water_data$access_rate, na.rm = TRUE)\nsd_rate &lt;- sd(clean_water_data$access_rate, na.rm = TRUE)\nn &lt;- nrow(clean_water_data)\nerror_margin &lt;- qt(0.975, df = n-1) * sd_rate / sqrt(n)\nci_lower &lt;- mean_rate - error_margin\nci_upper &lt;- mean_rate + error_margin\nci_lower\nci_upper",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-9-child-mortality-rates-by-region",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-9-child-mortality-rates-by-region",
    "title": "EDA Assignment Solutions",
    "section": "Problem 9: Child Mortality Rates by Region",
    "text": "Problem 9: Child Mortality Rates by Region\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nchild_mortality_data &lt;- read.csv(\"unicef_child_mortality_data.csv\")\n\n# Calculate the mean and median child mortality rates for each region\nmortality_summary &lt;- child_mortality_data %&gt;%\n  group_by(region) %&gt;%\n  summarize(mean_rate = mean(mortality_rate, na.rm = TRUE),\n            median_rate = median(mortality_rate, na.rm = TRUE))\n\n# Create a histogram of the child mortality rates\nggplot(child_mortality_data, aes(x = mortality_rate)) +\n  geom_histogram(binwidth = 5, fill = \"pink\", alpha = 0.7) +\n  xlab(\"Child Mortality Rate\") + ylab(\"Frequency\") + \n  ggtitle(\"Histogram of Child Mortality Rates\")\n\n# Calculate a 95% confidence interval for the average child mortality rate\nmortality_mean &lt;- mean(child_mortality_data$mortality_rate, na.rm = TRUE)\nmortality_sd &lt;- sd(child_mortality_data$mortality_rate, na.rm = TRUE)\nn &lt;- nrow(child_mortality_data)\nerror_margin &lt;- qt(0.975, df = n-1) * mortality_sd / sqrt(n)\nci_lower &lt;- mortality_mean - error_margin\nci_upper &lt;- mortality_mean + error_margin\nci_lower\nci_upper",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "EDA_Assignment_Sheet_Solutions.html#problem-10-literacy-rates-by-gender",
    "href": "EDA_Assignment_Sheet_Solutions.html#problem-10-literacy-rates-by-gender",
    "title": "EDA Assignment Solutions",
    "section": "Problem 10: Literacy Rates by Gender",
    "text": "Problem 10: Literacy Rates by Gender\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load the dataset\nliteracy_data &lt;- read.csv(\"unesco_literacy_data.csv\")\n\n# Calculate the mean and standard deviation of literacy rates for males and females\nliteracy_summary &lt;- literacy_data %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_rate = mean(literacy_rate, na.rm = TRUE),\n            sd_rate = sd(literacy_rate, na.rm = TRUE))\n\n# Create a faceted scatterplot to visualize the literacy rates for males and females by country\nggplot(literacy_data, aes(x = country, y = literacy_rate, color = gender)) +\n  geom_point() +\n  facet_wrap(~ gender) +\n  ylab(\"Literacy Rate\") + xlab(\"Country\") + \n  ggtitle(\"Literacy Rates by Gender and Country\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Calculate a 95% confidence interval for the average literacy rate for males and females\nliteracy_summary &lt;- literacy_summary %&gt;%\n  mutate(n = n(),\n         error_margin = qt(0.975, df = n-1) * sd_rate / sqrt(n),\n         ci_lower = mean_rate - error_margin,\n         ci_upper = mean_rate + error_margin)\nliteracy_summary",
    "crumbs": [
      "Appendices",
      "EDA Assignment Solutions"
    ]
  },
  {
    "objectID": "Individual_Lab_1_Intro_To_R.html",
    "href": "Individual_Lab_1_Intro_To_R.html",
    "title": "Individual Lab 1",
    "section": "",
    "text": "In this Lab we are going to go through some of the basics to programming in R. We will look at how to comment your work, basic calculator computations, create a variable, create a vector, how to install a library / package, and functions.\nComments\nComments are a very important part of coding. When you are writing code, you will want to leave notes to yourself and your collaborators to describe what you are doing at each step. You can also leave notes on parts of the code that are norking or that youo feel should be changed. It is a way to remind yourself and your collaborators the work that has been done, why it was done, what is broken, other changes you want to make, and more. They are especially important when you come back to the code after not having looked at it for a while. \nBasically, leave as many comments as possible while coding. You should start with the names of those that are working on the project with a synopsis on what is the purpose of the project. \nA comment is any text following a hashtag (#) on the same line. This text is ignored by the R compiler and does not affect how the script is run.\nCalculator \nR can be used as a calculator. Here are the operators :\nAddition +\n\n# This is an example of addition (and how to make a comment!).\n\n4 + 8\n\n[1] 12\n\n\n\nSubtraction -\n\n# Here is an example of subtraction.\n\n5 - 14\n\n[1] -9\n\n\n\nMultiplication *\n\n# Here is an example of multiplication\n\n8 * 17\n\n[1] 136\n\n\n\nDivision /\n\n# Division\n\n22 / 7\n\n[1] 3.142857\n\n\n\nExponentiation ^\n\n# Exponentiation\n\n5^3\n\n[1] 125\n\n\n\nSquare Roots and Radicals\nRecall that we can use exponents to calculate radicals, too. If you recall, the square root function is the same as raising a value to the (1 / 2) power!\n\n# Here is the square root of 9\n\n9^(1/2)\n\n[1] 3\n\n\n\nNotice that there are some levels of estimation / rounding here. For example, calculate the square root of 2\n\n2^(1/2)\n\n[1] 1.414214\n\n\n\nObviously the real answer goes further than six decimal places. Also think about the square root of 2 multiplied by itself. We should get the value 2 right back :\n\n2^(1/2) * 2^(1/2)\n\n[1] 2\n\n\n\nBut notice what happens if we then subtract 2 from the previous result :\n\n2^(1/2) * 2^(1/2) - 2\n\n[1] 4.440892e-16\n\n\n\nThis result is in scientific notation, but what does it mean? When you see the “e-16” part, that says to move the deciaml places 16 spots to the left, so this answer is closer to 0.000000000000000440892. Notice that it is not zero, as it should be. This is because of the estimation that was talked about above.\nCreating a variable\nWhat is a variable? Imagine dumping some data in a bucket and then giving that bucket a name. Now, every time you use that name, you are really referring to what is in the bucket. \nNOTE 1 : Recall our discussion from class about naming your variables. You want to pick a name for your variables that make sense. It will make editing your script much easier in the long run. If I called a variable “Quiz_Scores” then we know what types of values we are working with. If I called the variable “x” instead, what does that tell us about the variable itself? \nWe can use an “arrow” to assign a value into a variable. An arrow is just a less than sign followed by a dash: &lt;- \nFor example, what if I wanted to assign the value 3 into a variable called “x” and the value 7 into a variable called “y”\n\n# Assign 3 to the variable \"x\"\n\nx &lt;- 3\n\n# Assign 7 to the variable \"y\"\n\ny &lt;- 7\n\n\nNOTE 2 : At this point you should look at the ENVIRONMENT window. This window shows you the variables you are using and the values they contain.\nNow that we have values in these variables, we can now use them in our script. For example, what if I wanted to perform some basic calculations with these variables :\nAddition of two variables : x + y\n\n# We are going to calculate x + y. \n\n# Recall that x = 3 and y = 7 from above, so this should return 10\n\nx + y\n\n[1] 10\n\n\n\nWe could do the same for several different operations :\n\n# Example 1 : Subtract two variables :\n\ny - x\n\n[1] 4\n\n# Example 2 : Multiply a variable by a constant. We could multiply the variable x by 9 to\n# get 9 * 3 = 27\n\n9*x\n\n[1] 27\n\n# Example 3 : Take a linear combination of two different variables. For example, we can\n# multiply your by 2 and x by 3 and add them together to get \n# 2*7 + 3*3 = 13 + 9 = 23 \n\n2*y + 3*x\n\n[1] 23\n\n# Example 4 : Use one variable as an exponent for another variable. In this case, take the \n# variable x and raise it to the power y. In this case, we are computing 3 ^ 7\n# which is 3 * 3 * 3 * 3 * 3 * 3 * 3 = 2187:\n\nx^y\n\n[1] 2187\n\n\n\nAssigning Operations to Variables  We could also take the result from an operation and assign that to a different variable. For exampe, we could multiply x by 7 and multiply y by 2, subtract them and store the result in a new variable z.\n\nz &lt;- 7*x - 2*y\n\n\nIf you want to print out what is now in the variable z, you can now see it in the ENVIRONMENT window. You can also type it out in the script :\n\n# Print out the variable z\n\nz\n\n[1] 7\n\n\n\nRecall from class that there are different kinds of variables we might be asked to consider. We could have QUANTITATIVE data that could be in the form of continuous or discrete values. Another kind of data we talked about was CATEGORICAL data. Depending on the type of data you are analyzing, you would need to perform different operations.\nCreate a vector\nLet’s assume the class takes a quiz and I want to keep track of them. Instead of creating a variable for each individual quiz, I can create one vector that will have all of the scores. \nFor example, if I have the quiz scores 10, 5, 8, 9, 4 then I could create the variables Student_1_Quiz, Student_2_Quiz, etc. It is much easier to create a single variable that holds all of these values. \nWe will use the following command : c( ) \nThe “c” is shorthand for “concatenate” which means to link objects together in a chain or series. \nIf I wanted to create a vector for the quiz scores above, I would create a vector called Quiz1_Scores and assign the variables as follows : \nNotice the order is important. If we want to assign the first student a 10, the second a 5, the third an 8, the fourth a 9, and the fifth a 4, then we would do the following :\n\n# Create a vector and call it \"Quiz1_Scores\" \n\nQuiz1_Scores &lt;- c(10, 5, 8, 9 ,4)\n\n\nNotice how this is represented in the ENVIRONMENT window :\n\n# We can print out the variable to check what it contains\n\nQuiz1_Scores     \n\n[1] 10  5  8  9  4\n\n\nThis tells us the scores are located in spots 1, 2, 3, 4, 5 in the vector. In this vector, the values in the spots are then shown as the values : 10 5 8 9 4 \nLet’s say Student 3 comes to see us and ask about their quiz grade. We can then access the individual value as follows :\n\nQuiz1_Scores[3]\n\n[1] 8\n\n\n\nIf we wanted the fifth value in the vector we could say :\n\nQuiz1_Scores[5]\n\n[1] 4\n\n\n\nIf we wanted to print out the 3rd, 4th, and 5th scores, we could say :\n\nQuiz1_Scores[3:5]\n\n[1] 8 9 4\n\n\n\nObviously we would want to make sure we enter in the data in an appropriate order because if I rearrange the order I get a completely different vector :\n\nQuiz1_Scores_B &lt;- c(5, 10, 4, 9, 8)\n\n\nWhile I have the same scores, they are located in different spots of the vectors and would assign different values to Student 1, Student 2, etc. \nWe could also use characters in our vectors. We would need to make sure we use quotation marks so the compiler does not think we are using other variables in our vector.\n\nStudents &lt;- c(\"Alice\", \"Bob\", \"Chad\", \"Debbie\", \"Eric\")\n\nCheck out what is now in the ENVIRONMENT window :\n\n# Print out the Students vector :\n\nStudents\n\n[1] \"Alice\"  \"Bob\"    \"Chad\"   \"Debbie\" \"Eric\"  \n\n\n\nThe only real difference from above is the we are using characters instead of numeric values and that is identified because of the “chr” notation in the description. \nWe can look at the individual entries jsut as we did above. To look at the fourth entry in the vector we would type :\n\nStudents[4]\n\n[1] \"Debbie\"\n\n\n\nIf you are given a variable or vector and want to know what type of values it contains, you can use the “class” command to tell you. Here are some examples to check entire vectors or individual locations in a vector :\n\nclass(Quiz1_Scores)\n\n[1] \"numeric\"\n\nclass(Quiz1_Scores[2])\n\n[1] \"numeric\"\n\nclass(Students)\n\n[1] \"character\"\n\nclass(Students[1])\n\n[1] \"character\"\n\nclass(Students)\n\n[1] \"character\"\n\n\n\nWhat happens if we mix our variables and have numbers and characters in the same vector? \nHere is how one could be created :\n\nblah &lt;- c(4, \"dffdg\", 6, 9, \"trte\")\n\n\nHow does R interpret these values?\n\nclass(blah)\n\n[1] \"character\"\n\n\n\nNotice that it considers EVERY entry in this vector to be a CHARACTER even though we entered some as numbers. Be careful with this as it could cause issues on how we work and interact with this vector.\nLibraries and Packages \nWhen we start up a session of R, there are some commands that are already built into the program that we can use. For instance, we used the basic mathematics operations above. \nYou will eventually want to do a deeper analysis of the data that needs a command that is not already installed in your current session of R. This is where the idea of Libraries or Packages comes into play. \nIf there is a command we want to use that is not currently loaded into R, we can install the package that includes the command. \nYou can see what is loaded already by clicking on the PACKAGES tab. \nYou will see the packages that are loaded up as they will have a check mark indicated they have been installed. \nLet’s say there was something in the “tcltk” library I wanted to use. I could then click on the check box for “tcltk” and a message should come up in the Console showing that the package was installed.\n&gt; library(tcltk, lib.loc = “/opt/R/4.3.1/lib/R/library”) \nWe could remove the package by unclicking on the check box. We get a confirmation in the Console :\n&gt; detach(“package:tcltk”, unload = TRUE) \nWhat happens if we need a package that is not included in the list. There are hundreds of packages that people have developed to use in R. \nFor example, consider the tidyverse library. This is a library that contains several commands that we will be using over the semester. \nIf we know we are going to be using a specific library in our R script, then we should install it at the top of the script. We would enter in a command such as :  install.packages(“tidyverse”) \nOnce we do this, you will see several different commands that are now available for us to use to analyze our data set. \nIf you look at the Packages tab, you will see several new packages that we can add to use in the program. \ntidyverse comes with several packages. If you click on the tidyverse package, you will see several packages installed :\n&gt; library(tidyverse)\n── Attaching core tidyverse packages ────────────────────── tidyverse 2.0.0 ──\n✔ dplyr 1.1.2 ✔ readr 2.1.4\n✔ forcats 1.0.0 ✔ stringr 1.5.0\n✔ ggplot2 3.4.2 ✔ tibble 3.2.1\n✔ lubridate 1.9.2 ✔ tidyr 1.3.0\n✔ purrr 1.0.1\n\nThese packages are now loaded into R. Note that is we uncheck the tidyverse package, these are still loaded into R. We can remove them by unchecking their package. For example, if I uncheck the ggplot2 package you will see the following message : \n\ndetach(“package:ggplot2”, unload = TRUE) \n\nWhen you are starting to become a programmer it might be tempting just to load up EVERYTING, but that is not good practice. When these packages are loaded up, they are taking up memory. This can lead to slower computation times as well as lead to larger files being generated, wasting space. Try to be efficient and load up what you need and avoid bloat.\nFunctions \nThere are two kinds of functions we are going to deal with in class - the ones that are built into R and ones we create ourselves. This lesson will only consider the functions that are built in. \nA FUNCTION is a command that takes in some kind of data, manipulates it, and returns a value. It has the following form : \nFUNCTION_NAME( data ) \nThis will simply print out the result. Remember we could also assign the result to a variable.\nresult &lt;- FUNCTION_NAME ( data)\nFor example, go back to the quiz grades we had listed earlier. What if I wanted to calculated the average (mean) of the quiz scores? I could enter the following :\n\nmean(Quiz1_Scores)\n\n[1] 7.2\n\n\n\nI could assign the result to a variable, such as :\n\n# Calculate the average\n\nQuiz1_Average &lt;- mean(Quiz1_Scores)\n\n# Print out the average\n\nQuiz1_Average\n\n[1] 7.2\n\n\n\nThere are several other built in functions. Here are a few : \nmin( ), max( ), mean( ), median( ), sum( ), range( ), abs( ) \nIf we wanted the highest quiz grade, we could say :\n\n# Find the maximum value :\n\nmax(Quiz1_Scores)\n\n[1] 10\n\n\n\nIf I wanted to know how many values are in my vector, I could say :\n\n# Use the length function :\n\nlength(Quiz1_Scores)\n\n[1] 5\n\n\n\nIf I wanted to pick a random number from 1 - 100, I could type the following\n\nsample(100,1)\n\n[1] 88\n\n\n\nIf I wanted to pick three random numbers (all different) from 1 - 100, we could say this :\n\nsample(100,3)\n\n[1] 22  4 87\n\n\n\nIf I wanted to pick seven random numbers from 1 - 100 where we could have (but not guarantee) duplicate values, I could say this :\n\nsample(100, 7, replace=TRUE)\n\n[1]  2 87 79 15 17 58 21\n\n\n\nTo create a random vector for a range of values, we can use sample function. We just need to pass the range and the sample size inside the sample function. \nFor example, if we want to create a random sample of size 20 for a range of values between 1 to 100 then we can use the command sample(1:100,20) and if the sample size is larger than 100 then we can add replace=TRUE as shown in the below examples.\n\n# Create random sample of 20 values from 1 - 100\n\nx1 &lt;- sample(1:100,20)\n\n# Print out the result\n\nx1\n\n [1]  25  96  81  53  28 100   1  89  36  24  85  90   8  43  42  71   4  32  72\n[20]  49\n\n\n\n\n# Create a sample of 200 values from 1 - 100. Obviously there will be repeats!\n\nx2 &lt;- sample(1:100,200,replace=TRUE)\n\n# Print out the sample\n\nx2\n\n  [1]  46  35  56  49  58  24  16  93  29  96  47  75  97  84   7  30  85  98\n [19]   8   5  29  67  34  18  69  93  36  29  88  39  54  17   2  32  19  31\n [37]  41  86  64  47  79  17  43  42   2  93  42  87   1  92  11  77  43  15\n [55]  63  93  94  12  77  94  82  73  65  60  96  38  48  96  56  67  56  67\n [73]  95  59  95  64  31  88  44  17  82  76  42  12   6   6  56  34  32  84\n [91]  67  93  30  23  42  12  27  45   6  17  78  30  52  99  16  47  21  85\n[109]  82  73  48   4  76  20   9  17  30  34  58  83 100  90  47  99  80  87\n[127]  21  14  62  24  74  51  21  46  40  29  70  28  96  71  38  23  22  79\n[145]  23  22  25 100   3  97  58  68  50  57  16  65  27  95  10  23  51  20\n[163]   2  56  32  84  67  66  96  10  59  60 100  84  59  37  13  57  62  68\n[181]  91  88  20  21  83  44  20  49  89  61  24  21  59  11  82  46  40  59\n[199]  30  13\n\n\n\nThere are far too many built in functions to list. You may have to use a book or The Google to help you find an appropriate one to use. \nLastly, make sure that the data you enter into the function makes sense. For example, what if I tried to find the average of the names of the students we put into the vector “Students” :\n\nmean(Students)\n\nWarning in mean.default(Students): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\n\nYou will see that this returns an error : \n[1] NA\nWarning message:\nIn mean.default(Students) :\nargument is not numeric or logical: returning NA\nPRACTICE PROBLEMS\n\nCalculate the following, printing the outputs to the console :\n\n\n284 * 14563\n895 - 427865\n87267 / 3578\n7 raised the the 3rd power\nthe square root of 97\n\n\n# Responses\n\n\nAssign the following to the vector x1 : 125 + 15 `# Print out x1 to the console to check your work.\n\n\n# Response\n\n\nAssign the sentence “Transylvania Pioneers - 2023 national Champions in Womens Basketball” to the variable Sentence-1. Print out Sentence-1 to check your work.\n\n\n# Response\n\n\nCreate a vector called “Friends” that has the names of your friends. Add 7 names to the list. Check your vector by printing it out.\n\n\n# Response\n\n\nPrint out the 3rd name in the Friends vector.\nPrint out the Friends names in positions 4 - 7\n\n\n# Response\n\n\nCreate a vector called “Family” and another vector called Ages” that has the ages of your family. Make sure the locations of the ages match up with the Family vector. Check your vector by printing it out.\n\n\n# Response\n\n\nFor each of the previous four vectors (x1, Sentence-1, Friends, Ages), determine the CLASS of each vector.\n\n\n# Response\n\n\nInstall the package “hms”\n\n\n# Response\n\n\nCreate a vector of 50 random numbers between 200 - 700 without replacement and store it in a vector called Random1. Print out the values of the vector.\n\n\n# Response\n\n\nCreate a vector of 50 random numbers between 30 - 50 with replacement and store it in a vector called Random2. Print out the values\n\n\n# Response\n\n\nFind the average of the values in Random1 and Random 2 (Avg1 and Avg2) and print them out\n\n\n# Response\n\n\nWrite a single line of code that will create a vector named R1 that will select a random number of values (from 1 to 100) from a range of numbers from 500 - 700.\n\n\nUse the length function to determine how many values got selected.\nFind the average of the values.\n\n\n# Response\n\n\nCreate three more vectors (R2, R3, R4) with vectors of size 10, 1000, 100000 that will select a sample from a range of numbers from 500 - 700.\n\n\nFind the average values for R2, R3, R4.\nRepeat this process 2 more times for R2, R3, R4\n\n\n# Response\n\n\nThink about picking numbers at random from 500 - 700. If you take the average of these values, what number do you think should you be close to? Why?\n\n\n# Response\n\n\nLook at your repeated averages. What were the ranges of scores for the samples of size 10? size 1000? size 100000?\n\n\n# Response\n\n\nBased on this data, what do you think will happen to the average if we take even larger samples? Why?\n\n\n# Response",
    "crumbs": [
      "Appendices",
      "Individual Lab 1"
    ]
  },
  {
    "objectID": "Individual_Lab_2_ Read_In_Data_Assignment.html",
    "href": "Individual_Lab_2_ Read_In_Data_Assignment.html",
    "title": "Individual Lab 2",
    "section": "",
    "text": "We will be working with a database from the Star Wars universe. We need to first install the package : \ninstall.packages(“starwarsdb”)\n\n# Response\n\n\nPlease make note that this is a different kind of way we can enter data into R. We have entered what is called a “database” which is different from a text file, a dataframe, a tibble, etc. \nWe will need to use the “dplyr” library to access this data, so install it if it is not already installed. \ninstall.packages(“dplyr”)\n\n# Response\n\n\ndplyr should now be installed, but may not be loaded. You can load the library by checking it off in the packages tab, or you can enter the following :\nlibrary(dplyr)\n\n# Response\n\n\nWe are now ready to create a dataframe using a combination of “dplyr” and the “as.data.frame” command. This is similar to the “as.tibble” command used above. Let’s store the information in the variable starwars_DF : \nstarwars_DF &lt;- as.data.frame(dplyr::starwars)\n\n# Response\n\n\nCheck the variable to make sure it is a dataframe.\nclass(starwars_DF)\n\n# Response\n\n\nNow we are ready to start answering questions.\n\nUse two different methods for finding the number of rows in your data frame. What does this represent?\n\n\n# Response\n\n\n\nUse two different methods for finding the number of columns in your data frame. What does this represent?\n\n\n# Response\n\n\n\nDetermine the dimensions of the dataframe using the dim command. What does this tell us?\n\n\n# Response\n\n\n\nWhat command can we use to list off the variable names in the dataframe? Carry out that command.\n\n\n# Response\n\n\n\nWe want to pull out the “height” variable. Carry out two different commands that will pull out this information and save it to a new variable called SW_height. How many values are listed? Does this raise any concerns?\n\n\n# Response\n\n\n\nDo all of the observations have a value for height? If not, how many are missing and who are they missing from?\n\n\n# Response\n\n\n\nPrint out the heights from smallest to largest. Also save the sorted list to a new variable called SW_height_sorted_1\n\n\n# Response\n\n\n\nUse the sort help file to see how to sort the values from largest to smallest. Then print out the heights from largest to smallest and save new sorted list to SW_height_sorted_2\n\n\n# Response\n\n\n\nFind two ways to find the maximum values of the height. Save this value to the variable SW_max_height.\n\n\n# Response\n\n\n\nFind two ways to find the minimum values of the height. Save this value to the variable SW_max_height.\n\n\n# Response\n\n\n\nTurn this dataframe into a tibble named SW_tibble.\n\n\n# Response",
    "crumbs": [
      "Appendices",
      "Individual Lab 2"
    ]
  },
  {
    "objectID": "Individual_Lab_3_Review.html",
    "href": "Individual_Lab_3_Review.html",
    "title": "Individual Lab 3",
    "section": "",
    "text": "This lab will review the following concepts:\n\nReading in a data set\n\n\nMake sure the datasets library is installed and loaded up.\nWe will be using the “iris” data set.\n\n\nUse ggplot to create graphs by :\n\n\nChanging aesthetics\nAdding labels\nUsing Facets\n\n\nPulling variables from a data set\nFinding the maximum and minimum values of a variable\nFinding the mean and median of the variables.\nComparing values from the species to the entire data set\nTurning a Data Frame into a Tibble \n\nStep 1 : Install / load the “tidyverse” package\n\n# Response\n\n\nStep 2 : Install / load the “datasets” package. Examine the package and determine how many datasets are in the package.\n\n# Response\n\n\nStep 3 : Copy the dataset “iris” into the variable “iris_data”. Print out iris_data to make sure it is correct.\n\n# Response\n\n\nWe will now start to create a plot one step at a time : \nStep 4 : Create a graph using Sepal Length as the x axis and Sepal Width as the y axis, but no points are plotted.\n\n# Response\n\n\nStep 5 : Add the data points using basic dots.\n\n# Response\n\n\nStep 6 : Differentiate the dots by coloring the by their species.\n\n# Response\n\n\nWe need to start adding labels. \nStep 7 : Add the title “Sepal Length vs Sepal Width”\n\n# Response\n\n\nStep 8 : Add a subtitle “Wowsers”\n\n# Response\n\n\nStep 9 : Add better looking labels on the x and y axis. Rename the x axis “Sepal Length” and the y axis “Sepal Width”\n\n# Response\n\n\nStep 10 : Add a caption at the bottom of the graph that says “Source : Iris Data Set”\n\n# Response\n\n\nStep 11 : Change the shape of the dots by species\n\n# Response\n\n\nStep 12 : Change the size of the dots by Petal Length\n\n# Response\n\n\nStep 13 : Create a facet_grid for each species using Species ~ Petal.Length\n\n# Response\n\n\nStep 14 : Create a facet wrap. Describe the patterns you see. Is it random? Do the facets follow any kind of a pattern? Linear? Quadratic?\n\n# Response\n\n\nStep 15 : Let’s describe each species values by finding the maximum and minimum values of the Sepal Length, Sepal Width, Petal Length, and Petal Width, and the means and medians of each species and comparing that to the mean of the entire data set. \nFind the mean and median of the variables using two different techniques : \n\nPulling out the data from the data set, storing it in a different variable name, and then using the mean command. \n\nEx : name_1 &lt;- dataset[,3]  name_2 &lt;- mean(name_1)\n\n# Response\n\n\n\nUse the mean and median command directly on the variable in the data set.  Ex : name_3 &lt;- mean(dataset$variable_name)\n\n\n# Response\n\n\n\nCompare the means of the species to the mean of the entire data set. Which variables had values above the mean? Which had values below the mean? \n\nFor example, did the variable Sepal Length of the species virginica have a value larger or smaller than the Sepal Length of the entire data set?\n\n# Response\n\n\n\nDetermine the maximum and minimum values for each of the variables for the four species.\n\n\n# Response\n\n\nTurn the iris_data dataframe into a tibble. Verify your result.\n\n# Response",
    "crumbs": [
      "Appendices",
      "Individual Lab 3"
    ]
  },
  {
    "objectID": "Individual_Lab_4.html",
    "href": "Individual_Lab_4.html",
    "title": "Individual Lab 4.",
    "section": "",
    "text": "Dataset : okcupid_profiles.csv \nThe data consists of the public profiles of 59,946 OkCupid users who were living within 25 miles of San Francisco, had active profiles during a period in the 2010s, were online in the previous year, and had at least one picture in their profile. Using a Python script, data was scraped from users’ public profiles four days later; any non-publicly facing information such as messaging was not accessible. \nFor a complete list of variables and more details, see the accompanying codebook  okcupid_codebook.txt .\nInstructions : You are expected to comment throughout your R script, writing out the question, your analysis, and answer any questions that were asked. For your visualizations, you are expected to try several different variations until you find one that you feel best answers the question. Do not settle for a basic graphic. Add labels, change colors, add textures, etc.\n\n\nLoad the CSV data set into a variable called “Cupid1”. \n\n\n# Response 1\n\n\n\nHeight is one of three numerical variables in the data set. What are the other two?\n\n\n# Response 2\n\n\n\nInvestigate the overall heights of the individuals using the favstats( ) function. It can be found in the mosaic package, if it is not loaded up. (Don’t forget the quotation marks about the name of the package!) Here is the syntax :\n\n\nfavstats( ~ variable_name, data = data_set)\n\n\nSo if we wanted to investigate the age variable in Cupid1, we would type :\n\nfavstats( ~ age, data = Cupid1)\n\n\nwhich should lead to something like this :\n\n\n\n\n\n# Response 3\n\n\n\nDo a similar investigation for the heights variable. Briefly explain your findings. Do you have any concerns about the results? \n\n\n# Response 4\n\n  If we think there are issues with some of the data, then it is possible to remove some of the data points from our set. For example, assume we have a data set where we want to only use the IQ scores between 100 and 150. We can use the filter( ) command to do this for us. This command comes from the dplyr package. \nWe could use the following command to filter our data set :\n\nNew_Data &lt;- filter(Old+Data, IQ &gt;+100 & IQ &lt;=150)\n\n\n\nUsing the filter( ) command, create a new data set called “Cupid2” that only uses heights between 55 and 80 from Cupid1. Does the new data set only have the heights or did it keep all the other variables? \n\n\n# Response 5\n\n\n\nHow do the heights of female and male OkCupid users compare? - Create a histogram for the heights variable in Cupid2 using gplot. - Create a boxplot for female and male heights using ggplot. (Hint : If needed, use the fiklter( ) command to create two new variables, “Female_Data” and “Male_Data”.) - Create a graph with both boxplots at the same time. (Hint :  Here ) - Add a stripchart( ) to the boxplots. Is this useful? Explain. \n\n\n# Response 6\n\n  Let’s look at a different way to make histograms. The histograms we made in class allowed us to create a plot but were limited to only creating the histogram from the point of view of a single variable. It would be nice to create a histogram that can have a single variable broken into different parts based on a secondary variable. For instance, can we create a histogram that will take all of the heights and break them into male and female groups for us? (Yes, it may be possible to do this with the ggplot( ) command, but this is a cool way, too.) \nWe compare the distributions of male and female heights using histograms. While we could plot two separate histograms without regard to the scale of the two axes, we instead use the histogram( ) function from the mosaic package to:\n\nPlot heights given sex by defining the formula:\n\n~ height | sex.\n\n\nPlot them simultaneously in a lattice consisting of two rows and one column of plots by setting\n\nlayout = c(1,2)\n\n\nPlot them with bin widths matching the granularity of the observations (inches) by setting\n\nwidth = 1\n\n\n\nThe histogram( ) function automatically matches the scales of the axes for both plots.\n\n\n\n\n\nUsing a variation of the command above, create and customize a histogram each for female and male heights. \n\n\n# Response 7\n\n\nThese first exercises stress many important considerations students should keep in mind when working with real data. Firstly, it emphasizes the importance of performing an exploratory data analysis to identify anomalous observations and confronts students with the question of what to do with them. For example, while a height of 1 inch is clearly an outlier that needs to be removed, at what point does a height no longer become reasonable and what impact does the removal of unreasonable heights have on the conclusions? In our case, since only a small number of observations are removed, the impact is minimal. \nSecondly, this exercise demonstrates the power of simple data visualizations such as histograms to convey insight and hence emphasizes the importance of putting careful thought into their construction. In our case, while having students plot two histograms simultaneously in order to demonstrate that males have on average greater height may seem to be a pedantic goal at first, we encouraged students to take a closer look at the histograms and steered their focus towards the unusual peaks at 72 inches (6 feet) for males and 64 inches (5’4”) for females. Many of the students could explain the phenomena of the peak at 72 inches for men: sociological perceptions of the rounded height of 6 feet. On the other hand, consensus was not as strong about perceptions of the height of 5’4” for women.",
    "crumbs": [
      "Appendices",
      "Individual Lab 4."
    ]
  },
  {
    "objectID": "Individual_Lab_5.html",
    "href": "Individual_Lab_5.html",
    "title": "Individual Lab 5",
    "section": "",
    "text": "Correlation and Regression Lines\nInstructions : You have been hired as a data analyst for the 2024 WNBA champion New York Liberty. The first task that you are being assigned is to try to determine what the team needs to improve in order to increase the number of wins from last season. Some people might think that points scored should obviously be the statistics that determines if a team is going to win, but is that true? In the past season the Liberty were second in the league in points scored, but is that what really helped them win games? Was it something on defense that helped more than points scored?\nYour task is to investigate team statistics from the 2023 - 2024 season and try to determine which variables affect a team’s winning percentage (found here). You can find team statistics here.\nThis site has team offensive (Team) and defensive (Opponent) statistics. You are to analyze which of these statistics would be a good predictor for winning percentage (or games won).\nThis could lead to good information for the front office. For example, if you discover that the percentage of 3-pointers made is a good indicator for winning percentage that tells us that we need to go out and get players that can make these shots. Perhaps your analysis would say that defensive rebounds is a good indicator for winning percentage, in which case we would want to go out and sign a free agent that has good rebounding statistics.\nYou are being asked to complete the following :\nNote that the size of the text is 2 to make it easier to read. You may want to zoom in on the values.",
    "crumbs": [
      "Appendices",
      "Individual Lab 5"
    ]
  },
  {
    "objectID": "Individual_Lab_5.html#correlation-and-regression-lines",
    "href": "Individual_Lab_5.html#correlation-and-regression-lines",
    "title": "Individual Lab 5",
    "section": "",
    "text": "Download the statistics into a Google Sheet. You can find them under the FILES section on Canvas called, “2024 WNBA Stats.csv”\nExport the Google Sheet as a Comma-Separated Values (csv) file and upload it to Posit Cloud.\nLoad the data into the variable “df”. Check to make sure that the data is loaded properly.\nConstruct a scatterplot comparing the variable Win Percentage to Free Throws Made by the Opponent. Describe the scatterplot, find the correlation between the two variables and explain what the data is implying.\nThe command cor(DATAFRAME_NAME) will return a table with all of the variables correlated against each other. However, what happens when you run the command cor(df) ? What is the error message and what could be the problem?\nIf we want to calculate several correlations at the same time, we will need to remove the column with Team names. Create a new variable called df2 that will have this column removed. Check and verify that the new variable is correct.\nWe can now run the command cor(df2). Do this and save the result to the variable cor_values.\nReferring back to the script we used in class when talking about Advanced Correlation Techniques.R\n\n\nInstall and load the package PerformanceAnalytics.\nRun the command chart.Correlation(df2, histogram=TRUE, pch=19)\n\nNote this may take 10 - 15 seconds, so be patient!\n\nIs this information useful? Why or why not?\n\n\nAgain referring back to the script, install and load the package corrplot. This is a different command that we can use to plot the correlations. Carry out this command by typing, corrplot(cor(df2))\n\n\nExplain the information given to you in this plot.\nIs this more useful than the previous plot? Explain.\nUsing this plot, see if you can determine three variables that have a high positive and high negative correlation. What are the variables? Note that this does not give the values.\n\n\nFor our last visualization, refer one final time to the script and use ggcorrplot as follows:\n\n\nggcorrplt(cor(df2)) +\n  geom_text( aes(Var2, Var1, label = value), color=\"white\", size=2)\n  \n\n\nWe can now see which values we should use for our analysis. It is probably clear that Points Scored by our team, Field Goals Made for our team, and Field Goals Percentage are obviously going to affect the Winning Percentage. We want to dig deeper than the obvious choices. Without using these three, find the three variables with the strongest positive and negative correlations, when compared to Winning Percentage. What are they? What are their values?\nWhat are the two variables that affect the winning percentage the least?\nFor each of these variables, examine scatterplots, residual plots, correlation, and regression lines. Make predictions on what could happen to the winning percentage if these explanatory values go up or down. How far would they have to increase or decrease to make a significant change to the winning percentage.\nIf you were going to report to the GM, what attributes they should look for in players to sign to the team to improve winning percentage, as well as those that should not affect winning percentage. Explain to them the information that you found.\nShare your report with me via Google Docs or however the GM requests.",
    "crumbs": [
      "Appendices",
      "Individual Lab 5"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html",
    "href": "Individual_Lab_6.html",
    "title": "Individual Lab 6",
    "section": "",
    "text": "Data Cleaning\nInstructions : In this lab we are going to work on how to properly clean a data set. There are some very advanced techniques one can use to clean a data set. However, we are just getting started so we will be looking at some of the basic ideas to think about when confronted with a data set that is “dirty.” We will cover the following topics in this lesson :\nWe will also be using a few commands that we have not gone over in class. Please make sure to read about these commands so you know what they are supposed to do for you. Don’t just type in a command without knowing and thinking about what is supposed to happen.\nYour job is to follow along and enter in the commands as shown. Please go slow and write out any questions or notes as you need them.\nFirst, let’s create a data set to clean up. Enter this in EXACTLY to make sure you get the same results as my code. Cutting and pasting this is probably easiest :\n# Create example data frame\n\ndata &lt;- data.frame(x1 = c(1:4, 99999, 1, NA, 1, 1, NA),\n                   x1 = c(1:5, 1, \"NA\", 1, 1, \"NA\"),\n                   x1 = c(letters[c(1:3)], \"x x\", \"x\",\" y y y\", \"x\", \"a\", \"a\", NA),\n                   x4 = \"\",\n                   x5 = NA)\n# Print example data frame\n\ndata\n\n      x1 x1.1   x1.2 x4 x5\n1      1    1      a    NA\n2      2    2      b    NA\n3      3    3      c    NA\n4      4    4    x x    NA\n5  99999    5      x    NA\n6      1    1  y y y    NA\n7     NA   NA      x    NA\n8      1    1      a    NA\n9      1    1      a    NA\n10    NA   NA   &lt;NA&gt;    NA",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#data-cleaning",
    "href": "Individual_Lab_6.html#data-cleaning",
    "title": "Individual Lab 6",
    "section": "",
    "text": "Skill 1: Modify Column Names\nSkill 2: Format Missing Values\nSkill 3: Remove Empty Rows & Columns\nSkill 4: Remove Rows with Missing Values\nSkill 5: Remove Duplicates\nSkill 6: Modify Classes of Columns\nSkill 7: Detect & Remove Outliers\nSkill 8: Remove Spaces in Character Strings\nSkill 9: Combine Categories",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-1---modify-column-names",
    "href": "Individual_Lab_6.html#skill-1---modify-column-names",
    "title": "Individual Lab 6",
    "section": "Skill 1 - Modify Column Names",
    "text": "Skill 1 - Modify Column Names\nYou can examine the names of the columns using the colnames( ) command :\n\n# Examine column names\n\ncolnames(data)\n\n[1] \"x1\"   \"x1.1\" \"x1.2\" \"x4\"   \"x5\"  \n\n\n Let’s assume that we want to change these column names to a consecutive range with the prefix “col”. In other words, col1, col2, col3, col4, and col5. Then, we can apply the colnames, paste0, and ncol functions as shown below: \n\n# Modify column names\n\ncolnames(data) &lt;- paste0(\"col\", 1:ncol(data))\n\n# Print the updated data frame\n\ndata\n\n    col1 col2   col3 col4 col5\n1      1    1      a        NA\n2      2    2      b        NA\n3      3    3      c        NA\n4      4    4    x x        NA\n5  99999    5      x        NA\n6      1    1  y y y        NA\n7     NA   NA      x        NA\n8      1    1      a        NA\n9      1    1      a        NA\n10    NA   NA   &lt;NA&gt;        NA\n\n\n\nAs yo ucan see above, he previous syntax has created an updated version of our data frame where the column names have been changed.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-2---format-missing-values",
    "href": "Individual_Lab_6.html#skill-2---format-missing-values",
    "title": "Individual Lab 6",
    "section": "Skill 2 - Format Missing Values",
    "text": "Skill 2 - Format Missing Values\nA typical problem for each data preparation and cleaning task are missing values. \nIn the R programming language, missing values are usually represented by NA. For that reason, it is useful to convert all missing values to this NA format. \nIn our specific example data frame, we have the problem that some missing values are represented by blank character strings. \nWe can print all those blanks to the RStudio console as shown below:\n\n# Print all blank character strings\n\ndata[data == \"\"]\n\n [1] NA NA NA \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" NA NA NA NA NA NA NA NA NA NA\n\n\n\nIf we want to assign NA values to those blank cells, we can use the following syntax:\n\n# Format missing values by replacing blank character strings with NA\n\ndata[data == \"\"] &lt;- NA\n\n# Print the updated data frame\n\ndata\n\n    col1 col2   col3 col4 col5\n1      1    1      a &lt;NA&gt;   NA\n2      2    2      b &lt;NA&gt;   NA\n3      3    3      c &lt;NA&gt;   NA\n4      4    4    x x &lt;NA&gt;   NA\n5  99999    5      x &lt;NA&gt;   NA\n6      1    1  y y y &lt;NA&gt;   NA\n7     NA   NA      x &lt;NA&gt;   NA\n8      1    1      a &lt;NA&gt;   NA\n9      1    1      a &lt;NA&gt;   NA\n10    NA   NA   &lt;NA&gt; &lt;NA&gt;   NA\n\n\n\nAnother typical problem with missing values – that also occurs in our data set – is that NA values are formatted as the character string “NA”. \nLet’s have a closer look at the column col2: \n\n# Print all cells of column col2\n\ndata$col2\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"1\"  \"NA\" \"1\"  \"1\"  \"NA\"\n\n\n\nAs you can see in the previous output, the NA values in this column are shown between quotes (i.e. “NA”). This indicates that those NA values are formatted as characters instead of real NA values. \nWe can change that using the following R code:\n\n# Format missing values by replacing \"NA\" character strings with NA\n\ndata$col2[data$col2 == \"NA\"] &lt;- NA\n\n# Print the updated data frame\n\ndata\n\n    col1 col2   col3 col4 col5\n1      1    1      a &lt;NA&gt;   NA\n2      2    2      b &lt;NA&gt;   NA\n3      3    3      c &lt;NA&gt;   NA\n4      4    4    x x &lt;NA&gt;   NA\n5  99999    5      x &lt;NA&gt;   NA\n6      1    1  y y y &lt;NA&gt;   NA\n7     NA &lt;NA&gt;      x &lt;NA&gt;   NA\n8      1    1      a &lt;NA&gt;   NA\n9      1    1      a &lt;NA&gt;   NA\n10    NA &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;   NA\n\n\n\nNote that the NA in columns 1 and 5 are different than the ones in columns 2, 3, and 4. Why do you think that is?",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-3---remove-empty-rows-columns",
    "href": "Individual_Lab_6.html#skill-3---remove-empty-rows-columns",
    "title": "Individual Lab 6",
    "section": "Skill 3 - Remove Empty Rows & Columns",
    "text": "Skill 3 - Remove Empty Rows & Columns\nExample 3 demonstrates how to identify and delete rows and columns that contain only missing values. \nOn a side note: Example 2 was also important for this step, since the false formatted NA values would not have been recognized by the following R code. \nThe syntax below demonstrates how to use the rowSums, is.na, and ncol functions to remove only-NA rows:\n\n# Remove rows with only missing values\n\ndata &lt;- data[rowSums(is.na(data)) != ncol(data), ]\n\n# Print the updated data frame\n\ndata\n\n   col1 col2   col3 col4 col5\n1     1    1      a &lt;NA&gt;   NA\n2     2    2      b &lt;NA&gt;   NA\n3     3    3      c &lt;NA&gt;   NA\n4     4    4    x x &lt;NA&gt;   NA\n5 99999    5      x &lt;NA&gt;   NA\n6     1    1  y y y &lt;NA&gt;   NA\n7    NA &lt;NA&gt;      x &lt;NA&gt;   NA\n8     1    1      a &lt;NA&gt;   NA\n9     1    1      a &lt;NA&gt;   NA\n\n\n\nComparing the latest table with the previous one, you can see that the rows with only missing values have been removed. In other words, the last row in the previous example has been deleted. \nSimilar to that, we can also exclude columns that contain only NA values:\n\n# Remove columns with only missing values\n\ndata &lt;- data[, colSums(is.na(data)) != nrow(data)]\n\n# Print the updated data frame\n\ndata\n\n   col1 col2   col3\n1     1    1      a\n2     2    2      b\n3     3    3      c\n4     4    4    x x\n5 99999    5      x\n6     1    1  y y y\n7    NA &lt;NA&gt;      x\n8     1    1      a\n9     1    1      a\n\n\n\nThis command removed columns 4 and 5, leaving us with only the first three columns.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-4---remove-rows-with-missing-values",
    "href": "Individual_Lab_6.html#skill-4---remove-rows-with-missing-values",
    "title": "Individual Lab 6",
    "section": "Skill 4 - Remove Rows with Missing Values",
    "text": "Skill 4 - Remove Rows with Missing Values\nAs you can see in the previously shown table, our data still contains some NA values in the 7th row of the data frame. \nIn this example, I’ll explain how to delete all rows with at least one NA value. \nThis method is called listwise deletion or complete cases analysis, and it should be done with care! Statistical bias might be introduced to your results, if data is removed without theoretical justification. \nHowever, in case you have decided to remove all rows with one or more NA values, you may use the na.omit function as shown below:\n\n# Remove rows with missing values\n\ndata &lt;- na.omit(data)\n\n# Print the updated data frame\n\ndata\n\n   col1 col2   col3\n1     1    1      a\n2     2    2      b\n3     3    3      c\n4     4    4    x x\n5 99999    5      x\n6     1    1  y y y\n8     1    1      a\n9     1    1      a\n\n\n\nIn the previous output, you can see that the 7th row has been removed from the data frame.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-5---remove-duplicates",
    "href": "Individual_Lab_6.html#skill-5---remove-duplicates",
    "title": "Individual Lab 6",
    "section": "Skill 5 - Remove Duplicates",
    "text": "Skill 5 - Remove Duplicates\nIn this example, we will see how to keep only unique rows in a data frame. \nFor this task, we can apply the unique( ) function to the data frame.\n\n# Remove duplicates\n\ndata &lt;- unique(data)\n\n# Print the updated data frame\n\ndata\n\n   col1 col2   col3\n1     1    1      a\n2     2    2      b\n3     3    3      c\n4     4    4    x x\n5 99999    5      x\n6     1    1  y y y\n\n\n\nIn the previous table, rows 8 and 9 were copies of row 1, so they were removed.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-6---modify-classes-of-columns",
    "href": "Individual_Lab_6.html#skill-6---modify-classes-of-columns",
    "title": "Individual Lab 6",
    "section": "Skill 6 - Modify Classes of Columns",
    "text": "Skill 6 - Modify Classes of Columns\nThe class of the columns of a data frame is another critical topic when it comes to data cleaning. \nThis example explains how to format each column to the most appropriate data type automatically. \nLet’s first check the current classes of our data frame columns:\n\n# Check the classes of the columns\n\nsapply(data, class)\n\n       col1        col2        col3 \n  \"numeric\" \"character\" \"character\" \n\n\n\nWe can now use the type.convert function to change the column classes whenever it is appropriate:\n\n# Modify classes of columns\n\ndata &lt;- type.convert(data, as.is = TRUE)\n\n# Print the updated data frame\n\ndata\n\n   col1 col2   col3\n1     1    1      a\n2     2    2      b\n3     3    3      c\n4     4    4    x x\n5 99999    5      x\n6     1    1  y y y\n\n\n\nVisually, there’s no difference. \nHowever, if we print the data types of our columns once again, we can see that the first two columns have been changed to the integer class. The character class was retained for the third column.\n\n# Check the classes of the columns\n\nsapply(data, class)\n\n       col1        col2        col3 \n  \"integer\"   \"integer\" \"character\"",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-7---detect-remove-outliers",
    "href": "Individual_Lab_6.html#skill-7---detect-remove-outliers",
    "title": "Individual Lab 6",
    "section": "Skill 7 - Detect & Remove Outliers",
    "text": "Skill 7 - Detect & Remove Outliers\nIn Skill 7, we will demonstrate a different method to detect and delete outliers. We could calculate these using the Five Number Summary alongside the IQR to find the upper and lower fences to determine outliers, but here is a new way. \nPlease note: Outlier deletion is another very controversial topic. Please verify that it is justified to extract the outliers from your data frame. Please have a look at the outlier removal guidelines &lt;a href=“https://statisticsbyjim.com/basics/remove-outliers/”here. \nHowever, one method to detect outliers is provided by the boxplot.stats function. The following R code demonstrates how to test for outliers in our data frame column col1:\n\n# Detect outliers in column col1\n\ndata$col1[data$col1 %in% boxplot.stats(data$col1)$out]\n\n[1] 99999\n\n\n\nThe previous output shows that the value 99999 is an outlier in column col1. This value is obviously much higher than the other values in the column. \nLet’s assume that we have confirmed theoretically that the observation containing this outlier should be removed. Then, we can apply the R code below:\n\n# Remove outliers in column col1\n\ndata &lt;- data[!data$col1 %in% boxplot.stats(data$col1)$out, ]\n  \n# Print the updated data frame\n\ndata\n\n  col1 col2   col3\n1    1    1      a\n2    2    2      b\n3    3    3      c\n4    4    4    x x\n6    1    1  y y y\n\n\n\nIn the previous output, you can see that the row containing the outlier has been removed.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-8---remove-spaces-in-character-strings",
    "href": "Individual_Lab_6.html#skill-8---remove-spaces-in-character-strings",
    "title": "Individual Lab 6",
    "section": "Skill 8 - Remove Spaces in Character Strings",
    "text": "Skill 8 - Remove Spaces in Character Strings\nThe manipulation of character strings is another important aspect of the data cleaning process. \nThis example demonstrates how to avoid blank spaces in the character strings of a certain variable. \nFor this task, we can use the gsub function as demonstrated below:\n\n# Remove spaces in character strings\n\ndata$col3 &lt;- gsub(\" \", \"\", data$col3)\n\n# Print the updated data frame\n\ndata\n\n  col1 col2 col3\n1    1    1    a\n2    2    2    b\n3    3    3    c\n4    4    4   xx\n6    1    1  yyy\n\n\n\nAll blanks in the column col3 have been dropped and only the actual letters have been kept.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Individual_Lab_6.html#skill-9---combine-categories",
    "href": "Individual_Lab_6.html#skill-9---combine-categories",
    "title": "Individual Lab 6",
    "section": "Skill 9 - Combine Categories",
    "text": "Skill 9 - Combine Categories\nExample 9 shows how to merge certain categories of a categorical variable. \nThe following R code illustrates how to group the categories “a”, “b”, and “c” in a single category “a”. \nConsider the R syntax below\n\n# Combine categories\n\ndata$col3[data$col3 %in% c(\"b\", \"c\")] &lt;- \"a\"\n\n# Print the updated data frame\n\ndata\n\n  col1 col2 col3\n1    1    1    a\n2    2    2    a\n3    3    3    a\n4    4    4   xx\n6    1    1  yyy\n\n\n\nWe have created another version of our data frame where the categories “b” and “c” have been replaced by the category “a”. \nThat completes our introduction to Data Cleaning. As you can imagine, there are dozens (hundreds?) of different methods one can use to clean a data set. This lesson goes over some of the most important concepts one needs to understand in cleaning data. The method one uses to do the actual cleaning is important, but it is more important to know what strategies you should use when cleaning a data set. This lesson gives you 9 ideas to think about when encountering a data set. Naturally, there are many more ideas one might have to discover, depending on the data that needs to be cleaned.",
    "crumbs": [
      "Appendices",
      "Individual Lab 6"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html#tabyl",
    "href": "Reading_and_Interpreting_Tables.html#tabyl",
    "title": "Reading and Interpreting Tables",
    "section": "tabyl()",
    "text": "tabyl()\nThe tabyl( ) function from the janitor package is a powerful tool for creating contingency tables. It is a more modern and user-friendly version of the table( ) function. The tabyl( ) function is used to create a contingency table of counts or proportions or both! \n\n# Make sure you install the janitor package if it is not already loaded up\n# and ready to go.\n\n# install.packages(\"janitor\")\n\nlibrary(janitor)\n\nWe previously used this command to create a frequency table for the Gender variable in the supermarket data set.\n\ntable(supermarket$Gender)\n\n\nWe can use the tabyl() function to create the same table, along with the percentages. \n\nt1 &lt;- tabyl(supermarket, Gender)\n\nt1\n\n Gender    n   percent\n      F 7170 0.5099936\n      M 6889 0.4900064\n\n\n\nWhile we are here, let’s take a look at the structure we just created, t1.\n\nclass(t1)\n\n[1] \"tabyl\"      \"data.frame\"\n\n\n\nNice! We have created a data frame! This sets us up to more easily manipulate the data.  \nWe can also create a two-way table by adding another variable. For example, let’s create a table that tallies up the variables Marital Status and Gender. \n\n# Here was the code from above:\n\n# table(supermarket$`Marital Status`, supermarket$Gender)\n\n# Here is the code using the tabyl() function:\n\nt2 &lt;- tabyl(supermarket,`Marital Status`, Gender)\n\nt2\n\n Marital Status    F    M\n              M 3602 3264\n              S 3568 3625\n\n\n\nWe can also add percentages to the table using the adorn_percentages( ) function. Since this is a two way table, we need to know if we want the percentages to be calculated by row or by column. \nNote : We will use the pipe operator %&gt;% to chain the functions together. We will go over this in more detail in the Beginning Data Cleaning section. \n\n# Here is the default command. the %&gt;% bascially means to take t2 and \n# send it to the next function, adorn_percentages() :\n\nt2 %&gt;% adorn_percentages()\n\n Marital Status         F         M\n              M 0.5246140 0.4753860\n              S 0.4960378 0.5039622\n\n\n\nThis shows us that the default is to calculate the percentages by row. If we want to calculate the percentages by column, we can use the denominator parameter. \n\nt2 %&gt;% adorn_percentages(denominator = \"col\")\n\n Marital Status        F         M\n              M 0.502371 0.4737988\n              S 0.497629 0.5262012\n\n\n\nWe can also add the counts to the table using the adorn_ns( ) function. \n\nt2 %&gt;% adorn_percentages(denominator = \"col\") %&gt;% adorn_ns()\n\n Marital Status                F                 M\n              M 0.502371 (3,602) 0.4737988 (3,264)\n              S 0.497629 (3,568) 0.5262012 (3,625)\n\n\n\nYou can see from the output that the counts are added to the table parenthetically. \nLastly, we could add the totals (marginals) to the table using the adorn_totals( ) function. \n\nt2 %&gt;% adorn_percentages(denominator = \"col\")  |&gt; adorn_ns()\n\n Marital Status                F                 M\n              M 0.502371 (3,602) 0.4737988 (3,264)\n              S 0.497629 (3,568) 0.5262012 (3,625)\n\n\n\nThis now gives us two different methods to create contingency tables in R. tabyl() is a more modern and user-friendly version of the table() function. It can be incorporated into the dplyr workflow and is a great tool for creating contingency tables. table() is a base R function that is also useful for creating contingency tables. It is a bit more straightforward than tabyl() and is also a good tool for creating contingency tables.",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "Reading_and_Interpreting_Tables.html#table",
    "href": "Reading_and_Interpreting_Tables.html#table",
    "title": "Reading and Interpreting Tables",
    "section": "",
    "text": "Frequencies\nTo produce contingency tables which calculate counts for each combination of categorical variables we can use R’s table( ) function. For instance, we may want to get the total count of female and male customers. \n\ntable(supermarket$Gender)\n\n\n   F    M \n7170 6889 \n\n\n\nIf we want to understand the number of married and single females and male customers we can produce a cross classification table: \n\n# cross classication counts for gender by marital status\n\ntable(supermarket$`Marital Status`, supermarket$Gender)\n\n   \n       F    M\n  M 3602 3264\n  S 3568 3625\n\n\n\nWe can also produce multidimensional tables based on three or more categorical variables. For this, we leverage the ftable( ) function to print the results more attractively. In this case we assess the count of customers by marital status, gender, and location: \n\n# customer counts across location by gender and marital status\n\ntable1 &lt;- table(supermarket$`Marital Status`, supermarket$Gender, supermarket$`State or Province`)\n\n# Remember that the previous command is taking the table we are creating and saving it into\n# a new variable called \"table1\". We will now take this table and evaluate it \n# using the **ftable( )** command.\n\nftable(table1)\n\n       BC   CA   DF Guerrero Jalisco   OR Veracruz   WA Yucatan Zacatecas\n                                                                         \nM F   190  638  188       77      15  510      142 1166     200       476\n  M   197  692  210       94       5  514      108 1160     129       155\nS F   183  686  175      107      30  607      125 1134     164       357\n  M   239  717  242      105      25  631       89 1107     161       309\n\n\n\n\n\nProportions\nWe can also produce contingency tables that present the proportions (percentages) of each category or combination of categories. To do this we simply feed the frequency tables produced by table( ) to the prop.table( ) function. The following reproduces the previous tables but calculates the proportions rather than counts: \n\n# Calculate the percentages of gender categories\n\n# We will first create a new table so we don't accidentally hurt our previous work.\n\ntable2 &lt;- table(supermarket$Gender)\n\n# After saving the output ( new table) to the variable table2, we will now send\n# this table2 to prop.table( ).\n\nprop.table(table2)\n\n\n        F         M \n0.5099936 0.4900064 \n\n\n\nBased on the output, we can see that there are about 51% of respondents saying they are female (F) and about 49% of the respondents saying they are male (M). \nWe could also create a two-way table by adding another variable. For example, let’s create a table that tallies up the variables Marital Status and Gender. \n\n# We shall create a new table (table3) to analyze. \n\ntable3 &lt;- table(supermarket$`Marital Status`, supermarket$Gender)\n\n# We can now create a table of proportions for these variables.\n\nprop.table(table3)\n\n   \n            F         M\n  M 0.2562060 0.2321644\n  S 0.2537876 0.2578420\n\n\n\nWe can interpret this tables as follows :\n\n25.6% of the respondents identify as Female (F) and Married (M)\n23.2% of the respondents identify as Male (M) and Married (M)\n25.3% of the respondents identify as Female (F) and Single (S)\n25.8% of the respondents identify as Male (M) and Single (S) \n\nNote that we can tell ftable( ) how many decimal place to use when reporting the results. For example, go back to table 1. We can combine several commands together into one :\n\nWe want to run prop.table( ) on table 1\nWe want to limit to 3 decimal places\nWe want to round the results\nWe want to take this result and use ftable( ) \n\n\nftable(round(prop.table(table1), 3))\n\n        BC    CA    DF Guerrero Jalisco    OR Veracruz    WA Yucatan Zacatecas\n                                                                              \nM F  0.014 0.045 0.013    0.005   0.001 0.036    0.010 0.083   0.014     0.034\n  M  0.014 0.049 0.015    0.007   0.000 0.037    0.008 0.083   0.009     0.011\nS F  0.013 0.049 0.012    0.008   0.002 0.043    0.009 0.081   0.012     0.025\n  M  0.017 0.051 0.017    0.007   0.002 0.045    0.006 0.079   0.011     0.022\n\n\n\n\n\nMarginals\nMarginals show the total counts or percentages across columns or rows in a contingency table. For instance, if we go back to table3 which is the cross classification counts for gender by marital status: \n\n\n\n\nThe margins are simply the sums of the rows and the columns. For example, if we look at table3, I might want to know, “How many respondents identify as Single?” This is the sum on the last row, 3568 + 3625 = 7,193. Similarly, the amount of those identifying as Married would be 3602 + 3264 = 6,866. We can calculate these values using the margin.table( ) command. \n\n# FREQUENCY MARGINALS\n# row marginals - totals for each marital status across gender\n\nmargin.table(table3, 1)\n\n\n   M    S \n6866 7193 \n\n# This command takes in the table for which we want to find the margins.\n# The second parameter tells us if we want row (1) or column (2) margins.\n\n We can see that this example verifes the values we calculated above. \nWe could also calculate the column margins by changing the second parameter to 2. It is left to you to verify that these values are correct. \n\n# column marginals - totals for each gender across marital status\n\nmargin.table(table3, 2)\n\n\n   F    M \n7170 6889 \n\n\n\nIf we were more interested in proportions / percentage rather than counts, we could use the prop.table( ) command to calculate these proportions. The first example will calculate the row percentages. \n\n# PERCENTAGE MARGINALS\n\n# row marginals - row percentages across gender\n\nprop.table(table3, margin = 1)\n\n   \n            F         M\n  M 0.5246140 0.4753860\n  S 0.4960378 0.5039622\n\n\n\nWe could easily calcuate the column percentages using the following command. \n\n# column marginals - column percentages across marital status\n\nprop.table(table3, margin = 2)\n\n   \n            F         M\n  M 0.5023710 0.4737988\n  S 0.4976290 0.5262012",
    "crumbs": [
      "Step 2 - Data Preparation",
      "Reading and Interpreting Tables"
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#built-in-themes",
    "href": "Advanced_Boxplot_Techniques.html#built-in-themes",
    "title": "Advanced Boxplot Techniques.",
    "section": "Built-In Themes",
    "text": "Built-In Themes\nThere are several different built in themes you could use to clean up your ggplot visializations. You can view several of them here :\nhttps://r-charts.com/ggplot2/themes/ \nHere are a couple of examples for you. Feel free to look them up and play around a bit. We will go back to the first data set we created and mess around with that one.\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s add a few themes to see what happens.\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_dark()\n\n\n\n\n\n\n\n\n\n# Set the seed for reproducibility\n\nset.seed(123)\n\n# Create a data frame\n\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = c(rnorm(100), rnorm(100, mean = 1), rnorm(100, mean = 2))\n)\n# Create a boxplot using ggplot\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_void()\n\n\n\n\n\n\n\n\nYou can even download other themes. You can find several online at the link given above. Here we will download, install, and use the hrbrthemes package.\n\n# install.packages(\"hrbrthemes\")\nlibrary(hrbrthemes)\n\n# This package loads up 8 themes. We will use the theme \"theme_ipsum()\"\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_ipsum()",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Advanced_Boxplot_Techniques.html#additional-themes",
    "href": "Advanced_Boxplot_Techniques.html#additional-themes",
    "title": "Advanced Boxplot Techniques.",
    "section": "Additional Themes",
    "text": "Additional Themes\nThere is a package we can install that contains additional themes. This is the ggthemes package.\n\n# install.packages(\"ggthemes\")\n\nlibrary(ggthemes)\n\nggthemes contains several themes that you can use. Here is a list of some of the themes that are available :\n\ntheme_economist()\ntheme_excel()\ntheme_few()\ntheme_fivethirtyeight()\ntheme_gdocs()\ntheme_hc()\ntheme_solarized()\ntheme_tufte()\ntheme_wsj()\n\nFor example, the theme theme_economist() is based on the style of the Economist magazine, and theme_wsj() is based on the style of the Wall Street Journal. \nYou can find more information about these themes at the following link : \nhttps://jrnold.github.io/ggthemes/\nHere is an example of how to use the theme_economist() theme :\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_economist()\n\n\n\n\n\n\n\n\nHere is an example of how to use the theme_wsj() theme :\n\nggplot(df, aes(x = group, y = value)) +\n  geom_boxplot() +\n  theme_wsj()\n\n\n\n\n\n\n\n\nFeel free to play around with these themes as you are working through your visualizations. Find one that you feel is a good fir for your data and your presentation. \nFor more information about boxplots, here is a useful webpage : \nhttps://www.appsilon.com/post/ggplot2-boxplots",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Advanced Boxplot Techniques."
    ]
  },
  {
    "objectID": "Barplots_And_Histograms.html#barplots",
    "href": "Barplots_And_Histograms.html#barplots",
    "title": "Barplots and Histograms",
    "section": "",
    "text": "You go ahead and immediately use the entore data set.\nOnly assign the x-axis variable.\nUse the default settings to have ggplot count the y values.\n\nNote : If we were making a histogram, this would be the same as using the command geom_histogram(stat=\"count\"). \n\n\n\n\nWrangle the data before plotting to set up a new data frame.\nMap x and y axis using variables\nuse stat=\"identity\" to tell ggplot that the y values are already calculated.\nThis tells ggplot to use your values and not to do any calculations on its own.",
    "crumbs": [
      "Step 3 - Exploration and Visualization",
      "Barplots and Histograms"
    ]
  }
]